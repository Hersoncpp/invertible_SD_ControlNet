nohup: ignoring input
25-09-27 13:08:29.559 - INFO: DataParallel(
  (module): Model(
    (model): Hinet(
      (inv1): INV_block(
        (r): ResidualDenseBlock_out(
          (conv1): Conv2d(12, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (conv2): Conv2d(44, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (conv3): Conv2d(76, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (conv4): Conv2d(108, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (conv5): Conv2d(140, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
        )
        (y): ResidualDenseBlock_out(
          (conv1): Conv2d(12, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (conv2): Conv2d(44, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (conv3): Conv2d(76, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (conv4): Conv2d(108, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (conv5): Conv2d(140, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
        )
        (f): ResidualDenseBlock_out(
          (conv1): Conv2d(12, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (conv2): Conv2d(44, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (conv3): Conv2d(76, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (conv4): Conv2d(108, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (conv5): Conv2d(140, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
        )
      )
      (inv2): INV_block(
        (r): ResidualDenseBlock_out(
          (conv1): Conv2d(12, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (conv2): Conv2d(44, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (conv3): Conv2d(76, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (conv4): Conv2d(108, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (conv5): Conv2d(140, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
        )
        (y): ResidualDenseBlock_out(
          (conv1): Conv2d(12, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (conv2): Conv2d(44, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (conv3): Conv2d(76, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (conv4): Conv2d(108, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (conv5): Conv2d(140, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
        )
        (f): ResidualDenseBlock_out(
          (conv1): Conv2d(12, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (conv2): Conv2d(44, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (conv3): Conv2d(76, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (conv4): Conv2d(108, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (conv5): Conv2d(140, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
        )
      )
      (inv3): INV_block(
        (r): ResidualDenseBlock_out(
          (conv1): Conv2d(12, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (conv2): Conv2d(44, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (conv3): Conv2d(76, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (conv4): Conv2d(108, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (conv5): Conv2d(140, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
        )
        (y): ResidualDenseBlock_out(
          (conv1): Conv2d(12, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (conv2): Conv2d(44, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (conv3): Conv2d(76, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (conv4): Conv2d(108, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (conv5): Conv2d(140, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
        )
        (f): ResidualDenseBlock_out(
          (conv1): Conv2d(12, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (conv2): Conv2d(44, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (conv3): Conv2d(76, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (conv4): Conv2d(108, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (conv5): Conv2d(140, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
        )
      )
      (inv4): INV_block(
        (r): ResidualDenseBlock_out(
          (conv1): Conv2d(12, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (conv2): Conv2d(44, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (conv3): Conv2d(76, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (conv4): Conv2d(108, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (conv5): Conv2d(140, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
        )
        (y): ResidualDenseBlock_out(
          (conv1): Conv2d(12, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (conv2): Conv2d(44, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (conv3): Conv2d(76, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (conv4): Conv2d(108, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (conv5): Conv2d(140, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
        )
        (f): ResidualDenseBlock_out(
          (conv1): Conv2d(12, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (conv2): Conv2d(44, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (conv3): Conv2d(76, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (conv4): Conv2d(108, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (conv5): Conv2d(140, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
        )
      )
      (inv5): INV_block(
        (r): ResidualDenseBlock_out(
          (conv1): Conv2d(12, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (conv2): Conv2d(44, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (conv3): Conv2d(76, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (conv4): Conv2d(108, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (conv5): Conv2d(140, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
        )
        (y): ResidualDenseBlock_out(
          (conv1): Conv2d(12, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (conv2): Conv2d(44, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (conv3): Conv2d(76, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (conv4): Conv2d(108, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (conv5): Conv2d(140, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
        )
        (f): ResidualDenseBlock_out(
          (conv1): Conv2d(12, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (conv2): Conv2d(44, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (conv3): Conv2d(76, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (conv4): Conv2d(108, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (conv5): Conv2d(140, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
        )
      )
      (inv6): INV_block(
        (r): ResidualDenseBlock_out(
          (conv1): Conv2d(12, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (conv2): Conv2d(44, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (conv3): Conv2d(76, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (conv4): Conv2d(108, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (conv5): Conv2d(140, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
        )
        (y): ResidualDenseBlock_out(
          (conv1): Conv2d(12, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (conv2): Conv2d(44, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (conv3): Conv2d(76, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (conv4): Conv2d(108, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (conv5): Conv2d(140, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
        )
        (f): ResidualDenseBlock_out(
          (conv1): Conv2d(12, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (conv2): Conv2d(44, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (conv3): Conv2d(76, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (conv4): Conv2d(108, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (conv5): Conv2d(140, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
        )
      )
      (inv7): INV_block(
        (r): ResidualDenseBlock_out(
          (conv1): Conv2d(12, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (conv2): Conv2d(44, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (conv3): Conv2d(76, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (conv4): Conv2d(108, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (conv5): Conv2d(140, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
        )
        (y): ResidualDenseBlock_out(
          (conv1): Conv2d(12, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (conv2): Conv2d(44, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (conv3): Conv2d(76, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (conv4): Conv2d(108, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (conv5): Conv2d(140, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
        )
        (f): ResidualDenseBlock_out(
          (conv1): Conv2d(12, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (conv2): Conv2d(44, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (conv3): Conv2d(76, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (conv4): Conv2d(108, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (conv5): Conv2d(140, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
        )
      )
      (inv8): INV_block(
        (r): ResidualDenseBlock_out(
          (conv1): Conv2d(12, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (conv2): Conv2d(44, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (conv3): Conv2d(76, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (conv4): Conv2d(108, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (conv5): Conv2d(140, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
        )
        (y): ResidualDenseBlock_out(
          (conv1): Conv2d(12, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (conv2): Conv2d(44, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (conv3): Conv2d(76, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (conv4): Conv2d(108, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (conv5): Conv2d(140, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
        )
        (f): ResidualDenseBlock_out(
          (conv1): Conv2d(12, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (conv2): Conv2d(44, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (conv3): Conv2d(76, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (conv4): Conv2d(108, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (conv5): Conv2d(140, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
        )
      )
      (inv9): INV_block(
        (r): ResidualDenseBlock_out(
          (conv1): Conv2d(12, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (conv2): Conv2d(44, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (conv3): Conv2d(76, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (conv4): Conv2d(108, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (conv5): Conv2d(140, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
        )
        (y): ResidualDenseBlock_out(
          (conv1): Conv2d(12, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (conv2): Conv2d(44, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (conv3): Conv2d(76, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (conv4): Conv2d(108, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (conv5): Conv2d(140, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
        )
        (f): ResidualDenseBlock_out(
          (conv1): Conv2d(12, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (conv2): Conv2d(44, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (conv3): Conv2d(76, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (conv4): Conv2d(108, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (conv5): Conv2d(140, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
        )
      )
      (inv10): INV_block(
        (r): ResidualDenseBlock_out(
          (conv1): Conv2d(12, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (conv2): Conv2d(44, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (conv3): Conv2d(76, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (conv4): Conv2d(108, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (conv5): Conv2d(140, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
        )
        (y): ResidualDenseBlock_out(
          (conv1): Conv2d(12, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (conv2): Conv2d(44, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (conv3): Conv2d(76, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (conv4): Conv2d(108, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (conv5): Conv2d(140, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
        )
        (f): ResidualDenseBlock_out(
          (conv1): Conv2d(12, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (conv2): Conv2d(44, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (conv3): Conv2d(76, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (conv4): Conv2d(108, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (conv5): Conv2d(140, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
        )
      )
      (inv11): INV_block(
        (r): ResidualDenseBlock_out(
          (conv1): Conv2d(12, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (conv2): Conv2d(44, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (conv3): Conv2d(76, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (conv4): Conv2d(108, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (conv5): Conv2d(140, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
        )
        (y): ResidualDenseBlock_out(
          (conv1): Conv2d(12, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (conv2): Conv2d(44, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (conv3): Conv2d(76, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (conv4): Conv2d(108, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (conv5): Conv2d(140, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
        )
        (f): ResidualDenseBlock_out(
          (conv1): Conv2d(12, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (conv2): Conv2d(44, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (conv3): Conv2d(76, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (conv4): Conv2d(108, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (conv5): Conv2d(140, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
        )
      )
      (inv12): INV_block(
        (r): ResidualDenseBlock_out(
          (conv1): Conv2d(12, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (conv2): Conv2d(44, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (conv3): Conv2d(76, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (conv4): Conv2d(108, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (conv5): Conv2d(140, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
        )
        (y): ResidualDenseBlock_out(
          (conv1): Conv2d(12, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (conv2): Conv2d(44, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (conv3): Conv2d(76, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (conv4): Conv2d(108, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (conv5): Conv2d(140, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
        )
        (f): ResidualDenseBlock_out(
          (conv1): Conv2d(12, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (conv2): Conv2d(44, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (conv3): Conv2d(76, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (conv4): Conv2d(108, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (conv5): Conv2d(140, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
        )
      )
      (inv13): INV_block(
        (r): ResidualDenseBlock_out(
          (conv1): Conv2d(12, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (conv2): Conv2d(44, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (conv3): Conv2d(76, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (conv4): Conv2d(108, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (conv5): Conv2d(140, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
        )
        (y): ResidualDenseBlock_out(
          (conv1): Conv2d(12, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (conv2): Conv2d(44, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (conv3): Conv2d(76, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (conv4): Conv2d(108, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (conv5): Conv2d(140, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
        )
        (f): ResidualDenseBlock_out(
          (conv1): Conv2d(12, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (conv2): Conv2d(44, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (conv3): Conv2d(76, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (conv4): Conv2d(108, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (conv5): Conv2d(140, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
        )
      )
      (inv14): INV_block(
        (r): ResidualDenseBlock_out(
          (conv1): Conv2d(12, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (conv2): Conv2d(44, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (conv3): Conv2d(76, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (conv4): Conv2d(108, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (conv5): Conv2d(140, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
        )
        (y): ResidualDenseBlock_out(
          (conv1): Conv2d(12, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (conv2): Conv2d(44, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (conv3): Conv2d(76, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (conv4): Conv2d(108, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (conv5): Conv2d(140, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
        )
        (f): ResidualDenseBlock_out(
          (conv1): Conv2d(12, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (conv2): Conv2d(44, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (conv3): Conv2d(76, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (conv4): Conv2d(108, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (conv5): Conv2d(140, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
        )
      )
      (inv15): INV_block(
        (r): ResidualDenseBlock_out(
          (conv1): Conv2d(12, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (conv2): Conv2d(44, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (conv3): Conv2d(76, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (conv4): Conv2d(108, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (conv5): Conv2d(140, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
        )
        (y): ResidualDenseBlock_out(
          (conv1): Conv2d(12, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (conv2): Conv2d(44, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (conv3): Conv2d(76, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (conv4): Conv2d(108, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (conv5): Conv2d(140, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
        )
        (f): ResidualDenseBlock_out(
          (conv1): Conv2d(12, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (conv2): Conv2d(44, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (conv3): Conv2d(76, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (conv4): Conv2d(108, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (conv5): Conv2d(140, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
        )
      )
      (inv16): INV_block(
        (r): ResidualDenseBlock_out(
          (conv1): Conv2d(12, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (conv2): Conv2d(44, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (conv3): Conv2d(76, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (conv4): Conv2d(108, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (conv5): Conv2d(140, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
        )
        (y): ResidualDenseBlock_out(
          (conv1): Conv2d(12, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (conv2): Conv2d(44, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (conv3): Conv2d(76, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (conv4): Conv2d(108, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (conv5): Conv2d(140, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
        )
        (f): ResidualDenseBlock_out(
          (conv1): Conv2d(12, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (conv2): Conv2d(44, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (conv3): Conv2d(76, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (conv4): Conv2d(108, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (conv5): Conv2d(140, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
        )
      )
    )
  )
)
Loaded 1603 data from /home/hesong/disk1/DF_INV/code/ControlNet-v1-1-nightly/dataset/OmniEdit-Filtered-1.2M_train_filtered/prompts.json
Loaded 103 data from /home/hesong/disk1/DF_INV/code/ControlNet-v1-1-nightly/inv_modules/IVOP/codes/data/dataset/ControlNet_ST/prompts.json
==========================================================================================
Config options:

  IMAGE_PATH               	/home/hesong/disk1/DF_INV/code/ControlNet-v1-1-nightly/inv_modules/HiNet/image/
  IMAGE_PATH_cover         	/home/hesong/disk1/DF_INV/code/ControlNet-v1-1-nightly/inv_modules/HiNet/image/cover/
  IMAGE_PATH_secret        	/home/hesong/disk1/DF_INV/code/ControlNet-v1-1-nightly/inv_modules/HiNet/image/secret/
  IMAGE_PATH_secret_rev    	/home/hesong/disk1/DF_INV/code/ControlNet-v1-1-nightly/inv_modules/HiNet/image/secret-rev/
  IMAGE_PATH_steg          	/home/hesong/disk1/DF_INV/code/ControlNet-v1-1-nightly/inv_modules/HiNet/image/steg/
  MODEL_PATH               	/home/hesong/disk1/DF_INV/code/ControlNet-v1-1-nightly/inv_modules/HiNet/model/
  SAVE_freq                	50
  TRAIN_JSON_PATH          	/home/hesong/disk1/DF_INV/code/ControlNet-v1-1-nightly/dataset/OmniEdit-Filtered-1.2M_train_filtered/prompts.json
  VAL_JSON_PATH            	/home/hesong/disk1/DF_INV/code/ControlNet-v1-1-nightly/inv_modules/IVOP/codes/data/dataset/ControlNet_ST/prompts.json
  batch_size               	8
  batchsize_val            	2
  betas                    	(0.5, 0.999)
  channels_in              	3
  checkpoint_on_error      	True
  clamp                    	2.0
  cropsize                 	192
  cropsize_val             	1024
  device_ids               	[1]
  epochs                   	1000
  format_train             	png
  format_val               	png
  gamma                    	0.5
  init_scale               	0.01
  lamda_guide              	1
  lamda_low_frequency      	1
  lamda_reconstruction     	5
  live_visualization       	False
  log10_lr                 	-4.5
  loss_display_cutoff      	2.0
  loss_names               	['L', 'lr']
  lr                       	3.1622776601683795e-05
  progress_bar             	False
  save_suffix              	full
  shuffle_val              	False
  silent                   	False
  suffix                   	modelmodel_checkpoint_00100.pt
  tain_next                	False
  trained_epoch            	0
  val_freq                 	50
  weight_decay             	1e-05
  weight_step              	1000
==========================================================================================

Epoch		L		lr
{'Total': 4050240, 'Trainable': 4050240}
25-09-27 13:11:21.969 - INFO: Learning rate: 3.1622776601683795e-05
25-09-27 13:11:21.969 - INFO: Train epoch 1:   Loss: 438693.2723 | r_Loss: 82291.3770 | g_Loss: 24063.7463 | l_Loss: 3172.6424 | 
                                                                                 001		438693.2723		-4.5000
25-09-27 13:14:12.620 - INFO: Learning rate: 3.1622776601683795e-05
25-09-27 13:14:12.621 - INFO: Train epoch 2:   Loss: 80900.8457 | r_Loss: 14271.8368 | g_Loss: 8080.0565 | l_Loss: 1461.6048 | 
                                                                                 002		80900.8457		-4.5000
25-09-27 13:17:02.731 - INFO: Learning rate: 3.1622776601683795e-05
25-09-27 13:17:02.732 - INFO: Train epoch 3:   Loss: 56254.2444 | r_Loss: 9271.1346 | g_Loss: 7457.3086 | l_Loss: 2441.2627 | 
                                                                                 003		56254.2444		-4.5000
25-09-27 13:19:52.414 - INFO: Learning rate: 3.1622776601683795e-05
25-09-27 13:19:52.416 - INFO: Train epoch 4:   Loss: 104534.9765 | r_Loss: 17411.1044 | g_Loss: 14508.4776 | l_Loss: 2970.9776 | 
                                                                                 004		104534.9765		-4.5000
25-09-27 13:22:42.317 - INFO: Learning rate: 3.1622776601683795e-05
25-09-27 13:22:42.317 - INFO: Train epoch 5:   Loss: 53856.1447 | r_Loss: 8489.2789 | g_Loss: 9044.1350 | l_Loss: 2365.6152 | 
                                                                                 005		53856.1447		-4.5000
25-09-27 13:25:32.766 - INFO: Learning rate: 3.1622776601683795e-05
25-09-27 13:25:32.767 - INFO: Train epoch 6:   Loss: 37767.6157 | r_Loss: 5614.7764 | g_Loss: 7900.3292 | l_Loss: 1793.4048 | 
                                                                                 006		37767.6157		-4.5000
25-09-27 13:28:24.720 - INFO: Learning rate: 3.1622776601683795e-05
25-09-27 13:28:24.721 - INFO: Train epoch 7:   Loss: 34795.1972 | r_Loss: 5457.8984 | g_Loss: 6425.4717 | l_Loss: 1080.2334 | 
                                                                                 007		34795.1972		-4.5000
25-09-27 13:31:16.654 - INFO: Learning rate: 3.1622776601683795e-05
25-09-27 13:31:16.655 - INFO: Train epoch 8:   Loss: 29869.6213 | r_Loss: 4736.2182 | g_Loss: 5502.5996 | l_Loss: 685.9308 | 
                                                                                 008		29869.6213		-4.5000
25-09-27 13:34:07.972 - INFO: Learning rate: 3.1622776601683795e-05
25-09-27 13:34:07.972 - INFO: Train epoch 9:   Loss: 26550.7568 | r_Loss: 4216.6491 | g_Loss: 4928.9783 | l_Loss: 538.5329 | 
                                                                                 009		26550.7568		-4.5000
25-09-27 13:36:58.417 - INFO: Learning rate: 3.1622776601683795e-05
25-09-27 13:36:58.418 - INFO: Train epoch 10:   Loss: 24826.9844 | r_Loss: 3997.5920 | g_Loss: 4368.9214 | l_Loss: 470.1027 | 
                                                                                 010		24826.9844		-4.5000
25-09-27 13:39:47.815 - INFO: Learning rate: 3.1622776601683795e-05
25-09-27 13:39:47.816 - INFO: Train epoch 11:   Loss: 23319.8541 | r_Loss: 3751.3922 | g_Loss: 4111.0462 | l_Loss: 451.8467 | 
                                                                                 011		23319.8541		-4.5000
25-09-27 13:42:38.789 - INFO: Learning rate: 3.1622776601683795e-05
25-09-27 13:42:38.790 - INFO: Train epoch 12:   Loss: 19991.8204 | r_Loss: 3168.2121 | g_Loss: 3717.3204 | l_Loss: 433.4394 | 
                                                                                 012		19991.8204		-4.5000
25-09-27 13:45:29.145 - INFO: Learning rate: 3.1622776601683795e-05
25-09-27 13:45:29.146 - INFO: Train epoch 13:   Loss: 18634.8003 | r_Loss: 2974.8564 | g_Loss: 3332.6972 | l_Loss: 427.8213 | 
                                                                                 013		18634.8003		-4.5000
25-09-27 13:48:19.933 - INFO: Learning rate: 3.1622776601683795e-05
25-09-27 13:48:19.934 - INFO: Train epoch 14:   Loss: 18167.8804 | r_Loss: 2895.1638 | g_Loss: 3250.0714 | l_Loss: 441.9900 | 
                                                                                 014		18167.8804		-4.5000
25-09-27 13:51:10.575 - INFO: Learning rate: 3.1622776601683795e-05
25-09-27 13:51:10.576 - INFO: Train epoch 15:   Loss: 17016.9652 | r_Loss: 2684.6447 | g_Loss: 3149.3838 | l_Loss: 444.3580 | 
                                                                                 015		17016.9652		-4.5000
25-09-27 13:54:02.130 - INFO: Learning rate: 3.1622776601683795e-05
25-09-27 13:54:02.132 - INFO: Train epoch 16:   Loss: 15087.4869 | r_Loss: 2321.4206 | g_Loss: 3052.5618 | l_Loss: 427.8219 | 
                                                                                 016		15087.4869		-4.5000
25-09-27 13:56:52.738 - INFO: Learning rate: 3.1622776601683795e-05
25-09-27 13:56:52.739 - INFO: Train epoch 17:   Loss: 13845.0623 | r_Loss: 2091.6124 | g_Loss: 3032.2738 | l_Loss: 354.7267 | 
                                                                                 017		13845.0623		-4.5000
25-09-27 13:59:43.931 - INFO: Learning rate: 3.1622776601683795e-05
25-09-27 13:59:43.932 - INFO: Train epoch 18:   Loss: 12465.9081 | r_Loss: 1843.2545 | g_Loss: 2986.5654 | l_Loss: 263.0702 | 
                                                                                 018		12465.9081		-4.5000
25-09-27 14:02:35.623 - INFO: Learning rate: 3.1622776601683795e-05
25-09-27 14:02:35.624 - INFO: Train epoch 19:   Loss: 11572.6229 | r_Loss: 1713.5245 | g_Loss: 2787.3075 | l_Loss: 217.6927 | 
                                                                                 019		11572.6229		-4.5000
25-09-27 14:05:26.083 - INFO: Learning rate: 3.1622776601683795e-05
25-09-27 14:05:26.084 - INFO: Train epoch 20:   Loss: 10512.4648 | r_Loss: 1551.5086 | g_Loss: 2557.8720 | l_Loss: 197.0498 | 
                                                                                 020		10512.4648		-4.5000
25-09-27 14:08:16.515 - INFO: Learning rate: 3.1622776601683795e-05
25-09-27 14:08:16.516 - INFO: Train epoch 21:   Loss: 9983.6743 | r_Loss: 1456.3939 | g_Loss: 2513.2283 | l_Loss: 188.4766 | 
                                                                                 021		9983.6743		-4.5000
25-09-27 14:11:07.553 - INFO: Learning rate: 3.1622776601683795e-05
25-09-27 14:11:07.553 - INFO: Train epoch 22:   Loss: 9149.2914 | r_Loss: 1334.5503 | g_Loss: 2304.7507 | l_Loss: 171.7891 | 
                                                                                 022		9149.2914		-4.5000
25-09-27 14:13:58.359 - INFO: Learning rate: 3.1622776601683795e-05
25-09-27 14:13:58.360 - INFO: Train epoch 23:   Loss: 8621.1924 | r_Loss: 1250.0751 | g_Loss: 2206.8820 | l_Loss: 163.9348 | 
                                                                                 023		8621.1924		-4.5000
25-09-27 14:16:49.562 - INFO: Learning rate: 3.1622776601683795e-05
25-09-27 14:16:49.563 - INFO: Train epoch 24:   Loss: 10043.1163 | r_Loss: 1481.2933 | g_Loss: 2452.2470 | l_Loss: 184.4027 | 
                                                                                 024		10043.1163		-4.5000
25-09-27 14:19:40.862 - INFO: Learning rate: 3.1622776601683795e-05
25-09-27 14:19:40.863 - INFO: Train epoch 25:   Loss: 8062.8899 | r_Loss: 1138.7351 | g_Loss: 2220.1732 | l_Loss: 149.0413 | 
                                                                                 025		8062.8899		-4.5000
25-09-27 14:22:31.753 - INFO: Learning rate: 3.1622776601683795e-05
25-09-27 14:22:31.754 - INFO: Train epoch 26:   Loss: 7198.2608 | r_Loss: 1021.1527 | g_Loss: 1959.7744 | l_Loss: 132.7230 | 
                                                                                 026		7198.2608		-4.5000
25-09-27 14:25:22.117 - INFO: Learning rate: 3.1622776601683795e-05
25-09-27 14:25:22.118 - INFO: Train epoch 27:   Loss: 7041.7227 | r_Loss: 1003.6075 | g_Loss: 1898.1954 | l_Loss: 125.4901 | 
                                                                                 027		7041.7227		-4.5000
25-09-27 14:28:14.393 - INFO: Learning rate: 3.1622776601683795e-05
25-09-27 14:28:14.394 - INFO: Train epoch 28:   Loss: 6647.8725 | r_Loss: 939.8340 | g_Loss: 1830.1125 | l_Loss: 118.5900 | 
                                                                                 028		6647.8725		-4.5000
25-09-27 14:31:06.459 - INFO: Learning rate: 3.1622776601683795e-05
25-09-27 14:31:06.461 - INFO: Train epoch 29:   Loss: 6237.8607 | r_Loss: 869.7462 | g_Loss: 1778.6107 | l_Loss: 110.5188 | 
                                                                                 029		6237.8607		-4.5000
25-09-27 14:33:58.278 - INFO: Learning rate: 3.1622776601683795e-05
25-09-27 14:33:58.279 - INFO: Train epoch 30:   Loss: 63885.1564 | r_Loss: 11164.1706 | g_Loss: 7066.2290 | l_Loss: 998.0753 | 
                                                                                 030		63885.1564		-4.5000
25-09-27 14:36:48.693 - INFO: Learning rate: 3.1622776601683795e-05
25-09-27 14:36:48.693 - INFO: Train epoch 31:   Loss: 21337.8024 | r_Loss: 2755.3336 | g_Loss: 7165.1793 | l_Loss: 395.9553 | 
                                                                                 031		21337.8024		-4.5000
25-09-27 14:39:40.286 - INFO: Learning rate: 3.1622776601683795e-05
25-09-27 14:39:40.287 - INFO: Train epoch 32:   Loss: 12812.0542 | r_Loss: 1719.8171 | g_Loss: 3983.2425 | l_Loss: 229.7262 | 
                                                                                 032		12812.0542		-4.5000
25-09-27 14:42:32.090 - INFO: Learning rate: 3.1622776601683795e-05
25-09-27 14:42:32.091 - INFO: Train epoch 33:   Loss: 10815.2878 | r_Loss: 1469.6461 | g_Loss: 3292.3083 | l_Loss: 174.7493 | 
                                                                                 033		10815.2878		-4.5000
25-09-27 14:45:23.981 - INFO: Learning rate: 3.1622776601683795e-05
25-09-27 14:45:23.982 - INFO: Train epoch 34:   Loss: 9959.7841 | r_Loss: 1343.9229 | g_Loss: 3085.2159 | l_Loss: 154.9537 | 
                                                                                 034		9959.7841		-4.5000
25-09-27 14:48:15.792 - INFO: Learning rate: 3.1622776601683795e-05
25-09-27 14:48:15.793 - INFO: Train epoch 35:   Loss: 9024.4829 | r_Loss: 1219.8463 | g_Loss: 2789.4588 | l_Loss: 135.7925 | 
                                                                                 035		9024.4829		-4.5000
25-09-27 14:51:07.585 - INFO: Learning rate: 3.1622776601683795e-05
25-09-27 14:51:07.586 - INFO: Train epoch 36:   Loss: 8258.2256 | r_Loss: 1119.0856 | g_Loss: 2542.3048 | l_Loss: 120.4930 | 
                                                                                 036		8258.2256		-4.5000
25-09-27 14:53:59.810 - INFO: Learning rate: 3.1622776601683795e-05
25-09-27 14:53:59.811 - INFO: Train epoch 37:   Loss: 7824.4866 | r_Loss: 1052.6803 | g_Loss: 2447.4125 | l_Loss: 113.6726 | 
                                                                                 037		7824.4866		-4.5000
25-09-27 14:56:51.148 - INFO: Learning rate: 3.1622776601683795e-05
25-09-27 14:56:51.149 - INFO: Train epoch 38:   Loss: 7502.8620 | r_Loss: 1003.7968 | g_Loss: 2375.4565 | l_Loss: 108.4214 | 
                                                                                 038		7502.8620		-4.5000
25-09-27 14:59:42.129 - INFO: Learning rate: 3.1622776601683795e-05
25-09-27 14:59:42.130 - INFO: Train epoch 39:   Loss: 7145.6365 | r_Loss: 959.1326 | g_Loss: 2247.9320 | l_Loss: 102.0413 | 
                                                                                 039		7145.6365		-4.5000
25-09-27 15:02:33.288 - INFO: Learning rate: 3.1622776601683795e-05
25-09-27 15:02:33.289 - INFO: Train epoch 40:   Loss: 6640.9273 | r_Loss: 888.9677 | g_Loss: 2102.9828 | l_Loss: 93.1057 | 
                                                                                 040		6640.9273		-4.5000
25-09-27 15:05:24.696 - INFO: Learning rate: 3.1622776601683795e-05
25-09-27 15:05:24.698 - INFO: Train epoch 41:   Loss: 6464.9644 | r_Loss: 874.6751 | g_Loss: 2003.2935 | l_Loss: 88.2953 | 
                                                                                 041		6464.9644		-4.5000
25-09-27 15:08:16.435 - INFO: Learning rate: 3.1622776601683795e-05
25-09-27 15:08:16.436 - INFO: Train epoch 42:   Loss: 6065.9295 | r_Loss: 800.2067 | g_Loss: 1979.2351 | l_Loss: 85.6609 | 
                                                                                 042		6065.9295		-4.5000
25-09-27 15:11:08.097 - INFO: Learning rate: 3.1622776601683795e-05
25-09-27 15:11:08.098 - INFO: Train epoch 43:   Loss: 5776.9448 | r_Loss: 753.8866 | g_Loss: 1923.1302 | l_Loss: 84.3817 | 
                                                                                 043		5776.9448		-4.5000
25-09-27 15:13:59.945 - INFO: Learning rate: 3.1622776601683795e-05
25-09-27 15:13:59.947 - INFO: Train epoch 44:   Loss: 5476.8314 | r_Loss: 731.3328 | g_Loss: 1745.2877 | l_Loss: 74.8797 | 
                                                                                 044		5476.8314		-4.5000
25-09-27 15:16:51.820 - INFO: Learning rate: 3.1622776601683795e-05
25-09-27 15:16:51.821 - INFO: Train epoch 45:   Loss: 5330.3323 | r_Loss: 690.7924 | g_Loss: 1800.2631 | l_Loss: 76.1074 | 
                                                                                 045		5330.3323		-4.5000
25-09-27 15:19:43.590 - INFO: Learning rate: 3.1622776601683795e-05
25-09-27 15:19:43.591 - INFO: Train epoch 46:   Loss: 4951.4255 | r_Loss: 653.9513 | g_Loss: 1613.8826 | l_Loss: 67.7864 | 
                                                                                 046		4951.4255		-4.5000
25-09-27 15:22:34.764 - INFO: Learning rate: 3.1622776601683795e-05
25-09-27 15:22:34.765 - INFO: Train epoch 47:   Loss: 4804.7004 | r_Loss: 629.0397 | g_Loss: 1592.9674 | l_Loss: 66.5346 | 
                                                                                 047		4804.7004		-4.5000
25-09-27 15:25:26.014 - INFO: Learning rate: 3.1622776601683795e-05
25-09-27 15:25:26.015 - INFO: Train epoch 48:   Loss: 4496.3693 | r_Loss: 588.4407 | g_Loss: 1492.8706 | l_Loss: 61.2954 | 
                                                                                 048		4496.3693		-4.5000
25-09-27 15:28:17.494 - INFO: Learning rate: 3.1622776601683795e-05
25-09-27 15:28:17.495 - INFO: Train epoch 49:   Loss: 6250.9300 | r_Loss: 860.4267 | g_Loss: 1877.8940 | l_Loss: 70.9026 | 
                                                                                 049		6250.9300		-4.5000
25-09-27 15:32:12.338 - INFO: TEST:   PSNR_S: 36.9996 | PSNR_C: 28.1443 | 
25-09-27 15:32:12.339 - INFO: Learning rate: 3.1622776601683795e-05
25-09-27 15:32:12.340 - INFO: Train epoch 50:   Loss: 4461.5376 | r_Loss: 580.7900 | g_Loss: 1503.9274 | l_Loss: 53.6601 | 
                                                                                 050		4461.5376		-4.5000
25-09-27 15:35:03.187 - INFO: Learning rate: 3.1622776601683795e-05
25-09-27 15:35:03.188 - INFO: Train epoch 51:   Loss: 4190.6513 | r_Loss: 543.5028 | g_Loss: 1420.8738 | l_Loss: 52.2634 | 
                                                                                 051		4190.6513		-4.5000
25-09-27 15:37:53.182 - INFO: Learning rate: 3.1622776601683795e-05
25-09-27 15:37:53.183 - INFO: Train epoch 52:   Loss: 3881.6545 | r_Loss: 502.9248 | g_Loss: 1318.2949 | l_Loss: 48.7358 | 
                                                                                 052		3881.6545		-4.5000
25-09-27 15:40:43.763 - INFO: Learning rate: 3.1622776601683795e-05
25-09-27 15:40:43.764 - INFO: Train epoch 53:   Loss: 3924.4243 | r_Loss: 509.1157 | g_Loss: 1328.4136 | l_Loss: 50.4320 | 
                                                                                 053		3924.4243		-4.5000
25-09-27 15:43:35.200 - INFO: Learning rate: 3.1622776601683795e-05
25-09-27 15:43:35.201 - INFO: Train epoch 54:   Loss: 28644.2763 | r_Loss: 4390.7412 | g_Loss: 6281.5142 | l_Loss: 409.0562 | 
                                                                                 054		28644.2763		-4.5000
25-09-27 15:46:26.643 - INFO: Learning rate: 3.1622776601683795e-05
25-09-27 15:46:26.643 - INFO: Train epoch 55:   Loss: 8852.3117 | r_Loss: 1043.8087 | g_Loss: 3525.0212 | l_Loss: 108.2469 | 
                                                                                 055		8852.3117		-4.5000
25-09-27 15:49:20.911 - INFO: Learning rate: 3.1622776601683795e-05
25-09-27 15:49:20.912 - INFO: Train epoch 56:   Loss: 6863.0719 | r_Loss: 845.3463 | g_Loss: 2559.0850 | l_Loss: 77.2554 | 
                                                                                 056		6863.0719		-4.5000
25-09-27 15:52:12.899 - INFO: Learning rate: 3.1622776601683795e-05
25-09-27 15:52:12.899 - INFO: Train epoch 57:   Loss: 5477.3214 | r_Loss: 688.7726 | g_Loss: 1982.0455 | l_Loss: 51.4130 | 
                                                                                 057		5477.3214		-4.5000
25-09-27 15:55:03.659 - INFO: Learning rate: 3.1622776601683795e-05
25-09-27 15:55:03.660 - INFO: Train epoch 58:   Loss: 4569.3908 | r_Loss: 559.0507 | g_Loss: 1730.0495 | l_Loss: 44.0876 | 
                                                                                 058		4569.3908		-4.5000
25-09-27 15:57:53.287 - INFO: Learning rate: 3.1622776601683795e-05
25-09-27 15:57:53.288 - INFO: Train epoch 59:   Loss: 4338.3961 | r_Loss: 544.0236 | g_Loss: 1577.3373 | l_Loss: 40.9406 | 
                                                                                 059		4338.3961		-4.5000
25-09-27 16:00:48.413 - INFO: Learning rate: 3.1622776601683795e-05
25-09-27 16:00:48.413 - INFO: Train epoch 60:   Loss: 3958.8841 | r_Loss: 496.1716 | g_Loss: 1440.8710 | l_Loss: 37.1553 | 
                                                                                 060		3958.8841		-4.5000
25-09-27 16:03:43.137 - INFO: Learning rate: 3.1622776601683795e-05
25-09-27 16:03:43.138 - INFO: Train epoch 61:   Loss: 3638.3638 | r_Loss: 455.9710 | g_Loss: 1323.3704 | l_Loss: 35.1383 | 
                                                                                 061		3638.3638		-4.5000
25-09-27 16:06:33.293 - INFO: Learning rate: 3.1622776601683795e-05
25-09-27 16:06:33.293 - INFO: Train epoch 62:   Loss: 3524.1287 | r_Loss: 444.7417 | g_Loss: 1265.8480 | l_Loss: 34.5720 | 
                                                                                 062		3524.1287		-4.5000
25-09-27 16:09:23.982 - INFO: Learning rate: 3.1622776601683795e-05
25-09-27 16:09:23.982 - INFO: Train epoch 63:   Loss: 3279.0507 | r_Loss: 411.6051 | g_Loss: 1187.9368 | l_Loss: 33.0882 | 
                                                                                 063		3279.0507		-4.5000
25-09-27 16:12:15.075 - INFO: Learning rate: 3.1622776601683795e-05
25-09-27 16:12:15.075 - INFO: Train epoch 64:   Loss: 3206.9285 | r_Loss: 405.0406 | g_Loss: 1149.2192 | l_Loss: 32.5062 | 
                                                                                 064		3206.9285		-4.5000
25-09-27 16:15:05.713 - INFO: Learning rate: 3.1622776601683795e-05
25-09-27 16:15:05.713 - INFO: Train epoch 65:   Loss: 3418.1415 | r_Loss: 440.9213 | g_Loss: 1180.0073 | l_Loss: 33.5278 | 
                                                                                 065		3418.1415		-4.5000
25-09-27 16:17:57.249 - INFO: Learning rate: 3.1622776601683795e-05
25-09-27 16:17:57.249 - INFO: Train epoch 66:   Loss: 6147.1705 | r_Loss: 792.7873 | g_Loss: 2112.6068 | l_Loss: 70.6270 | 
                                                                                 066		6147.1705		-4.5000
25-09-27 16:20:48.260 - INFO: Learning rate: 3.1622776601683795e-05
25-09-27 16:20:48.260 - INFO: Train epoch 67:   Loss: 3194.9455 | r_Loss: 389.4665 | g_Loss: 1216.1492 | l_Loss: 31.4637 | 
                                                                                 067		3194.9455		-4.5000
25-09-27 16:23:39.823 - INFO: Learning rate: 3.1622776601683795e-05
25-09-27 16:23:39.824 - INFO: Train epoch 68:   Loss: 2990.2040 | r_Loss: 372.2161 | g_Loss: 1098.7165 | l_Loss: 30.4069 | 
                                                                                 068		2990.2040		-4.5000
25-09-27 16:26:35.712 - INFO: Learning rate: 3.1622776601683795e-05
25-09-27 16:26:35.713 - INFO: Train epoch 69:   Loss: 2727.3013 | r_Loss: 342.0989 | g_Loss: 989.1630 | l_Loss: 27.6438 | 
                                                                                 069		2727.3013		-4.5000
25-09-27 16:29:27.555 - INFO: Learning rate: 3.1622776601683795e-05
25-09-27 16:29:27.556 - INFO: Train epoch 70:   Loss: 2612.5758 | r_Loss: 328.0093 | g_Loss: 945.5262 | l_Loss: 27.0032 | 
                                                                                 070		2612.5758		-4.5000
25-09-27 16:32:18.936 - INFO: Learning rate: 3.1622776601683795e-05
25-09-27 16:32:18.937 - INFO: Train epoch 71:   Loss: 13725.9388 | r_Loss: 1890.7576 | g_Loss: 4099.0305 | l_Loss: 173.1199 | 
                                                                                 071		13725.9388		-4.5000
25-09-27 16:35:10.062 - INFO: Learning rate: 3.1622776601683795e-05
25-09-27 16:35:10.063 - INFO: Train epoch 72:   Loss: 4948.6963 | r_Loss: 577.3540 | g_Loss: 2006.3977 | l_Loss: 55.5284 | 
                                                                                 072		4948.6963		-4.5000
25-09-27 16:38:01.360 - INFO: Learning rate: 3.1622776601683795e-05
25-09-27 16:38:01.360 - INFO: Train epoch 73:   Loss: 3548.1683 | r_Loss: 431.6614 | g_Loss: 1358.5120 | l_Loss: 31.3493 | 
                                                                                 073		3548.1683		-4.5000
25-09-27 16:41:12.142 - INFO: Learning rate: 3.1622776601683795e-05
25-09-27 16:41:12.142 - INFO: Train epoch 74:   Loss: 3132.0518 | r_Loss: 388.1681 | g_Loss: 1162.2561 | l_Loss: 28.9552 | 
                                                                                 074		3132.0518		-4.5000
25-09-27 16:44:05.255 - INFO: Learning rate: 3.1622776601683795e-05
25-09-27 16:44:05.256 - INFO: Train epoch 75:   Loss: 2671.1207 | r_Loss: 328.9173 | g_Loss: 1003.8109 | l_Loss: 22.7234 | 
                                                                                 075		2671.1207		-4.5000
25-09-27 16:47:03.916 - INFO: Learning rate: 3.1622776601683795e-05
25-09-27 16:47:03.916 - INFO: Train epoch 76:   Loss: 2583.5796 | r_Loss: 324.9813 | g_Loss: 934.8072 | l_Loss: 23.8660 | 
                                                                                 076		2583.5796		-4.5000
25-09-27 16:49:56.834 - INFO: Learning rate: 3.1622776601683795e-05
25-09-27 16:49:56.834 - INFO: Train epoch 77:   Loss: 2509.2278 | r_Loss: 319.0823 | g_Loss: 889.2707 | l_Loss: 24.5456 | 
                                                                                 077		2509.2278		-4.5000
25-09-27 16:52:48.396 - INFO: Learning rate: 3.1622776601683795e-05
25-09-27 16:52:48.397 - INFO: Train epoch 78:   Loss: 13131.9572 | r_Loss: 1903.6896 | g_Loss: 3263.2630 | l_Loss: 350.2460 | 
                                                                                 078		13131.9572		-4.5000
25-09-27 16:55:39.055 - INFO: Learning rate: 3.1622776601683795e-05
25-09-27 16:55:39.056 - INFO: Train epoch 79:   Loss: 5007.8892 | r_Loss: 530.8345 | g_Loss: 2293.8844 | l_Loss: 59.8322 | 
                                                                                 079		5007.8892		-4.5000
25-09-27 16:58:30.523 - INFO: Learning rate: 3.1622776601683795e-05
25-09-27 16:58:30.524 - INFO: Train epoch 80:   Loss: 3322.4711 | r_Loss: 389.7507 | g_Loss: 1342.0151 | l_Loss: 31.7025 | 
                                                                                 080		3322.4711		-4.5000
25-09-27 17:01:21.363 - INFO: Learning rate: 3.1622776601683795e-05
25-09-27 17:01:21.364 - INFO: Train epoch 81:   Loss: 2644.2058 | r_Loss: 319.3801 | g_Loss: 1022.2294 | l_Loss: 25.0759 | 
                                                                                 081		2644.2058		-4.5000
25-09-27 17:04:17.409 - INFO: Learning rate: 3.1622776601683795e-05
25-09-27 17:04:17.410 - INFO: Train epoch 82:   Loss: 2493.7857 | r_Loss: 307.8276 | g_Loss: 930.1899 | l_Loss: 24.4577 | 
                                                                                 082		2493.7857		-4.5000
25-09-27 17:07:09.588 - INFO: Learning rate: 3.1622776601683795e-05
25-09-27 17:07:09.588 - INFO: Train epoch 83:   Loss: 2346.9817 | r_Loss: 287.9164 | g_Loss: 881.4555 | l_Loss: 25.9442 | 
                                                                                 083		2346.9817		-4.5000
25-09-27 17:10:00.441 - INFO: Learning rate: 3.1622776601683795e-05
25-09-27 17:10:00.441 - INFO: Train epoch 84:   Loss: 2175.2342 | r_Loss: 267.8263 | g_Loss: 814.7859 | l_Loss: 21.3170 | 
                                                                                 084		2175.2342		-4.5000
25-09-27 17:12:55.145 - INFO: Learning rate: 3.1622776601683795e-05
25-09-27 17:12:55.146 - INFO: Train epoch 85:   Loss: 5162.9373 | r_Loss: 648.5312 | g_Loss: 1846.8459 | l_Loss: 73.4352 | 
                                                                                 085		5162.9373		-4.5000
25-09-27 17:15:45.840 - INFO: Learning rate: 3.1622776601683795e-05
25-09-27 17:15:45.841 - INFO: Train epoch 86:   Loss: 2373.4189 | r_Loss: 275.7017 | g_Loss: 974.1709 | l_Loss: 20.7395 | 
                                                                                 086		2373.4189		-4.5000
25-09-27 17:18:45.042 - INFO: Learning rate: 3.1622776601683795e-05
25-09-27 17:18:45.043 - INFO: Train epoch 87:   Loss: 2067.9509 | r_Loss: 251.6890 | g_Loss: 790.8021 | l_Loss: 18.7039 | 
                                                                                 087		2067.9509		-4.5000
25-09-27 17:22:23.783 - INFO: Learning rate: 3.1622776601683795e-05
25-09-27 17:22:23.784 - INFO: Train epoch 88:   Loss: 1965.5590 | r_Loss: 241.9944 | g_Loss: 736.9498 | l_Loss: 18.6372 | 
                                                                                 088		1965.5590		-4.5000
25-09-27 17:25:13.579 - INFO: Learning rate: 3.1622776601683795e-05
25-09-27 17:25:13.580 - INFO: Train epoch 89:   Loss: 1886.1874 | r_Loss: 233.3446 | g_Loss: 700.8924 | l_Loss: 18.5720 | 
                                                                                 089		1886.1874		-4.5000
25-09-27 17:28:11.659 - INFO: Learning rate: 3.1622776601683795e-05
25-09-27 17:28:11.660 - INFO: Train epoch 90:   Loss: 1814.7706 | r_Loss: 222.2959 | g_Loss: 684.1593 | l_Loss: 19.1318 | 
                                                                                 090		1814.7706		-4.5000
25-09-27 17:31:44.640 - INFO: Learning rate: 3.1622776601683795e-05
25-09-27 17:31:44.640 - INFO: Train epoch 91:   Loss: 11737.9758 | r_Loss: 1731.2695 | g_Loss: 2936.6358 | l_Loss: 144.9927 | 
                                                                                 091		11737.9758		-4.5000
25-09-27 17:34:36.930 - INFO: Learning rate: 3.1622776601683795e-05
25-09-27 17:34:36.931 - INFO: Train epoch 92:   Loss: 3797.4371 | r_Loss: 403.8116 | g_Loss: 1739.0399 | l_Loss: 39.3390 | 
                                                                                 092		3797.4371		-4.5000
25-09-27 17:37:33.736 - INFO: Learning rate: 3.1622776601683795e-05
25-09-27 17:37:33.737 - INFO: Train epoch 93:   Loss: 3035.1802 | r_Loss: 359.3700 | g_Loss: 1213.5430 | l_Loss: 24.7872 | 
                                                                                 093		3035.1802		-4.5000
25-09-27 17:40:24.387 - INFO: Learning rate: 3.1622776601683795e-05
25-09-27 17:40:24.388 - INFO: Train epoch 94:   Loss: 2492.4630 | r_Loss: 290.8799 | g_Loss: 1017.9495 | l_Loss: 20.1140 | 
                                                                                 094		2492.4630		-4.5000
25-09-27 17:43:21.260 - INFO: Learning rate: 3.1622776601683795e-05
25-09-27 17:43:21.261 - INFO: Train epoch 95:   Loss: 2209.4849 | r_Loss: 265.7733 | g_Loss: 863.1688 | l_Loss: 17.4495 | 
                                                                                 095		2209.4849		-4.5000
25-09-27 17:46:19.513 - INFO: Learning rate: 3.1622776601683795e-05
25-09-27 17:46:19.514 - INFO: Train epoch 96:   Loss: 1997.5257 | r_Loss: 240.3371 | g_Loss: 779.3281 | l_Loss: 16.5122 | 
                                                                                 096		1997.5257		-4.5000
25-09-27 17:49:26.451 - INFO: Learning rate: 3.1622776601683795e-05
25-09-27 17:49:26.452 - INFO: Train epoch 97:   Loss: 1871.1226 | r_Loss: 225.8410 | g_Loss: 725.6416 | l_Loss: 16.2758 | 
                                                                                 097		1871.1226		-4.5000
25-09-27 17:52:16.663 - INFO: Learning rate: 3.1622776601683795e-05
25-09-27 17:52:16.664 - INFO: Train epoch 98:   Loss: 1738.5740 | r_Loss: 209.4311 | g_Loss: 675.4027 | l_Loss: 16.0158 | 
                                                                                 098		1738.5740		-4.5000
25-09-27 17:55:19.431 - INFO: Learning rate: 3.1622776601683795e-05
25-09-27 17:55:19.432 - INFO: Train epoch 99:   Loss: 1693.6280 | r_Loss: 205.9442 | g_Loss: 647.9416 | l_Loss: 15.9656 | 
                                                                                 099		1693.6280		-4.5000
25-09-27 18:00:26.970 - INFO: TEST:   PSNR_S: 39.1812 | PSNR_C: 26.7171 | 
25-09-27 18:00:26.973 - INFO: Learning rate: 3.1622776601683795e-05
25-09-27 18:00:26.973 - INFO: Train epoch 100:   Loss: 3978.7679 | r_Loss: 525.6596 | g_Loss: 1305.3846 | l_Loss: 45.0854 | 
                                                                                 100		3978.7679		-4.5000
25-09-27 18:03:16.508 - INFO: Learning rate: 3.1622776601683795e-05
25-09-27 18:03:16.509 - INFO: Train epoch 101:   Loss: 2152.2263 | r_Loss: 220.6589 | g_Loss: 1029.7225 | l_Loss: 19.2093 | 
                                                                                 101		2152.2263		-4.5000
25-09-27 18:06:08.656 - INFO: Learning rate: 3.1622776601683795e-05
25-09-27 18:06:08.657 - INFO: Train epoch 102:   Loss: 1666.1980 | r_Loss: 197.3098 | g_Loss: 664.5251 | l_Loss: 15.1238 | 
                                                                                 102		1666.1980		-4.5000
25-09-27 18:08:59.947 - INFO: Learning rate: 3.1622776601683795e-05
25-09-27 18:08:59.948 - INFO: Train epoch 103:   Loss: 1525.2375 | r_Loss: 182.6094 | g_Loss: 597.2444 | l_Loss: 14.9462 | 
                                                                                 103		1525.2375		-4.5000
25-09-27 18:11:56.026 - INFO: Learning rate: 3.1622776601683795e-05
25-09-27 18:11:56.027 - INFO: Train epoch 104:   Loss: 1471.3411 | r_Loss: 177.7314 | g_Loss: 567.8318 | l_Loss: 14.8523 | 
                                                                                 104		1471.3411		-4.5000
25-09-27 18:14:48.105 - INFO: Learning rate: 3.1622776601683795e-05
25-09-27 18:14:48.106 - INFO: Train epoch 105:   Loss: 2419.7932 | r_Loss: 320.1022 | g_Loss: 792.2415 | l_Loss: 27.0408 | 
                                                                                 105		2419.7932		-4.5000
25-09-27 18:18:05.237 - INFO: Learning rate: 3.1622776601683795e-05
25-09-27 18:18:05.238 - INFO: Train epoch 106:   Loss: 1713.3644 | r_Loss: 186.0543 | g_Loss: 768.1793 | l_Loss: 14.9138 | 
                                                                                 106		1713.3644		-4.5000
25-09-27 18:21:03.410 - INFO: Learning rate: 3.1622776601683795e-05
25-09-27 18:21:03.411 - INFO: Train epoch 107:   Loss: 1394.2827 | r_Loss: 166.5246 | g_Loss: 548.7180 | l_Loss: 12.9415 | 
                                                                                 107		1394.2827		-4.5000
25-09-27 18:23:54.923 - INFO: Learning rate: 3.1622776601683795e-05
25-09-27 18:23:54.924 - INFO: Train epoch 108:   Loss: 1345.5903 | r_Loss: 162.0048 | g_Loss: 522.8652 | l_Loss: 12.7014 | 
                                                                                 108		1345.5903		-4.5000
25-09-27 18:27:30.751 - INFO: Learning rate: 3.1622776601683795e-05
25-09-27 18:27:30.751 - INFO: Train epoch 109:   Loss: 6184.2103 | r_Loss: 902.8461 | g_Loss: 1606.8782 | l_Loss: 63.1014 | 
                                                                                 109		6184.2103		-4.5000
25-09-27 18:30:23.272 - INFO: Learning rate: 3.1622776601683795e-05
25-09-27 18:30:23.273 - INFO: Train epoch 110:   Loss: 3481.7603 | r_Loss: 368.4596 | g_Loss: 1606.1831 | l_Loss: 33.2793 | 
                                                                                 110		3481.7603		-4.5000
25-09-27 18:33:13.522 - INFO: Learning rate: 3.1622776601683795e-05
25-09-27 18:33:13.522 - INFO: Train epoch 111:   Loss: 1907.6515 | r_Loss: 204.8144 | g_Loss: 867.1445 | l_Loss: 16.4351 | 
                                                                                 111		1907.6515		-4.5000
25-09-27 18:36:04.120 - INFO: Learning rate: 3.1622776601683795e-05
25-09-27 18:36:04.121 - INFO: Train epoch 112:   Loss: 5115.2998 | r_Loss: 783.2423 | g_Loss: 1124.7909 | l_Loss: 74.2971 | 
                                                                                 112		5115.2998		-4.5000
25-09-27 18:38:55.245 - INFO: Learning rate: 3.1622776601683795e-05
25-09-27 18:38:55.245 - INFO: Train epoch 113:   Loss: 3292.4191 | r_Loss: 295.7808 | g_Loss: 1757.8728 | l_Loss: 55.6425 | 
                                                                                 113		3292.4191		-4.5000
25-09-27 18:41:45.781 - INFO: Learning rate: 3.1622776601683795e-05
25-09-27 18:41:45.783 - INFO: Train epoch 114:   Loss: 1953.4802 | r_Loss: 220.6969 | g_Loss: 832.0210 | l_Loss: 17.9746 | 
                                                                                 114		1953.4802		-4.5000
25-09-27 18:44:35.592 - INFO: Learning rate: 3.1622776601683795e-05
25-09-27 18:44:35.593 - INFO: Train epoch 115:   Loss: 1618.2395 | r_Loss: 188.1465 | g_Loss: 663.6256 | l_Loss: 13.8817 | 
                                                                                 115		1618.2395		-4.5000
25-09-27 18:47:25.955 - INFO: Learning rate: 3.1622776601683795e-05
25-09-27 18:47:25.956 - INFO: Train epoch 116:   Loss: 1483.3737 | r_Loss: 175.5109 | g_Loss: 593.5906 | l_Loss: 12.2287 | 
                                                                                 116		1483.3737		-4.5000
25-09-27 18:50:33.524 - INFO: Learning rate: 3.1622776601683795e-05
25-09-27 18:50:33.525 - INFO: Train epoch 117:   Loss: 1345.6243 | r_Loss: 160.2457 | g_Loss: 533.4829 | l_Loss: 10.9131 | 
                                                                                 117		1345.6243		-4.5000
25-09-27 18:53:33.612 - INFO: Learning rate: 3.1622776601683795e-05
25-09-27 18:53:33.613 - INFO: Train epoch 118:   Loss: 1269.9910 | r_Loss: 152.5562 | g_Loss: 496.9099 | l_Loss: 10.3002 | 
                                                                                 118		1269.9910		-4.5000
25-09-27 18:56:46.141 - INFO: Learning rate: 3.1622776601683795e-05
25-09-27 18:56:46.142 - INFO: Train epoch 119:   Loss: 1244.5831 | r_Loss: 149.3027 | g_Loss: 487.5985 | l_Loss: 10.4710 | 
                                                                                 119		1244.5831		-4.5000
25-09-27 18:59:46.101 - INFO: Learning rate: 3.1622776601683795e-05
25-09-27 18:59:46.101 - INFO: Train epoch 120:   Loss: 1202.5014 | r_Loss: 144.1753 | g_Loss: 471.0939 | l_Loss: 10.5311 | 
                                                                                 120		1202.5014		-4.5000
25-09-27 19:02:42.757 - INFO: Learning rate: 3.1622776601683795e-05
25-09-27 19:02:42.758 - INFO: Train epoch 121:   Loss: 1153.1323 | r_Loss: 138.1137 | g_Loss: 452.7811 | l_Loss: 9.7827 | 
                                                                                 121		1153.1323		-4.5000
25-09-27 19:05:34.372 - INFO: Learning rate: 3.1622776601683795e-05
25-09-27 19:05:34.373 - INFO: Train epoch 122:   Loss: 1165.9230 | r_Loss: 140.4377 | g_Loss: 453.6595 | l_Loss: 10.0750 | 
                                                                                 122		1165.9230		-4.5000
25-09-27 19:08:27.323 - INFO: Learning rate: 3.1622776601683795e-05
25-09-27 19:08:27.324 - INFO: Train epoch 123:   Loss: 5953.8933 | r_Loss: 862.8728 | g_Loss: 1574.4674 | l_Loss: 65.0619 | 
                                                                                 123		5953.8933		-4.5000
25-09-27 19:11:18.009 - INFO: Learning rate: 3.1622776601683795e-05
25-09-27 19:11:18.010 - INFO: Train epoch 124:   Loss: 2017.0329 | r_Loss: 225.2447 | g_Loss: 874.6666 | l_Loss: 16.1426 | 
                                                                                 124		2017.0329		-4.5000
25-09-27 19:14:10.340 - INFO: Learning rate: 3.1622776601683795e-05
25-09-27 19:14:10.341 - INFO: Train epoch 125:   Loss: 1547.6246 | r_Loss: 180.9451 | g_Loss: 630.5048 | l_Loss: 12.3945 | 
                                                                                 125		1547.6246		-4.5000
25-09-27 19:17:28.787 - INFO: Learning rate: 3.1622776601683795e-05
25-09-27 19:17:28.788 - INFO: Train epoch 126:   Loss: 1316.4833 | r_Loss: 152.1373 | g_Loss: 545.1676 | l_Loss: 10.6291 | 
                                                                                 126		1316.4833		-4.5000
25-09-27 19:20:27.356 - INFO: Learning rate: 3.1622776601683795e-05
25-09-27 19:20:27.357 - INFO: Train epoch 127:   Loss: 1202.0572 | r_Loss: 141.3416 | g_Loss: 485.3072 | l_Loss: 10.0421 | 
                                                                                 127		1202.0572		-4.5000
25-09-27 19:23:43.307 - INFO: Learning rate: 3.1622776601683795e-05
25-09-27 19:23:43.308 - INFO: Train epoch 128:   Loss: 1134.8574 | r_Loss: 133.2148 | g_Loss: 459.4560 | l_Loss: 9.3274 | 
                                                                                 128		1134.8574		-4.5000
25-09-27 19:26:37.795 - INFO: Learning rate: 3.1622776601683795e-05
25-09-27 19:26:37.796 - INFO: Train epoch 129:   Loss: 2394.6572 | r_Loss: 286.1585 | g_Loss: 926.9575 | l_Loss: 36.9074 | 
                                                                                 129		2394.6572		-4.5000
25-09-27 19:29:28.390 - INFO: Learning rate: 3.1622776601683795e-05
25-09-27 19:29:28.391 - INFO: Train epoch 130:   Loss: 1298.2280 | r_Loss: 142.9552 | g_Loss: 572.5603 | l_Loss: 10.8919 | 
                                                                                 130		1298.2280		-4.5000
25-09-27 19:32:18.547 - INFO: Learning rate: 3.1622776601683795e-05
25-09-27 19:32:18.548 - INFO: Train epoch 131:   Loss: 1106.7342 | r_Loss: 127.7891 | g_Loss: 458.6425 | l_Loss: 9.1463 | 
                                                                                 131		1106.7342		-4.5000
25-09-27 19:35:08.713 - INFO: Learning rate: 3.1622776601683795e-05
25-09-27 19:35:08.714 - INFO: Train epoch 132:   Loss: 1045.1789 | r_Loss: 122.4044 | g_Loss: 424.7140 | l_Loss: 8.4427 | 
                                                                                 132		1045.1789		-4.5000
25-09-27 19:37:58.751 - INFO: Learning rate: 3.1622776601683795e-05
25-09-27 19:37:58.753 - INFO: Train epoch 133:   Loss: 1036.6065 | r_Loss: 122.1217 | g_Loss: 417.6218 | l_Loss: 8.3761 | 
                                                                                 133		1036.6065		-4.5000
25-09-27 19:40:48.867 - INFO: Learning rate: 3.1622776601683795e-05
25-09-27 19:40:48.868 - INFO: Train epoch 134:   Loss: 1003.5745 | r_Loss: 118.3779 | g_Loss: 403.4138 | l_Loss: 8.2714 | 
                                                                                 134		1003.5745		-4.5000
25-09-27 19:43:40.756 - INFO: Learning rate: 3.1622776601683795e-05
25-09-27 19:43:40.757 - INFO: Train epoch 135:   Loss: 975.6221 | r_Loss: 115.0424 | g_Loss: 392.3310 | l_Loss: 8.0790 | 
                                                                                 135		975.6221		-4.5000
25-09-27 19:46:32.790 - INFO: Learning rate: 3.1622776601683795e-05
25-09-27 19:46:32.791 - INFO: Train epoch 136:   Loss: 2644.3347 | r_Loss: 333.2882 | g_Loss: 937.4561 | l_Loss: 40.4375 | 
                                                                                 136		2644.3347		-4.5000
25-09-27 19:49:24.590 - INFO: Learning rate: 3.1622776601683795e-05
25-09-27 19:49:24.590 - INFO: Train epoch 137:   Loss: 1130.3177 | r_Loss: 123.6155 | g_Loss: 502.1646 | l_Loss: 10.0755 | 
                                                                                 137		1130.3177		-4.5000
25-09-27 19:52:16.719 - INFO: Learning rate: 3.1622776601683795e-05
25-09-27 19:52:16.720 - INFO: Train epoch 138:   Loss: 963.1350 | r_Loss: 112.6978 | g_Loss: 391.6805 | l_Loss: 7.9653 | 
                                                                                 138		963.1350		-4.5000
25-09-27 19:55:09.382 - INFO: Learning rate: 3.1622776601683795e-05
25-09-27 19:55:09.383 - INFO: Train epoch 139:   Loss: 3681.5803 | r_Loss: 448.5287 | g_Loss: 1383.9946 | l_Loss: 54.9420 | 
                                                                                 139		3681.5803		-4.5000
25-09-27 19:58:00.886 - INFO: Learning rate: 3.1622776601683795e-05
25-09-27 19:58:00.887 - INFO: Train epoch 140:   Loss: 1177.7786 | r_Loss: 124.8685 | g_Loss: 543.2064 | l_Loss: 10.2296 | 
                                                                                 140		1177.7786		-4.5000
25-09-27 20:00:52.092 - INFO: Learning rate: 3.1622776601683795e-05
25-09-27 20:00:52.093 - INFO: Train epoch 141:   Loss: 1001.3958 | r_Loss: 113.8786 | g_Loss: 423.3552 | l_Loss: 8.6478 | 
                                                                                 141		1001.3958		-4.5000
25-09-27 20:03:44.217 - INFO: Learning rate: 3.1622776601683795e-05
25-09-27 20:03:44.218 - INFO: Train epoch 142:   Loss: 967.5823 | r_Loss: 112.4946 | g_Loss: 396.6940 | l_Loss: 8.4156 | 
                                                                                 142		967.5823		-4.5000
25-09-27 20:06:34.980 - INFO: Learning rate: 3.1622776601683795e-05
25-09-27 20:06:34.980 - INFO: Train epoch 143:   Loss: 10616.8988 | r_Loss: 1345.3735 | g_Loss: 3481.4871 | l_Loss: 408.5442 | 
                                                                                 143		10616.8988		-4.5000
25-09-27 20:09:26.017 - INFO: Learning rate: 3.1622776601683795e-05
25-09-27 20:09:26.018 - INFO: Train epoch 144:   Loss: 2122.5049 | r_Loss: 215.2217 | g_Loss: 1019.4492 | l_Loss: 26.9474 | 
                                                                                 144		2122.5049		-4.5000
25-09-27 20:12:17.517 - INFO: Learning rate: 3.1622776601683795e-05
25-09-27 20:12:17.517 - INFO: Train epoch 145:   Loss: 1562.0097 | r_Loss: 170.1858 | g_Loss: 692.6600 | l_Loss: 18.4208 | 
                                                                                 145		1562.0097		-4.5000
25-09-27 20:15:08.507 - INFO: Learning rate: 3.1622776601683795e-05
25-09-27 20:15:08.507 - INFO: Train epoch 146:   Loss: 1184.9928 | r_Loss: 127.7103 | g_Loss: 532.6918 | l_Loss: 13.7496 | 
                                                                                 146		1184.9928		-4.5000
25-09-27 20:18:00.038 - INFO: Learning rate: 3.1622776601683795e-05
25-09-27 20:18:00.039 - INFO: Train epoch 147:   Loss: 1074.9828 | r_Loss: 120.7483 | g_Loss: 459.5133 | l_Loss: 11.7280 | 
                                                                                 147		1074.9828		-4.5000
25-09-27 20:20:51.474 - INFO: Learning rate: 3.1622776601683795e-05
25-09-27 20:20:51.474 - INFO: Train epoch 148:   Loss: 996.1099 | r_Loss: 113.1382 | g_Loss: 419.1368 | l_Loss: 11.2819 | 
                                                                                 148		996.1099		-4.5000
25-09-27 20:23:43.262 - INFO: Learning rate: 3.1622776601683795e-05
25-09-27 20:23:43.263 - INFO: Train epoch 149:   Loss: 941.5101 | r_Loss: 106.9583 | g_Loss: 396.3230 | l_Loss: 10.3957 | 
                                                                                 149		941.5101		-4.5000
25-09-27 20:27:31.167 - INFO: TEST:   PSNR_S: 45.9227 | PSNR_C: 35.6844 | 
25-09-27 20:27:31.169 - INFO: Learning rate: 3.1622776601683795e-05
25-09-27 20:27:31.170 - INFO: Train epoch 150:   Loss: 921.6472 | r_Loss: 106.2356 | g_Loss: 380.1708 | l_Loss: 10.2983 | 
                                                                                 150		921.6472		-4.5000
25-09-27 20:30:22.401 - INFO: Learning rate: 3.1622776601683795e-05
25-09-27 20:30:22.402 - INFO: Train epoch 151:   Loss: 928.7370 | r_Loss: 108.9289 | g_Loss: 371.1406 | l_Loss: 12.9518 | 
                                                                                 151		928.7370		-4.5000
25-09-27 20:33:13.566 - INFO: Learning rate: 3.1622776601683795e-05
25-09-27 20:33:13.567 - INFO: Train epoch 152:   Loss: 1685.1932 | r_Loss: 194.2194 | g_Loss: 677.8984 | l_Loss: 36.1977 | 
                                                                                 152		1685.1932		-4.5000
25-09-27 20:36:05.077 - INFO: Learning rate: 3.1622776601683795e-05
25-09-27 20:36:05.078 - INFO: Train epoch 153:   Loss: 888.3319 | r_Loss: 100.8471 | g_Loss: 376.3454 | l_Loss: 7.7512 | 
                                                                                 153		888.3319		-4.5000
25-09-27 20:38:55.931 - INFO: Learning rate: 3.1622776601683795e-05
25-09-27 20:38:55.932 - INFO: Train epoch 154:   Loss: 843.0683 | r_Loss: 97.7089 | g_Loss: 347.2276 | l_Loss: 7.2960 | 
                                                                                 154		843.0683		-4.5000
25-09-27 20:41:47.506 - INFO: Learning rate: 3.1622776601683795e-05
25-09-27 20:41:47.507 - INFO: Train epoch 155:   Loss: 852.3614 | r_Loss: 98.7710 | g_Loss: 350.0465 | l_Loss: 8.4601 | 
                                                                                 155		852.3614		-4.5000
25-09-27 20:44:39.059 - INFO: Learning rate: 3.1622776601683795e-05
25-09-27 20:44:39.060 - INFO: Train epoch 156:   Loss: 822.0817 | r_Loss: 95.6405 | g_Loss: 335.9804 | l_Loss: 7.8987 | 
                                                                                 156		822.0817		-4.5000
25-09-27 20:47:31.009 - INFO: Learning rate: 3.1622776601683795e-05
25-09-27 20:47:31.010 - INFO: Train epoch 157:   Loss: 2594.5645 | r_Loss: 299.8957 | g_Loss: 1042.7923 | l_Loss: 52.2935 | 
                                                                                 157		2594.5645		-4.5000
25-09-27 20:50:22.936 - INFO: Learning rate: 3.1622776601683795e-05
25-09-27 20:50:22.937 - INFO: Train epoch 158:   Loss: 918.5789 | r_Loss: 95.5376 | g_Loss: 432.6376 | l_Loss: 8.2534 | 
                                                                                 158		918.5789		-4.5000
25-09-27 20:53:14.578 - INFO: Learning rate: 3.1622776601683795e-05
25-09-27 20:53:14.579 - INFO: Train epoch 159:   Loss: 846.1402 | r_Loss: 96.5985 | g_Loss: 356.0740 | l_Loss: 7.0737 | 
                                                                                 159		846.1402		-4.5000
25-09-27 20:56:05.607 - INFO: Learning rate: 3.1622776601683795e-05
25-09-27 20:56:05.607 - INFO: Train epoch 160:   Loss: 833.9925 | r_Loss: 96.5784 | g_Loss: 344.1610 | l_Loss: 6.9396 | 
                                                                                 160		833.9925		-4.5000
25-09-27 20:58:56.945 - INFO: Learning rate: 3.1622776601683795e-05
25-09-27 20:58:56.946 - INFO: Train epoch 161:   Loss: 782.0542 | r_Loss: 90.3248 | g_Loss: 323.7794 | l_Loss: 6.6508 | 
                                                                                 161		782.0542		-4.5000
25-09-27 21:01:48.407 - INFO: Learning rate: 3.1622776601683795e-05
25-09-27 21:01:48.408 - INFO: Train epoch 162:   Loss: 3723.5773 | r_Loss: 486.5788 | g_Loss: 1227.0659 | l_Loss: 63.6176 | 
                                                                                 162		3723.5773		-4.5000
25-09-27 21:04:39.815 - INFO: Learning rate: 3.1622776601683795e-05
25-09-27 21:04:39.816 - INFO: Train epoch 163:   Loss: 1288.4307 | r_Loss: 131.3161 | g_Loss: 619.3701 | l_Loss: 12.4799 | 
                                                                                 163		1288.4307		-4.5000
25-09-27 21:07:31.199 - INFO: Learning rate: 3.1622776601683795e-05
25-09-27 21:07:31.200 - INFO: Train epoch 164:   Loss: 942.1599 | r_Loss: 103.7519 | g_Loss: 414.3864 | l_Loss: 9.0141 | 
                                                                                 164		942.1599		-4.5000
25-09-27 21:10:23.008 - INFO: Learning rate: 3.1622776601683795e-05
25-09-27 21:10:23.008 - INFO: Train epoch 165:   Loss: 845.1125 | r_Loss: 96.1475 | g_Loss: 356.4069 | l_Loss: 7.9684 | 
                                                                                 165		845.1125		-4.5000
25-09-27 21:13:14.311 - INFO: Learning rate: 3.1622776601683795e-05
25-09-27 21:13:14.312 - INFO: Train epoch 166:   Loss: 984.6886 | r_Loss: 116.7789 | g_Loss: 390.9595 | l_Loss: 9.8343 | 
                                                                                 166		984.6886		-4.5000
25-09-27 21:16:06.231 - INFO: Learning rate: 3.1622776601683795e-05
25-09-27 21:16:06.232 - INFO: Train epoch 167:   Loss: 767.1806 | r_Loss: 83.1077 | g_Loss: 344.6548 | l_Loss: 6.9873 | 
                                                                                 167		767.1806		-4.5000
25-09-27 21:18:57.757 - INFO: Learning rate: 3.1622776601683795e-05
25-09-27 21:18:57.757 - INFO: Train epoch 168:   Loss: 1086.3244 | r_Loss: 130.5476 | g_Loss: 423.4493 | l_Loss: 10.1372 | 
                                                                                 168		1086.3244		-4.5000
25-09-27 21:21:49.245 - INFO: Learning rate: 3.1622776601683795e-05
25-09-27 21:21:49.245 - INFO: Train epoch 169:   Loss: 758.8964 | r_Loss: 82.6093 | g_Loss: 339.3524 | l_Loss: 6.4976 | 
                                                                                 169		758.8964		-4.5000
25-09-27 21:24:41.111 - INFO: Learning rate: 3.1622776601683795e-05
25-09-27 21:24:41.112 - INFO: Train epoch 170:   Loss: 744.8875 | r_Loss: 86.1457 | g_Loss: 307.8642 | l_Loss: 6.2950 | 
                                                                                 170		744.8875		-4.5000
25-09-27 21:27:32.398 - INFO: Learning rate: 3.1622776601683795e-05
25-09-27 21:27:32.398 - INFO: Train epoch 171:   Loss: 740.6249 | r_Loss: 85.7942 | g_Loss: 303.9193 | l_Loss: 7.7345 | 
                                                                                 171		740.6249		-4.5000
25-09-27 21:30:23.688 - INFO: Learning rate: 3.1622776601683795e-05
25-09-27 21:30:23.689 - INFO: Train epoch 172:   Loss: 12734.8153 | r_Loss: 1943.3163 | g_Loss: 2785.8163 | l_Loss: 232.4171 | 
                                                                                 172		12734.8153		-4.5000
25-09-27 21:33:14.957 - INFO: Learning rate: 3.1622776601683795e-05
25-09-27 21:33:14.958 - INFO: Train epoch 173:   Loss: 2156.4950 | r_Loss: 195.6145 | g_Loss: 1146.0949 | l_Loss: 32.3277 | 
                                                                                 173		2156.4950		-4.5000
25-09-27 21:36:06.465 - INFO: Learning rate: 3.1622776601683795e-05
25-09-27 21:36:06.466 - INFO: Train epoch 174:   Loss: 1618.6998 | r_Loss: 156.4458 | g_Loss: 815.1550 | l_Loss: 21.3158 | 
                                                                                 174		1618.6998		-4.5000
25-09-27 21:38:57.944 - INFO: Learning rate: 3.1622776601683795e-05
25-09-27 21:38:57.945 - INFO: Train epoch 175:   Loss: 1312.7218 | r_Loss: 128.3186 | g_Loss: 654.4255 | l_Loss: 16.7033 | 
                                                                                 175		1312.7218		-4.5000
25-09-27 21:41:49.132 - INFO: Learning rate: 3.1622776601683795e-05
25-09-27 21:41:49.132 - INFO: Train epoch 176:   Loss: 1171.1209 | r_Loss: 120.4640 | g_Loss: 554.2426 | l_Loss: 14.5583 | 
                                                                                 176		1171.1209		-4.5000
25-09-27 21:44:40.194 - INFO: Learning rate: 3.1622776601683795e-05
25-09-27 21:44:40.194 - INFO: Train epoch 177:   Loss: 6788.1050 | r_Loss: 1039.7210 | g_Loss: 1425.6718 | l_Loss: 163.8285 | 
                                                                                 177		6788.1050		-4.5000
25-09-27 21:47:31.306 - INFO: Learning rate: 3.1622776601683795e-05
25-09-27 21:47:31.307 - INFO: Train epoch 178:   Loss: 2819.4996 | r_Loss: 228.1035 | g_Loss: 1636.9509 | l_Loss: 42.0310 | 
                                                                                 178		2819.4996		-4.5000
25-09-27 21:50:23.376 - INFO: Learning rate: 3.1622776601683795e-05
25-09-27 21:50:23.377 - INFO: Train epoch 179:   Loss: 1529.2960 | r_Loss: 133.4905 | g_Loss: 843.8300 | l_Loss: 18.0138 | 
                                                                                 179		1529.2960		-4.5000
25-09-27 21:53:15.682 - INFO: Learning rate: 3.1622776601683795e-05
25-09-27 21:53:15.682 - INFO: Train epoch 180:   Loss: 1243.5931 | r_Loss: 120.0914 | g_Loss: 628.4832 | l_Loss: 14.6531 | 
                                                                                 180		1243.5931		-4.5000
25-09-27 21:56:07.223 - INFO: Learning rate: 3.1622776601683795e-05
25-09-27 21:56:07.223 - INFO: Train epoch 181:   Loss: 1048.1610 | r_Loss: 105.0416 | g_Loss: 510.7493 | l_Loss: 12.2037 | 
                                                                                 181		1048.1610		-4.5000
25-09-27 21:58:58.231 - INFO: Learning rate: 3.1622776601683795e-05
25-09-27 21:58:58.232 - INFO: Train epoch 182:   Loss: 959.7018 | r_Loss: 100.3340 | g_Loss: 447.0819 | l_Loss: 10.9498 | 
                                                                                 182		959.7018		-4.5000
25-09-27 22:01:49.663 - INFO: Learning rate: 3.1622776601683795e-05
25-09-27 22:01:49.663 - INFO: Train epoch 183:   Loss: 893.8564 | r_Loss: 94.9792 | g_Loss: 409.0302 | l_Loss: 9.9301 | 
                                                                                 183		893.8564		-4.5000
25-09-27 22:04:41.172 - INFO: Learning rate: 3.1622776601683795e-05
25-09-27 22:04:41.173 - INFO: Train epoch 184:   Loss: 882.3647 | r_Loss: 96.8589 | g_Loss: 388.7914 | l_Loss: 9.2788 | 
                                                                                 184		882.3647		-4.5000
25-09-27 22:07:31.946 - INFO: Learning rate: 3.1622776601683795e-05
25-09-27 22:07:31.947 - INFO: Train epoch 185:   Loss: 833.3531 | r_Loss: 92.4069 | g_Loss: 362.6903 | l_Loss: 8.6281 | 
                                                                                 185		833.3531		-4.5000
25-09-27 22:10:22.628 - INFO: Learning rate: 3.1622776601683795e-05
25-09-27 22:10:22.629 - INFO: Train epoch 186:   Loss: 804.2687 | r_Loss: 89.8168 | g_Loss: 346.9922 | l_Loss: 8.1927 | 
                                                                                 186		804.2687		-4.5000
25-09-27 22:13:14.844 - INFO: Learning rate: 3.1622776601683795e-05
25-09-27 22:13:14.844 - INFO: Train epoch 187:   Loss: 768.7544 | r_Loss: 85.2965 | g_Loss: 334.5478 | l_Loss: 7.7243 | 
                                                                                 187		768.7544		-4.5000
25-09-27 22:16:06.147 - INFO: Learning rate: 3.1622776601683795e-05
25-09-27 22:16:06.148 - INFO: Train epoch 188:   Loss: 777.2832 | r_Loss: 88.3495 | g_Loss: 327.7716 | l_Loss: 7.7643 | 
                                                                                 188		777.2832		-4.5000
25-09-27 22:18:57.316 - INFO: Learning rate: 3.1622776601683795e-05
25-09-27 22:18:57.317 - INFO: Train epoch 189:   Loss: 752.6728 | r_Loss: 85.8568 | g_Loss: 315.9245 | l_Loss: 7.4645 | 
                                                                                 189		752.6728		-4.5000
25-09-27 22:21:48.742 - INFO: Learning rate: 3.1622776601683795e-05
25-09-27 22:21:48.743 - INFO: Train epoch 190:   Loss: 17365.7118 | r_Loss: 2641.3155 | g_Loss: 3642.4790 | l_Loss: 516.6548 | 
                                                                                 190		17365.7118		-4.5000
25-09-27 22:24:40.459 - INFO: Learning rate: 3.1622776601683795e-05
25-09-27 22:24:40.460 - INFO: Train epoch 191:   Loss: 2939.3809 | r_Loss: 315.3485 | g_Loss: 1319.1431 | l_Loss: 43.4954 | 
                                                                                 191		2939.3809		-4.5000
25-09-27 22:27:32.000 - INFO: Learning rate: 3.1622776601683795e-05
25-09-27 22:27:32.001 - INFO: Train epoch 192:   Loss: 2365.7222 | r_Loss: 267.4495 | g_Loss: 999.1044 | l_Loss: 29.3703 | 
                                                                                 192		2365.7222		-4.5000
25-09-27 22:30:24.028 - INFO: Learning rate: 3.1622776601683795e-05
25-09-27 22:30:24.029 - INFO: Train epoch 193:   Loss: 1749.3282 | r_Loss: 185.8483 | g_Loss: 799.4245 | l_Loss: 20.6622 | 
                                                                                 193		1749.3282		-4.5000
25-09-27 22:33:15.712 - INFO: Learning rate: 3.1622776601683795e-05
25-09-27 22:33:15.713 - INFO: Train epoch 194:   Loss: 1552.3803 | r_Loss: 171.0349 | g_Loss: 679.0602 | l_Loss: 18.1456 | 
                                                                                 194		1552.3803		-4.5000
25-09-27 22:36:07.140 - INFO: Learning rate: 3.1622776601683795e-05
25-09-27 22:36:07.140 - INFO: Train epoch 195:   Loss: 1345.6007 | r_Loss: 148.7979 | g_Loss: 586.1330 | l_Loss: 15.4784 | 
                                                                                 195		1345.6007		-4.5000
25-09-27 22:38:59.054 - INFO: Learning rate: 3.1622776601683795e-05
25-09-27 22:38:59.055 - INFO: Train epoch 196:   Loss: 1122.6437 | r_Loss: 120.0675 | g_Loss: 509.2449 | l_Loss: 13.0610 | 
                                                                                 196		1122.6437		-4.5000
25-09-27 22:41:50.818 - INFO: Learning rate: 3.1622776601683795e-05
25-09-27 22:41:50.819 - INFO: Train epoch 197:   Loss: 1072.2469 | r_Loss: 118.6451 | g_Loss: 467.3211 | l_Loss: 11.7005 | 
                                                                                 197		1072.2469		-4.5000
25-09-27 22:44:44.472 - INFO: Learning rate: 3.1622776601683795e-05
25-09-27 22:44:44.473 - INFO: Train epoch 198:   Loss: 1019.6512 | r_Loss: 113.1635 | g_Loss: 441.8523 | l_Loss: 11.9815 | 
                                                                                 198		1019.6512		-4.5000
25-09-27 22:47:36.462 - INFO: Learning rate: 3.1622776601683795e-05
25-09-27 22:47:36.463 - INFO: Train epoch 199:   Loss: 945.6435 | r_Loss: 107.3759 | g_Loss: 398.8491 | l_Loss: 9.9148 | 
                                                                                 199		945.6435		-4.5000
25-09-27 22:51:25.545 - INFO: TEST:   PSNR_S: 45.7783 | PSNR_C: 35.0034 | 
25-09-27 22:51:25.547 - INFO: Learning rate: 3.1622776601683795e-05
25-09-27 22:51:25.547 - INFO: Train epoch 200:   Loss: 857.2195 | r_Loss: 94.1869 | g_Loss: 377.3185 | l_Loss: 8.9664 | 
                                                                                 200		857.2195		-4.5000
25-09-27 22:54:16.829 - INFO: Learning rate: 3.1622776601683795e-05
25-09-27 22:54:16.830 - INFO: Train epoch 201:   Loss: 851.5691 | r_Loss: 96.8022 | g_Loss: 358.7258 | l_Loss: 8.8322 | 
                                                                                 201		851.5691		-4.5000
25-09-27 22:57:08.347 - INFO: Learning rate: 3.1622776601683795e-05
25-09-27 22:57:08.348 - INFO: Train epoch 202:   Loss: 825.0103 | r_Loss: 92.8391 | g_Loss: 351.5238 | l_Loss: 9.2908 | 
                                                                                 202		825.0103		-4.5000
25-09-27 22:59:59.382 - INFO: Learning rate: 3.1622776601683795e-05
25-09-27 22:59:59.383 - INFO: Train epoch 203:   Loss: 805.6964 | r_Loss: 91.0803 | g_Loss: 339.3686 | l_Loss: 10.9263 | 
                                                                                 203		805.6964		-4.5000
25-09-27 23:02:51.390 - INFO: Learning rate: 3.1622776601683795e-05
25-09-27 23:02:51.390 - INFO: Train epoch 204:   Loss: 829.8631 | r_Loss: 94.9431 | g_Loss: 343.1883 | l_Loss: 11.9595 | 
                                                                                 204		829.8631		-4.5000
25-09-27 23:05:42.859 - INFO: Learning rate: 3.1622776601683795e-05
25-09-27 23:05:42.860 - INFO: Train epoch 205:   Loss: 755.2401 | r_Loss: 85.9345 | g_Loss: 317.4805 | l_Loss: 8.0868 | 
                                                                                 205		755.2401		-4.5000
25-09-27 23:08:34.242 - INFO: Learning rate: 3.1622776601683795e-05
25-09-27 23:08:34.242 - INFO: Train epoch 206:   Loss: 773.8278 | r_Loss: 87.9083 | g_Loss: 324.0719 | l_Loss: 10.2145 | 
                                                                                 206		773.8278		-4.5000
25-09-27 23:11:25.875 - INFO: Learning rate: 3.1622776601683795e-05
25-09-27 23:11:25.875 - INFO: Train epoch 207:   Loss: 768.2328 | r_Loss: 87.7205 | g_Loss: 317.8339 | l_Loss: 11.7961 | 
                                                                                 207		768.2328		-4.5000
25-09-27 23:14:17.734 - INFO: Learning rate: 3.1622776601683795e-05
25-09-27 23:14:17.734 - INFO: Train epoch 208:   Loss: 3558.1337 | r_Loss: 481.2004 | g_Loss: 1044.2676 | l_Loss: 107.8640 | 
                                                                                 208		3558.1337		-4.5000
25-09-27 23:17:08.781 - INFO: Learning rate: 3.1622776601683795e-05
25-09-27 23:17:08.781 - INFO: Train epoch 209:   Loss: 1549.6026 | r_Loss: 138.1299 | g_Loss: 836.0743 | l_Loss: 22.8785 | 
                                                                                 209		1549.6026		-4.5000
25-09-27 23:19:59.915 - INFO: Learning rate: 3.1622776601683795e-05
25-09-27 23:19:59.916 - INFO: Train epoch 210:   Loss: 1035.7109 | r_Loss: 102.5712 | g_Loss: 509.8023 | l_Loss: 13.0525 | 
                                                                                 210		1035.7109		-4.5000
25-09-27 23:22:51.001 - INFO: Learning rate: 3.1622776601683795e-05
25-09-27 23:22:51.002 - INFO: Train epoch 211:   Loss: 875.7981 | r_Loss: 92.7029 | g_Loss: 401.9858 | l_Loss: 10.2979 | 
                                                                                 211		875.7981		-4.5000
25-09-27 23:25:42.851 - INFO: Learning rate: 3.1622776601683795e-05
25-09-27 23:25:42.852 - INFO: Train epoch 212:   Loss: 780.7194 | r_Loss: 85.6372 | g_Loss: 343.7959 | l_Loss: 8.7373 | 
                                                                                 212		780.7194		-4.5000
25-09-27 23:28:34.685 - INFO: Learning rate: 3.1622776601683795e-05
25-09-27 23:28:34.685 - INFO: Train epoch 213:   Loss: 759.0965 | r_Loss: 86.2082 | g_Loss: 320.1853 | l_Loss: 7.8702 | 
                                                                                 213		759.0965		-4.5000
25-09-27 23:31:26.271 - INFO: Learning rate: 3.1622776601683795e-05
25-09-27 23:31:26.272 - INFO: Train epoch 214:   Loss: 706.2334 | r_Loss: 78.7642 | g_Loss: 304.8912 | l_Loss: 7.5213 | 
                                                                                 214		706.2334		-4.5000
25-09-27 23:34:17.237 - INFO: Learning rate: 3.1622776601683795e-05
25-09-27 23:34:17.237 - INFO: Train epoch 215:   Loss: 704.6556 | r_Loss: 80.6190 | g_Loss: 293.7541 | l_Loss: 7.8064 | 
                                                                                 215		704.6556		-4.5000
25-09-27 23:37:08.360 - INFO: Learning rate: 3.1622776601683795e-05
25-09-27 23:37:08.360 - INFO: Train epoch 216:   Loss: 695.4823 | r_Loss: 79.3240 | g_Loss: 289.6119 | l_Loss: 9.2504 | 
                                                                                 216		695.4823		-4.5000
25-09-27 23:39:59.430 - INFO: Learning rate: 3.1622776601683795e-05
25-09-27 23:39:59.431 - INFO: Train epoch 217:   Loss: 691.4570 | r_Loss: 77.7710 | g_Loss: 288.3110 | l_Loss: 14.2908 | 
                                                                                 217		691.4570		-4.5000
25-09-27 23:42:51.298 - INFO: Learning rate: 3.1622776601683795e-05
25-09-27 23:42:51.298 - INFO: Train epoch 218:   Loss: 3036.3444 | r_Loss: 380.1690 | g_Loss: 1023.3736 | l_Loss: 112.1255 | 
                                                                                 218		3036.3444		-4.5000
25-09-27 23:45:42.580 - INFO: Learning rate: 3.1622776601683795e-05
25-09-27 23:45:42.581 - INFO: Train epoch 219:   Loss: 960.3438 | r_Loss: 86.1405 | g_Loss: 517.7094 | l_Loss: 11.9320 | 
                                                                                 219		960.3438		-4.5000
25-09-27 23:48:33.590 - INFO: Learning rate: 3.1622776601683795e-05
25-09-27 23:48:33.591 - INFO: Train epoch 220:   Loss: 731.2172 | r_Loss: 78.0330 | g_Loss: 333.1614 | l_Loss: 7.8906 | 
                                                                                 220		731.2172		-4.5000
25-09-27 23:51:25.465 - INFO: Learning rate: 3.1622776601683795e-05
25-09-27 23:51:25.466 - INFO: Train epoch 221:   Loss: 717.2217 | r_Loss: 80.8516 | g_Loss: 305.9372 | l_Loss: 7.0268 | 
                                                                                 221		717.2217		-4.5000
25-09-27 23:54:16.912 - INFO: Learning rate: 3.1622776601683795e-05
25-09-27 23:54:16.913 - INFO: Train epoch 222:   Loss: 695.5059 | r_Loss: 79.7957 | g_Loss: 288.1915 | l_Loss: 8.3361 | 
                                                                                 222		695.5059		-4.5000
25-09-27 23:57:08.172 - INFO: Learning rate: 3.1622776601683795e-05
25-09-27 23:57:08.173 - INFO: Train epoch 223:   Loss: 704.1337 | r_Loss: 81.8723 | g_Loss: 287.5396 | l_Loss: 7.2325 | 
                                                                                 223		704.1337		-4.5000
25-09-27 23:59:59.735 - INFO: Learning rate: 3.1622776601683795e-05
25-09-27 23:59:59.736 - INFO: Train epoch 224:   Loss: 644.2892 | r_Loss: 71.1324 | g_Loss: 280.4481 | l_Loss: 8.1791 | 
                                                                                 224		644.2892		-4.5000
25-09-28 00:02:50.780 - INFO: Learning rate: 3.1622776601683795e-05
25-09-28 00:02:50.780 - INFO: Train epoch 225:   Loss: 682.0874 | r_Loss: 77.5147 | g_Loss: 282.9164 | l_Loss: 11.5974 | 
                                                                                 225		682.0874		-4.5000
25-09-28 00:05:41.943 - INFO: Learning rate: 3.1622776601683795e-05
25-09-28 00:05:41.943 - INFO: Train epoch 226:   Loss: 663.3842 | r_Loss: 74.1942 | g_Loss: 276.4292 | l_Loss: 15.9840 | 
                                                                                 226		663.3842		-4.5000
25-09-28 00:08:32.726 - INFO: Learning rate: 3.1622776601683795e-05
25-09-28 00:08:32.727 - INFO: Train epoch 227:   Loss: 17900.4677 | r_Loss: 2838.5166 | g_Loss: 3211.1381 | l_Loss: 496.7474 | 
                                                                                 227		17900.4677		-4.5000
25-09-28 00:11:21.714 - INFO: Learning rate: 3.1622776601683795e-05
25-09-28 00:11:21.715 - INFO: Train epoch 228:   Loss: 2607.7756 | r_Loss: 252.3167 | g_Loss: 1299.6689 | l_Loss: 46.5230 | 
                                                                                 228		2607.7756		-4.5000
25-09-28 00:14:10.751 - INFO: Learning rate: 3.1622776601683795e-05
25-09-28 00:14:10.752 - INFO: Train epoch 229:   Loss: 1906.5569 | r_Loss: 197.2271 | g_Loss: 890.9388 | l_Loss: 29.4828 | 
                                                                                 229		1906.5569		-4.5000
25-09-28 00:16:59.687 - INFO: Learning rate: 3.1622776601683795e-05
25-09-28 00:16:59.688 - INFO: Train epoch 230:   Loss: 1498.4731 | r_Loss: 153.2546 | g_Loss: 710.1174 | l_Loss: 22.0828 | 
                                                                                 230		1498.4731		-4.5000
25-09-28 00:19:48.645 - INFO: Learning rate: 3.1622776601683795e-05
25-09-28 00:19:48.645 - INFO: Train epoch 231:   Loss: 1416.9663 | r_Loss: 147.0103 | g_Loss: 659.9840 | l_Loss: 21.9308 | 
                                                                                 231		1416.9663		-4.5000
25-09-28 00:22:37.440 - INFO: Learning rate: 3.1622776601683795e-05
25-09-28 00:22:37.440 - INFO: Train epoch 232:   Loss: 1112.8962 | r_Loss: 114.0708 | g_Loss: 526.9201 | l_Loss: 15.6222 | 
                                                                                 232		1112.8962		-4.5000
25-09-28 00:25:26.423 - INFO: Learning rate: 3.1622776601683795e-05
25-09-28 00:25:26.423 - INFO: Train epoch 233:   Loss: 992.1352 | r_Loss: 103.6791 | g_Loss: 459.9878 | l_Loss: 13.7518 | 
                                                                                 233		992.1352		-4.5000
25-09-28 00:28:15.954 - INFO: Learning rate: 3.1622776601683795e-05
25-09-28 00:28:15.955 - INFO: Train epoch 234:   Loss: 881.6922 | r_Loss: 92.8462 | g_Loss: 405.5523 | l_Loss: 11.9089 | 
                                                                                 234		881.6922		-4.5000
25-09-28 00:31:06.001 - INFO: Learning rate: 3.1622776601683795e-05
25-09-28 00:31:06.002 - INFO: Train epoch 235:   Loss: 854.7587 | r_Loss: 92.4694 | g_Loss: 380.8829 | l_Loss: 11.5289 | 
                                                                                 235		854.7587		-4.5000
25-09-28 00:33:55.464 - INFO: Learning rate: 3.1622776601683795e-05
25-09-28 00:33:55.465 - INFO: Train epoch 236:   Loss: 766.7362 | r_Loss: 82.5225 | g_Loss: 344.3405 | l_Loss: 9.7832 | 
                                                                                 236		766.7362		-4.5000
25-09-28 00:36:45.731 - INFO: Learning rate: 3.1622776601683795e-05
25-09-28 00:36:45.732 - INFO: Train epoch 237:   Loss: 743.9313 | r_Loss: 81.9752 | g_Loss: 324.8649 | l_Loss: 9.1906 | 
                                                                                 237		743.9313		-4.5000
25-09-28 00:39:35.867 - INFO: Learning rate: 3.1622776601683795e-05
25-09-28 00:39:35.867 - INFO: Train epoch 238:   Loss: 733.4021 | r_Loss: 81.8672 | g_Loss: 315.2834 | l_Loss: 8.7826 | 
                                                                                 238		733.4021		-4.5000
25-09-28 00:42:26.684 - INFO: Learning rate: 3.1622776601683795e-05
25-09-28 00:42:26.691 - INFO: Train epoch 239:   Loss: 709.2303 | r_Loss: 79.2337 | g_Loss: 303.7852 | l_Loss: 9.2767 | 
                                                                                 239		709.2303		-4.5000
25-09-28 00:45:15.252 - INFO: Learning rate: 3.1622776601683795e-05
25-09-28 00:45:15.253 - INFO: Train epoch 240:   Loss: 711.3489 | r_Loss: 80.4085 | g_Loss: 300.2190 | l_Loss: 9.0873 | 
                                                                                 240		711.3489		-4.5000
25-09-28 00:48:04.627 - INFO: Learning rate: 3.1622776601683795e-05
25-09-28 00:48:04.628 - INFO: Train epoch 241:   Loss: 686.7575 | r_Loss: 78.0921 | g_Loss: 288.1065 | l_Loss: 8.1904 | 
                                                                                 241		686.7575		-4.5000
25-09-28 00:50:55.162 - INFO: Learning rate: 3.1622776601683795e-05
25-09-28 00:50:55.163 - INFO: Train epoch 242:   Loss: 655.3956 | r_Loss: 73.0922 | g_Loss: 281.5310 | l_Loss: 8.4035 | 
                                                                                 242		655.3956		-4.5000
25-09-28 00:53:44.299 - INFO: Learning rate: 3.1622776601683795e-05
25-09-28 00:53:44.300 - INFO: Train epoch 243:   Loss: 655.6347 | r_Loss: 74.2380 | g_Loss: 275.4563 | l_Loss: 8.9886 | 
                                                                                 243		655.6347		-4.5000
25-09-28 00:56:35.377 - INFO: Learning rate: 3.1622776601683795e-05
25-09-28 00:56:35.378 - INFO: Train epoch 244:   Loss: 662.3842 | r_Loss: 74.8852 | g_Loss: 277.5195 | l_Loss: 10.4385 | 
                                                                                 244		662.3842		-4.5000
25-09-28 00:59:25.449 - INFO: Learning rate: 3.1622776601683795e-05
25-09-28 00:59:25.451 - INFO: Train epoch 245:   Loss: 641.9707 | r_Loss: 73.0960 | g_Loss: 268.5265 | l_Loss: 7.9641 | 
                                                                                 245		641.9707		-4.5000
25-09-28 01:02:15.809 - INFO: Learning rate: 3.1622776601683795e-05
25-09-28 01:02:15.810 - INFO: Train epoch 246:   Loss: 632.8617 | r_Loss: 70.8497 | g_Loss: 268.3196 | l_Loss: 10.2937 | 
                                                                                 246		632.8617		-4.5000
25-09-28 01:05:05.760 - INFO: Learning rate: 3.1622776601683795e-05
25-09-28 01:05:05.761 - INFO: Train epoch 247:   Loss: 9494.0636 | r_Loss: 1597.4120 | g_Loss: 1313.3683 | l_Loss: 193.6356 | 
                                                                                 247		9494.0636		-4.5000
25-09-28 01:07:55.980 - INFO: Learning rate: 3.1622776601683795e-05
25-09-28 01:07:55.982 - INFO: Train epoch 248:   Loss: 3508.3258 | r_Loss: 272.2820 | g_Loss: 2064.4082 | l_Loss: 82.5075 | 
                                                                                 248		3508.3258		-4.5000
25-09-28 01:10:45.780 - INFO: Learning rate: 3.1622776601683795e-05
25-09-28 01:10:45.781 - INFO: Train epoch 249:   Loss: 1694.9173 | r_Loss: 147.5660 | g_Loss: 938.5503 | l_Loss: 18.5372 | 
                                                                                 249		1694.9173		-4.5000
25-09-28 01:15:26.937 - INFO: TEST:   PSNR_S: 43.2839 | PSNR_C: 32.5699 | 
25-09-28 01:15:26.939 - INFO: Learning rate: 3.1622776601683795e-05
25-09-28 01:15:26.940 - INFO: Train epoch 250:   Loss: 1236.5839 | r_Loss: 119.3285 | g_Loss: 626.4940 | l_Loss: 13.4476 | 
                                                                                 250		1236.5839		-4.5000
25-09-28 01:18:17.218 - INFO: Learning rate: 3.1622776601683795e-05
25-09-28 01:18:17.219 - INFO: Train epoch 251:   Loss: 1016.3936 | r_Loss: 104.4263 | g_Loss: 482.9979 | l_Loss: 11.2643 | 
                                                                                 251		1016.3936		-4.5000
25-09-28 01:21:07.471 - INFO: Learning rate: 3.1622776601683795e-05
25-09-28 01:21:07.472 - INFO: Train epoch 252:   Loss: 888.9299 | r_Loss: 93.6423 | g_Loss: 410.7761 | l_Loss: 9.9422 | 
                                                                                 252		888.9299		-4.5000
25-09-28 01:23:58.594 - INFO: Learning rate: 3.1622776601683795e-05
25-09-28 01:23:58.595 - INFO: Train epoch 253:   Loss: 818.7737 | r_Loss: 88.5502 | g_Loss: 366.8698 | l_Loss: 9.1528 | 
                                                                                 253		818.7737		-4.5000
25-09-28 01:26:48.323 - INFO: Learning rate: 3.1622776601683795e-05
25-09-28 01:26:48.324 - INFO: Train epoch 254:   Loss: 755.8186 | r_Loss: 81.5051 | g_Loss: 339.7190 | l_Loss: 8.5738 | 
                                                                                 254		755.8186		-4.5000
25-09-28 01:29:38.118 - INFO: Learning rate: 3.1622776601683795e-05
25-09-28 01:29:38.119 - INFO: Train epoch 255:   Loss: 731.5300 | r_Loss: 80.9281 | g_Loss: 318.6173 | l_Loss: 8.2723 | 
                                                                                 255		731.5300		-4.5000
25-09-28 01:32:27.497 - INFO: Learning rate: 3.1622776601683795e-05
25-09-28 01:32:27.497 - INFO: Train epoch 256:   Loss: 692.4569 | r_Loss: 76.3545 | g_Loss: 302.5782 | l_Loss: 8.1062 | 
                                                                                 256		692.4569		-4.5000
25-09-28 01:35:16.538 - INFO: Learning rate: 3.1622776601683795e-05
25-09-28 01:35:16.539 - INFO: Train epoch 257:   Loss: 684.0015 | r_Loss: 76.9271 | g_Loss: 291.2643 | l_Loss: 8.1019 | 
                                                                                 257		684.0015		-4.5000
25-09-28 01:38:06.309 - INFO: Learning rate: 3.1622776601683795e-05
25-09-28 01:38:06.309 - INFO: Train epoch 258:   Loss: 669.6358 | r_Loss: 75.3135 | g_Loss: 284.5659 | l_Loss: 8.5024 | 
                                                                                 258		669.6358		-4.5000
25-09-28 01:40:56.280 - INFO: Learning rate: 3.1622776601683795e-05
25-09-28 01:40:56.282 - INFO: Train epoch 259:   Loss: 646.6228 | r_Loss: 72.6305 | g_Loss: 275.5226 | l_Loss: 7.9478 | 
                                                                                 259		646.6228		-4.5000
25-09-28 01:43:46.096 - INFO: Learning rate: 3.1622776601683795e-05
25-09-28 01:43:46.097 - INFO: Train epoch 260:   Loss: 641.6178 | r_Loss: 71.4766 | g_Loss: 272.4367 | l_Loss: 11.7983 | 
                                                                                 260		641.6178		-4.5000
25-09-28 01:46:38.526 - INFO: Learning rate: 3.1622776601683795e-05
25-09-28 01:46:38.527 - INFO: Train epoch 261:   Loss: 662.3642 | r_Loss: 74.1703 | g_Loss: 276.7299 | l_Loss: 14.7826 | 
                                                                                 261		662.3642		-4.5000
25-09-28 01:49:27.890 - INFO: Learning rate: 3.1622776601683795e-05
25-09-28 01:49:27.891 - INFO: Train epoch 262:   Loss: 621.3999 | r_Loss: 69.5668 | g_Loss: 263.7301 | l_Loss: 9.8358 | 
                                                                                 262		621.3999		-4.5000
25-09-28 01:52:17.515 - INFO: Learning rate: 3.1622776601683795e-05
25-09-28 01:52:17.516 - INFO: Train epoch 263:   Loss: 659.8077 | r_Loss: 71.6729 | g_Loss: 277.8251 | l_Loss: 23.6182 | 
                                                                                 263		659.8077		-4.5000
25-09-28 01:55:07.180 - INFO: Learning rate: 3.1622776601683795e-05
25-09-28 01:55:07.181 - INFO: Train epoch 264:   Loss: 618.3370 | r_Loss: 70.3539 | g_Loss: 257.5671 | l_Loss: 9.0004 | 
                                                                                 264		618.3370		-4.5000
25-09-28 01:57:57.202 - INFO: Learning rate: 3.1622776601683795e-05
25-09-28 01:57:57.203 - INFO: Train epoch 265:   Loss: 10661.1993 | r_Loss: 1797.7668 | g_Loss: 1407.1280 | l_Loss: 265.2371 | 
                                                                                 265		10661.1993		-4.5000
25-09-28 02:00:46.601 - INFO: Learning rate: 3.1622776601683795e-05
25-09-28 02:00:46.602 - INFO: Train epoch 266:   Loss: 3530.8144 | r_Loss: 306.3698 | g_Loss: 1902.0794 | l_Loss: 96.8859 | 
                                                                                 266		3530.8144		-4.5000
25-09-28 02:03:42.257 - INFO: Learning rate: 3.1622776601683795e-05
25-09-28 02:03:42.258 - INFO: Train epoch 267:   Loss: 1981.0643 | r_Loss: 188.9465 | g_Loss: 1001.7665 | l_Loss: 34.5655 | 
                                                                                 267		1981.0643		-4.5000
25-09-28 02:06:33.047 - INFO: Learning rate: 3.1622776601683795e-05
25-09-28 02:06:33.047 - INFO: Train epoch 268:   Loss: 1423.3532 | r_Loss: 127.6728 | g_Loss: 763.4169 | l_Loss: 21.5722 | 
                                                                                 268		1423.3532		-4.5000
25-09-28 02:09:23.494 - INFO: Learning rate: 3.1622776601683795e-05
25-09-28 02:09:23.494 - INFO: Train epoch 269:   Loss: 1199.0329 | r_Loss: 113.9873 | g_Loss: 611.8557 | l_Loss: 17.2409 | 
                                                                                 269		1199.0329		-4.5000
25-09-28 02:12:22.888 - INFO: Learning rate: 3.1622776601683795e-05
25-09-28 02:12:22.890 - INFO: Train epoch 270:   Loss: 1109.7327 | r_Loss: 109.6233 | g_Loss: 544.6828 | l_Loss: 16.9335 | 
                                                                                 270		1109.7327		-4.5000
25-09-28 02:15:13.055 - INFO: Learning rate: 3.1622776601683795e-05
25-09-28 02:15:13.056 - INFO: Train epoch 271:   Loss: 948.8014 | r_Loss: 94.4299 | g_Loss: 463.3829 | l_Loss: 13.2693 | 
                                                                                 271		948.8014		-4.5000
25-09-28 02:18:03.215 - INFO: Learning rate: 3.1622776601683795e-05
25-09-28 02:18:03.216 - INFO: Train epoch 272:   Loss: 879.7052 | r_Loss: 91.1954 | g_Loss: 410.9803 | l_Loss: 12.7481 | 
                                                                                 272		879.7052		-4.5000
25-09-28 02:20:56.761 - INFO: Learning rate: 3.1622776601683795e-05
25-09-28 02:20:56.762 - INFO: Train epoch 273:   Loss: 805.3947 | r_Loss: 83.9783 | g_Loss: 373.8384 | l_Loss: 11.6648 | 
                                                                                 273		805.3947		-4.5000
25-09-28 02:23:46.870 - INFO: Learning rate: 3.1622776601683795e-05
25-09-28 02:23:46.871 - INFO: Train epoch 274:   Loss: 759.0493 | r_Loss: 80.9034 | g_Loss: 343.6731 | l_Loss: 10.8593 | 
                                                                                 274		759.0493		-4.5000
25-09-28 02:26:44.510 - INFO: Learning rate: 3.1622776601683795e-05
25-09-28 02:26:44.511 - INFO: Train epoch 275:   Loss: 725.4652 | r_Loss: 78.3112 | g_Loss: 323.4205 | l_Loss: 10.4887 | 
                                                                                 275		725.4652		-4.5000
25-09-28 02:29:34.662 - INFO: Learning rate: 3.1622776601683795e-05
25-09-28 02:29:34.664 - INFO: Train epoch 276:   Loss: 731.6489 | r_Loss: 80.8551 | g_Loss: 315.8344 | l_Loss: 11.5389 | 
                                                                                 276		731.6489		-4.5000
25-09-28 02:32:25.548 - INFO: Learning rate: 3.1622776601683795e-05
25-09-28 02:32:25.549 - INFO: Train epoch 277:   Loss: 684.4934 | r_Loss: 74.5477 | g_Loss: 301.1945 | l_Loss: 10.5605 | 
                                                                                 277		684.4934		-4.5000
25-09-28 02:35:16.135 - INFO: Learning rate: 3.1622776601683795e-05
25-09-28 02:35:16.136 - INFO: Train epoch 278:   Loss: 664.9776 | r_Loss: 72.7960 | g_Loss: 290.0481 | l_Loss: 10.9493 | 
                                                                                 278		664.9776		-4.5000
25-09-28 02:38:05.917 - INFO: Learning rate: 3.1622776601683795e-05
25-09-28 02:38:05.918 - INFO: Train epoch 279:   Loss: 660.9349 | r_Loss: 71.9138 | g_Loss: 286.7283 | l_Loss: 14.6377 | 
                                                                                 279		660.9349		-4.5000
25-09-28 02:40:55.979 - INFO: Learning rate: 3.1622776601683795e-05
25-09-28 02:40:55.980 - INFO: Train epoch 280:   Loss: 657.6043 | r_Loss: 72.4323 | g_Loss: 281.3478 | l_Loss: 14.0951 | 
                                                                                 280		657.6043		-4.5000
25-09-28 02:43:52.682 - INFO: Learning rate: 3.1622776601683795e-05
25-09-28 02:43:52.683 - INFO: Train epoch 281:   Loss: 256148622145.6076 | r_Loss: 51229704081.5114 | g_Loss: 67389.6317 | l_Loss: 23554.9964 | 
                                                                                 281		256148622145.6076		-4.5000
25-09-28 02:46:42.205 - INFO: Learning rate: 3.1622776601683795e-05
25-09-28 02:46:42.206 - INFO: Train epoch 282:   Loss: 424273.0267 | r_Loss: 66619.4357 | g_Loss: 78204.7979 | l_Loss: 12971.0510 | 
                                                                                 282		424273.0267		-4.5000
25-09-28 02:49:32.185 - INFO: Learning rate: 3.1622776601683795e-05
25-09-28 02:49:32.185 - INFO: Train epoch 283:   Loss: 423828.8387 | r_Loss: 66542.6837 | g_Loss: 78109.3397 | l_Loss: 13006.0793 | 
                                                                                 283		423828.8387		-4.5000
25-09-28 02:52:21.589 - INFO: Learning rate: 3.1622776601683795e-05
25-09-28 02:52:21.590 - INFO: Train epoch 284:   Loss: 422611.0255 | r_Loss: 66335.9987 | g_Loss: 77914.7336 | l_Loss: 13016.2993 | 
                                                                                 284		422611.0255		-4.5000
25-09-28 02:55:11.704 - INFO: Learning rate: 3.1622776601683795e-05
25-09-28 02:55:11.705 - INFO: Train epoch 285:   Loss: 421415.5087 | r_Loss: 66134.5094 | g_Loss: 77706.0395 | l_Loss: 13036.9225 | 
                                                                                 285		421415.5087		-4.5000
25-09-28 02:58:01.418 - INFO: Learning rate: 3.1622776601683795e-05
25-09-28 02:58:01.419 - INFO: Train epoch 286:   Loss: 420492.4049 | r_Loss: 65973.5558 | g_Loss: 77553.4025 | l_Loss: 13071.2229 | 
                                                                                 286		420492.4049		-4.5000
25-09-28 03:00:51.213 - INFO: Learning rate: 3.1622776601683795e-05
25-09-28 03:00:51.214 - INFO: Train epoch 287:   Loss: 419029.0059 | r_Loss: 65724.8645 | g_Loss: 77322.5939 | l_Loss: 13082.0913 | 
                                                                                 287		419029.0059		-4.5000
25-09-28 03:03:41.175 - INFO: Learning rate: 3.1622776601683795e-05
25-09-28 03:03:41.176 - INFO: Train epoch 288:   Loss: 417862.5406 | r_Loss: 65519.4426 | g_Loss: 77144.4789 | l_Loss: 13120.8500 | 
                                                                                 288		417862.5406		-4.5000
25-09-28 03:06:31.464 - INFO: Learning rate: 3.1622776601683795e-05
25-09-28 03:06:31.465 - INFO: Train epoch 289:   Loss: 416014.6141 | r_Loss: 65202.2873 | g_Loss: 76863.7862 | l_Loss: 13139.3906 | 
                                                                                 289		416014.6141		-4.5000
25-09-28 03:09:21.128 - INFO: Learning rate: 3.1622776601683795e-05
25-09-28 03:09:21.129 - INFO: Train epoch 290:   Loss: 414192.6755 | r_Loss: 64897.1509 | g_Loss: 76559.6303 | l_Loss: 13147.2899 | 
                                                                                 290		414192.6755		-4.5000
25-09-28 03:12:10.833 - INFO: Learning rate: 3.1622776601683795e-05
25-09-28 03:12:10.834 - INFO: Train epoch 291:   Loss: 405604.3146 | r_Loss: 63224.2140 | g_Loss: 76301.6755 | l_Loss: 13181.5689 | 
                                                                                 291		405604.3146		-4.5000
25-09-28 03:15:01.054 - INFO: Learning rate: 3.1622776601683795e-05
25-09-28 03:15:01.055 - INFO: Train epoch 292:   Loss: 369066.7646 | r_Loss: 55960.2463 | g_Loss: 76069.5450 | l_Loss: 13195.9864 | 
                                                                                 292		369066.7646		-4.5000
25-09-28 03:17:50.811 - INFO: Learning rate: 3.1622776601683795e-05
25-09-28 03:17:50.812 - INFO: Train epoch 293:   Loss: 363863.8023 | r_Loss: 54948.3346 | g_Loss: 75897.2982 | l_Loss: 13224.8311 | 
                                                                                 293		363863.8023		-4.5000
25-09-28 03:20:41.210 - INFO: Learning rate: 3.1622776601683795e-05
25-09-28 03:20:41.211 - INFO: Train epoch 294:   Loss: 360738.5705 | r_Loss: 54370.0807 | g_Loss: 75658.5293 | l_Loss: 13229.6356 | 
                                                                                 294		360738.5705		-4.5000
25-09-28 03:23:31.074 - INFO: Learning rate: 3.1622776601683795e-05
25-09-28 03:23:31.074 - INFO: Train epoch 295:   Loss: 357475.2491 | r_Loss: 53750.6037 | g_Loss: 75466.5922 | l_Loss: 13255.6366 | 
                                                                                 295		357475.2491		-4.5000
25-09-28 03:26:20.430 - INFO: Learning rate: 3.1622776601683795e-05
25-09-28 03:26:20.431 - INFO: Train epoch 296:   Loss: 354290.8236 | r_Loss: 53172.0659 | g_Loss: 75169.5258 | l_Loss: 13260.9681 | 
                                                                                 296		354290.8236		-4.5000
25-09-28 03:29:10.565 - INFO: Learning rate: 3.1622776601683795e-05
25-09-28 03:29:10.566 - INFO: Train epoch 297:   Loss: 351759.9889 | r_Loss: 52702.5835 | g_Loss: 74949.7888 | l_Loss: 13297.2825 | 
                                                                                 297		351759.9889		-4.5000
25-09-28 03:32:00.613 - INFO: Learning rate: 3.1622776601683795e-05
25-09-28 03:32:00.615 - INFO: Train epoch 298:   Loss: 349022.8977 | r_Loss: 52211.3983 | g_Loss: 74650.3928 | l_Loss: 13315.5151 | 
                                                                                 298		349022.8977		-4.5000
25-09-28 03:34:50.091 - INFO: Learning rate: 3.1622776601683795e-05
25-09-28 03:34:50.092 - INFO: Train epoch 299:   Loss: 346108.0591 | r_Loss: 51676.9042 | g_Loss: 74377.3511 | l_Loss: 13346.1860 | 
                                                                                 299		346108.0591		-4.5000
25-09-28 03:42:11.429 - INFO: TEST:   PSNR_S: 11.4186 | PSNR_C: 11.0863 | 
25-09-28 03:42:11.432 - INFO: Learning rate: 3.1622776601683795e-05
25-09-28 03:42:11.432 - INFO: Train epoch 300:   Loss: 343552.0542 | r_Loss: 51217.0992 | g_Loss: 74088.8462 | l_Loss: 13377.7120 | 
                                                                                 300		343552.0542		-4.5000
25-09-28 03:45:01.137 - INFO: Learning rate: 3.1622776601683795e-05
25-09-28 03:45:01.138 - INFO: Train epoch 301:   Loss: 340239.7132 | r_Loss: 50622.4913 | g_Loss: 73729.6987 | l_Loss: 13397.5567 | 
                                                                                 301		340239.7132		-4.5000
25-09-28 03:47:50.210 - INFO: Learning rate: 3.1622776601683795e-05
25-09-28 03:47:50.211 - INFO: Train epoch 302:   Loss: 337005.6986 | r_Loss: 50039.2605 | g_Loss: 73387.3438 | l_Loss: 13422.0505 | 
                                                                                 302		337005.6986		-4.5000
25-09-28 03:50:39.177 - INFO: Learning rate: 3.1622776601683795e-05
25-09-28 03:50:39.178 - INFO: Train epoch 303:   Loss: 333889.6795 | r_Loss: 49482.3163 | g_Loss: 73031.8709 | l_Loss: 13446.2281 | 
                                                                                 303		333889.6795		-4.5000
25-09-28 03:53:28.515 - INFO: Learning rate: 3.1622776601683795e-05
25-09-28 03:53:28.516 - INFO: Train epoch 304:   Loss: 329923.6516 | r_Loss: 48763.1360 | g_Loss: 72630.7102 | l_Loss: 13477.2620 | 
                                                                                 304		329923.6516		-4.5000
25-09-28 03:56:16.546 - INFO: Learning rate: 3.1622776601683795e-05
25-09-28 03:56:16.547 - INFO: Train epoch 305:   Loss: 326773.7357 | r_Loss: 48199.7671 | g_Loss: 72264.0071 | l_Loss: 13510.8928 | 
                                                                                 305		326773.7357		-4.5000
25-09-28 03:59:05.387 - INFO: Learning rate: 3.1622776601683795e-05
25-09-28 03:59:05.388 - INFO: Train epoch 306:   Loss: 322974.9598 | r_Loss: 47507.0094 | g_Loss: 71886.3650 | l_Loss: 13553.5473 | 
                                                                                 306		322974.9598		-4.5000
25-09-28 04:01:54.125 - INFO: Learning rate: 3.1622776601683795e-05
25-09-28 04:01:54.126 - INFO: Train epoch 307:   Loss: 318809.4209 | r_Loss: 46751.1420 | g_Loss: 71461.9555 | l_Loss: 13591.7554 | 
                                                                                 307		318809.4209		-4.5000
25-09-28 04:04:42.684 - INFO: Learning rate: 3.1622776601683795e-05
25-09-28 04:04:42.684 - INFO: Train epoch 308:   Loss: 314783.2707 | r_Loss: 46019.8537 | g_Loss: 71053.1311 | l_Loss: 13630.8705 | 
                                                                                 308		314783.2707		-4.5000
25-09-28 04:07:40.952 - INFO: Learning rate: 3.1622776601683795e-05
25-09-28 04:07:40.952 - INFO: Train epoch 309:   Loss: 310097.2347 | r_Loss: 45180.5567 | g_Loss: 70537.7796 | l_Loss: 13656.6731 | 
                                                                                 309		310097.2347		-4.5000
25-09-28 04:10:29.488 - INFO: Learning rate: 3.1622776601683795e-05
25-09-28 04:10:29.488 - INFO: Train epoch 310:   Loss: 305352.6109 | r_Loss: 44315.8620 | g_Loss: 70070.5147 | l_Loss: 13702.7855 | 
                                                                                 310		305352.6109		-4.5000
25-09-28 04:13:25.743 - INFO: Learning rate: 3.1622776601683795e-05
25-09-28 04:13:25.744 - INFO: Train epoch 311:   Loss: 300331.9617 | r_Loss: 43423.4820 | g_Loss: 69489.9034 | l_Loss: 13724.6478 | 
                                                                                 311		300331.9617		-4.5000
25-09-28 04:16:25.780 - INFO: Learning rate: 3.1622776601683795e-05
25-09-28 04:16:25.781 - INFO: Train epoch 312:   Loss: 295280.2405 | r_Loss: 42501.2015 | g_Loss: 68987.2657 | l_Loss: 13786.9663 | 
                                                                                 312		295280.2405		-4.5000
25-09-28 04:19:15.747 - INFO: Learning rate: 3.1622776601683795e-05
25-09-28 04:19:15.748 - INFO: Train epoch 313:   Loss: 290146.4809 | r_Loss: 41576.7603 | g_Loss: 68421.6368 | l_Loss: 13841.0415 | 
                                                                                 313		290146.4809		-4.5000
25-09-28 04:22:12.802 - INFO: Learning rate: 3.1622776601683795e-05
25-09-28 04:22:12.804 - INFO: Train epoch 314:   Loss: 284627.8127 | r_Loss: 40579.4957 | g_Loss: 67831.2880 | l_Loss: 13899.0468 | 
                                                                                 314		284627.8127		-4.5000
25-09-28 04:25:02.791 - INFO: Learning rate: 3.1622776601683795e-05
25-09-28 04:25:02.793 - INFO: Train epoch 315:   Loss: 279119.8112 | r_Loss: 39602.8263 | g_Loss: 67154.1004 | l_Loss: 13951.5790 | 
                                                                                 315		279119.8112		-4.5000
25-09-28 04:27:52.673 - INFO: Learning rate: 3.1622776601683795e-05
25-09-28 04:27:52.674 - INFO: Train epoch 316:   Loss: 273725.5112 | r_Loss: 38654.5906 | g_Loss: 66443.6051 | l_Loss: 14008.9522 | 
                                                                                 316		273725.5112		-4.5000
25-09-28 04:30:54.299 - INFO: Learning rate: 3.1622776601683795e-05
25-09-28 04:30:54.301 - INFO: Train epoch 317:   Loss: 268506.1596 | r_Loss: 37736.6738 | g_Loss: 65735.2987 | l_Loss: 14087.4911 | 
                                                                                 317		268506.1596		-4.5000
25-09-28 04:33:44.423 - INFO: Learning rate: 3.1622776601683795e-05
25-09-28 04:33:44.424 - INFO: Train epoch 318:   Loss: 263319.2914 | r_Loss: 36850.7077 | g_Loss: 64907.8986 | l_Loss: 14157.8540 | 
                                                                                 318		263319.2914		-4.5000
25-09-28 04:36:34.543 - INFO: Learning rate: 3.1622776601683795e-05
25-09-28 04:36:34.544 - INFO: Train epoch 319:   Loss: 257913.2506 | r_Loss: 35950.5564 | g_Loss: 63952.5219 | l_Loss: 14207.9461 | 
                                                                                 319		257913.2506		-4.5000
25-09-28 04:39:24.480 - INFO: Learning rate: 3.1622776601683795e-05
25-09-28 04:39:24.480 - INFO: Train epoch 320:   Loss: 252812.5473 | r_Loss: 35125.5965 | g_Loss: 62928.6211 | l_Loss: 14255.9423 | 
                                                                                 320		252812.5473		-4.5000
25-09-28 04:42:13.888 - INFO: Learning rate: 3.1622776601683795e-05
25-09-28 04:42:13.888 - INFO: Train epoch 321:   Loss: 247858.9029 | r_Loss: 34349.3587 | g_Loss: 61820.1323 | l_Loss: 14291.9778 | 
                                                                                 321		247858.9029		-4.5000
25-09-28 04:45:03.046 - INFO: Learning rate: 3.1622776601683795e-05
25-09-28 04:45:03.047 - INFO: Train epoch 322:   Loss: 243388.8166 | r_Loss: 33670.3785 | g_Loss: 60681.0109 | l_Loss: 14355.9128 | 
                                                                                 322		243388.8166		-4.5000
25-09-28 04:47:52.542 - INFO: Learning rate: 3.1622776601683795e-05
25-09-28 04:47:52.543 - INFO: Train epoch 323:   Loss: 238888.5700 | r_Loss: 33020.3559 | g_Loss: 59401.8714 | l_Loss: 14384.9195 | 
                                                                                 323		238888.5700		-4.5000
25-09-28 04:50:42.861 - INFO: Learning rate: 3.1622776601683795e-05
25-09-28 04:50:42.861 - INFO: Train epoch 324:   Loss: 234261.8548 | r_Loss: 32376.5255 | g_Loss: 58013.8117 | l_Loss: 14365.4162 | 
                                                                                 324		234261.8548		-4.5000
25-09-28 04:53:32.532 - INFO: Learning rate: 3.1622776601683795e-05
25-09-28 04:53:32.533 - INFO: Train epoch 325:   Loss: 229824.8436 | r_Loss: 31785.3575 | g_Loss: 56573.3944 | l_Loss: 14324.6616 | 
                                                                                 325		229824.8436		-4.5000
25-09-28 04:56:21.768 - INFO: Learning rate: 3.1622776601683795e-05
25-09-28 04:56:21.769 - INFO: Train epoch 326:   Loss: 225932.9828 | r_Loss: 31292.7261 | g_Loss: 55182.4372 | l_Loss: 14286.9155 | 
                                                                                 326		225932.9828		-4.5000
25-09-28 04:59:10.821 - INFO: Learning rate: 3.1622776601683795e-05
25-09-28 04:59:10.821 - INFO: Train epoch 327:   Loss: 221581.0526 | r_Loss: 30751.9946 | g_Loss: 53655.2940 | l_Loss: 14165.7859 | 
                                                                                 327		221581.0526		-4.5000
25-09-28 05:02:00.038 - INFO: Learning rate: 3.1622776601683795e-05
25-09-28 05:02:00.039 - INFO: Train epoch 328:   Loss: 217113.1402 | r_Loss: 30210.1104 | g_Loss: 52107.7926 | l_Loss: 13954.7950 | 
                                                                                 328		217113.1402		-4.5000
25-09-28 05:04:49.814 - INFO: Learning rate: 3.1622776601683795e-05
25-09-28 05:04:49.815 - INFO: Train epoch 329:   Loss: 213291.9352 | r_Loss: 29772.1954 | g_Loss: 50673.5082 | l_Loss: 13757.4506 | 
                                                                                 329		213291.9352		-4.5000
25-09-28 05:07:38.571 - INFO: Learning rate: 3.1622776601683795e-05
25-09-28 05:07:38.572 - INFO: Train epoch 330:   Loss: 208912.3667 | r_Loss: 29252.6926 | g_Loss: 49188.8744 | l_Loss: 13460.0295 | 
                                                                                 330		208912.3667		-4.5000
25-09-28 05:10:27.779 - INFO: Learning rate: 3.1622776601683795e-05
25-09-28 05:10:27.780 - INFO: Train epoch 331:   Loss: 204832.8060 | r_Loss: 28780.0023 | g_Loss: 47806.5396 | l_Loss: 13126.2546 | 
                                                                                 331		204832.8060		-4.5000
25-09-28 05:13:16.762 - INFO: Learning rate: 3.1622776601683795e-05
25-09-28 05:13:16.763 - INFO: Train epoch 332:   Loss: 200122.9356 | r_Loss: 28219.0324 | g_Loss: 46318.1206 | l_Loss: 12709.6530 | 
                                                                                 332		200122.9356		-4.5000
25-09-28 05:16:06.785 - INFO: Learning rate: 3.1622776601683795e-05
25-09-28 05:16:06.786 - INFO: Train epoch 333:   Loss: 196080.0655 | r_Loss: 27755.5381 | g_Loss: 44982.6363 | l_Loss: 12319.7385 | 
                                                                                 333		196080.0655		-4.5000
25-09-28 05:18:56.064 - INFO: Learning rate: 3.1622776601683795e-05
25-09-28 05:18:56.065 - INFO: Train epoch 334:   Loss: 191297.8546 | r_Loss: 27183.5110 | g_Loss: 43548.5312 | l_Loss: 11831.7666 | 
                                                                                 334		191297.8546		-4.5000
25-09-28 05:21:44.821 - INFO: Learning rate: 3.1622776601683795e-05
25-09-28 05:21:44.822 - INFO: Train epoch 335:   Loss: 186690.8214 | r_Loss: 26636.5943 | g_Loss: 42178.6917 | l_Loss: 11329.1590 | 
                                                                                 335		186690.8214		-4.5000
25-09-28 05:24:33.610 - INFO: Learning rate: 3.1622776601683795e-05
25-09-28 05:24:33.611 - INFO: Train epoch 336:   Loss: 181626.7514 | r_Loss: 26016.2744 | g_Loss: 40728.2591 | l_Loss: 10817.1193 | 
                                                                                 336		181626.7514		-4.5000
25-09-28 05:27:33.367 - INFO: Learning rate: 3.1622776601683795e-05
25-09-28 05:27:33.368 - INFO: Train epoch 337:   Loss: 176735.7658 | r_Loss: 25415.6613 | g_Loss: 39349.0179 | l_Loss: 10308.4417 | 
                                                                                 337		176735.7658		-4.5000
25-09-28 05:30:22.975 - INFO: Learning rate: 3.1622776601683795e-05
25-09-28 05:30:22.976 - INFO: Train epoch 338:   Loss: 171350.9952 | r_Loss: 24737.8237 | g_Loss: 37864.7904 | l_Loss: 9797.0862 | 
                                                                                 338		171350.9952		-4.5000
25-09-28 05:33:12.356 - INFO: Learning rate: 3.1622776601683795e-05
25-09-28 05:33:12.357 - INFO: Train epoch 339:   Loss: 165689.3579 | r_Loss: 24010.0243 | g_Loss: 36389.6733 | l_Loss: 9249.5634 | 
                                                                                 339		165689.3579		-4.5000
25-09-28 05:36:00.724 - INFO: Learning rate: 3.1622776601683795e-05
25-09-28 05:36:00.725 - INFO: Train epoch 340:   Loss: 159784.3626 | r_Loss: 23219.8265 | g_Loss: 34960.0130 | l_Loss: 8725.2172 | 
                                                                                 340		159784.3626		-4.5000
25-09-28 05:38:48.345 - INFO: Learning rate: 3.1622776601683795e-05
25-09-28 05:38:48.346 - INFO: Train epoch 341:   Loss: 153597.6198 | r_Loss: 22373.0628 | g_Loss: 33501.4816 | l_Loss: 8230.8247 | 
                                                                                 341		153597.6198		-4.5000
25-09-28 05:42:08.124 - INFO: Learning rate: 3.1622776601683795e-05
25-09-28 05:42:08.125 - INFO: Train epoch 342:   Loss: 146428.2860 | r_Loss: 21316.9839 | g_Loss: 32064.9385 | l_Loss: 7778.4282 | 
                                                                                 342		146428.2860		-4.5000
25-09-28 05:44:55.004 - INFO: Learning rate: 3.1622776601683795e-05
25-09-28 05:44:55.005 - INFO: Train epoch 343:   Loss: 137036.8056 | r_Loss: 19814.3559 | g_Loss: 30605.3934 | l_Loss: 7359.6320 | 
                                                                                 343		137036.8056		-4.5000
25-09-28 05:47:41.941 - INFO: Learning rate: 3.1622776601683795e-05
25-09-28 05:47:41.942 - INFO: Train epoch 344:   Loss: 121902.3665 | r_Loss: 17152.2088 | g_Loss: 29099.8958 | l_Loss: 7041.4264 | 
                                                                                 344		121902.3665		-4.5000
25-09-28 05:50:29.938 - INFO: Learning rate: 3.1622776601683795e-05
25-09-28 05:50:29.940 - INFO: Train epoch 345:   Loss: 99304.2674 | r_Loss: 13065.3692 | g_Loss: 26959.5929 | l_Loss: 7017.8287 | 
                                                                                 345		99304.2674		-4.5000
25-09-28 05:53:17.293 - INFO: Learning rate: 3.1622776601683795e-05
25-09-28 05:53:17.293 - INFO: Train epoch 346:   Loss: 79247.1710 | r_Loss: 9531.5432 | g_Loss: 24444.3426 | l_Loss: 7145.1124 | 
                                                                                 346		79247.1710		-4.5000
25-09-28 05:56:04.563 - INFO: Learning rate: 3.1622776601683795e-05
25-09-28 05:56:04.564 - INFO: Train epoch 347:   Loss: 67399.3582 | r_Loss: 7635.1968 | g_Loss: 22069.1690 | l_Loss: 7154.2048 | 
                                                                                 347		67399.3582		-4.5000
25-09-28 05:58:51.553 - INFO: Learning rate: 3.1622776601683795e-05
25-09-28 05:58:51.554 - INFO: Train epoch 348:   Loss: 60151.0804 | r_Loss: 6670.3476 | g_Loss: 19884.2703 | l_Loss: 6915.0718 | 
                                                                                 348		60151.0804		-4.5000
25-09-28 06:01:38.548 - INFO: Learning rate: 3.1622776601683795e-05
25-09-28 06:01:38.549 - INFO: Train epoch 349:   Loss: 55322.6229 | r_Loss: 6193.2345 | g_Loss: 17797.8545 | l_Loss: 6558.5959 | 
                                                                                 349		55322.6229		-4.5000
25-09-28 06:05:20.122 - INFO: TEST:   PSNR_S: 21.4605 | PSNR_C: 17.9562 | 
25-09-28 06:05:20.124 - INFO: Learning rate: 3.1622776601683795e-05
25-09-28 06:05:20.125 - INFO: Train epoch 350:   Loss: 51661.3191 | r_Loss: 5850.2324 | g_Loss: 16228.5261 | l_Loss: 6181.6307 | 
                                                                                 350		51661.3191		-4.5000
25-09-28 06:08:07.790 - INFO: Learning rate: 3.1622776601683795e-05
25-09-28 06:08:07.791 - INFO: Train epoch 351:   Loss: 48848.5130 | r_Loss: 5610.1259 | g_Loss: 14993.8921 | l_Loss: 5803.9915 | 
                                                                                 351		48848.5130		-4.5000
25-09-28 06:10:55.116 - INFO: Learning rate: 3.1622776601683795e-05
25-09-28 06:10:55.117 - INFO: Train epoch 352:   Loss: 46493.7227 | r_Loss: 5412.6633 | g_Loss: 13997.1197 | l_Loss: 5433.2862 | 
                                                                                 352		46493.7227		-4.5000
25-09-28 06:13:42.489 - INFO: Learning rate: 3.1622776601683795e-05
25-09-28 06:13:42.489 - INFO: Train epoch 353:   Loss: 44314.1428 | r_Loss: 5226.7677 | g_Loss: 13108.1274 | l_Loss: 5072.1768 | 
                                                                                 353		44314.1428		-4.5000
25-09-28 06:16:29.361 - INFO: Learning rate: 3.1622776601683795e-05
25-09-28 06:16:29.362 - INFO: Train epoch 354:   Loss: 42299.0632 | r_Loss: 5072.1842 | g_Loss: 12244.6283 | l_Loss: 4693.5139 | 
                                                                                 354		42299.0632		-4.5000
25-09-28 06:19:16.150 - INFO: Learning rate: 3.1622776601683795e-05
25-09-28 06:19:16.151 - INFO: Train epoch 355:   Loss: 40370.2129 | r_Loss: 4921.0564 | g_Loss: 11423.4861 | l_Loss: 4341.4451 | 
                                                                                 355		40370.2129		-4.5000
25-09-28 06:22:02.966 - INFO: Learning rate: 3.1622776601683795e-05
25-09-28 06:22:02.967 - INFO: Train epoch 356:   Loss: 38523.2607 | r_Loss: 4790.4215 | g_Loss: 10578.3122 | l_Loss: 3992.8409 | 
                                                                                 356		38523.2607		-4.5000
25-09-28 06:24:50.152 - INFO: Learning rate: 3.1622776601683795e-05
25-09-28 06:24:50.152 - INFO: Train epoch 357:   Loss: 36683.8673 | r_Loss: 4644.4855 | g_Loss: 9793.0488 | l_Loss: 3668.3909 | 
                                                                                 357		36683.8673		-4.5000
25-09-28 06:27:37.211 - INFO: Learning rate: 3.1622776601683795e-05
25-09-28 06:27:37.211 - INFO: Train epoch 358:   Loss: 34888.3060 | r_Loss: 4505.0881 | g_Loss: 9019.2579 | l_Loss: 3343.6075 | 
                                                                                 358		34888.3060		-4.5000
25-09-28 06:30:24.106 - INFO: Learning rate: 3.1622776601683795e-05
25-09-28 06:30:24.106 - INFO: Train epoch 359:   Loss: 33296.4031 | r_Loss: 4399.9673 | g_Loss: 8278.0447 | l_Loss: 3018.5218 | 
                                                                                 359		33296.4031		-4.5000
25-09-28 06:33:10.825 - INFO: Learning rate: 3.1622776601683795e-05
25-09-28 06:33:10.826 - INFO: Train epoch 360:   Loss: 31757.4847 | r_Loss: 4280.9515 | g_Loss: 7634.9359 | l_Loss: 2717.7914 | 
                                                                                 360		31757.4847		-4.5000
25-09-28 06:35:57.813 - INFO: Learning rate: 3.1622776601683795e-05
25-09-28 06:35:57.814 - INFO: Train epoch 361:   Loss: 30368.5698 | r_Loss: 4177.2032 | g_Loss: 7039.8359 | l_Loss: 2442.7177 | 
                                                                                 361		30368.5698		-4.5000
25-09-28 06:38:44.547 - INFO: Learning rate: 3.1622776601683795e-05
25-09-28 06:38:44.548 - INFO: Train epoch 362:   Loss: 29229.5963 | r_Loss: 4091.0541 | g_Loss: 6555.6209 | l_Loss: 2218.7049 | 
                                                                                 362		29229.5963		-4.5000
25-09-28 06:41:31.730 - INFO: Learning rate: 3.1622776601683795e-05
25-09-28 06:41:31.730 - INFO: Train epoch 363:   Loss: 28062.3178 | r_Loss: 3963.0406 | g_Loss: 6190.1903 | l_Loss: 2056.9243 | 
                                                                                 363		28062.3178		-4.5000
25-09-28 06:44:19.310 - INFO: Learning rate: 3.1622776601683795e-05
25-09-28 06:44:19.312 - INFO: Train epoch 364:   Loss: 27147.9078 | r_Loss: 3865.9136 | g_Loss: 5889.9256 | l_Loss: 1928.4145 | 
                                                                                 364		27147.9078		-4.5000
25-09-28 06:47:06.229 - INFO: Learning rate: 3.1622776601683795e-05
25-09-28 06:47:06.229 - INFO: Train epoch 365:   Loss: 26211.5075 | r_Loss: 3757.2470 | g_Loss: 5611.2372 | l_Loss: 1814.0350 | 
                                                                                 365		26211.5075		-4.5000
25-09-28 06:49:52.953 - INFO: Learning rate: 3.1622776601683795e-05
25-09-28 06:49:52.953 - INFO: Train epoch 366:   Loss: 25381.7926 | r_Loss: 3654.5392 | g_Loss: 5378.1278 | l_Loss: 1730.9690 | 
                                                                                 366		25381.7926		-4.5000
25-09-28 06:52:39.826 - INFO: Learning rate: 3.1622776601683795e-05
25-09-28 06:52:39.827 - INFO: Train epoch 367:   Loss: 24761.0682 | r_Loss: 3586.4281 | g_Loss: 5169.6661 | l_Loss: 1659.2613 | 
                                                                                 367		24761.0682		-4.5000
25-09-28 06:55:26.781 - INFO: Learning rate: 3.1622776601683795e-05
25-09-28 06:55:26.781 - INFO: Train epoch 368:   Loss: 24102.8230 | r_Loss: 3501.8645 | g_Loss: 4988.3890 | l_Loss: 1605.1117 | 
                                                                                 368		24102.8230		-4.5000
25-09-28 06:58:13.693 - INFO: Learning rate: 3.1622776601683795e-05
25-09-28 06:58:13.694 - INFO: Train epoch 369:   Loss: 23436.9682 | r_Loss: 3414.2921 | g_Loss: 4809.2629 | l_Loss: 1556.2450 | 
                                                                                 369		23436.9682		-4.5000
25-09-28 07:01:00.291 - INFO: Learning rate: 3.1622776601683795e-05
25-09-28 07:01:00.292 - INFO: Train epoch 370:   Loss: 22896.1044 | r_Loss: 3348.4326 | g_Loss: 4641.8152 | l_Loss: 1512.1261 | 
                                                                                 370		22896.1044		-4.5000
25-09-28 07:03:47.073 - INFO: Learning rate: 3.1622776601683795e-05
25-09-28 07:03:47.073 - INFO: Train epoch 371:   Loss: 22334.9970 | r_Loss: 3275.8244 | g_Loss: 4482.1125 | l_Loss: 1473.7627 | 
                                                                                 371		22334.9970		-4.5000
25-09-28 07:06:34.323 - INFO: Learning rate: 3.1622776601683795e-05
25-09-28 07:06:34.324 - INFO: Train epoch 372:   Loss: 21868.7705 | r_Loss: 3223.0442 | g_Loss: 4320.2012 | l_Loss: 1433.3484 | 
                                                                                 372		21868.7705		-4.5000
25-09-28 07:09:21.288 - INFO: Learning rate: 3.1622776601683795e-05
25-09-28 07:09:21.288 - INFO: Train epoch 373:   Loss: 21303.3339 | r_Loss: 3147.9418 | g_Loss: 4166.2835 | l_Loss: 1397.3413 | 
                                                                                 373		21303.3339		-4.5000
25-09-28 07:12:08.318 - INFO: Learning rate: 3.1622776601683795e-05
25-09-28 07:12:08.318 - INFO: Train epoch 374:   Loss: 20881.6188 | r_Loss: 3100.3743 | g_Loss: 4018.4656 | l_Loss: 1361.2819 | 
                                                                                 374		20881.6188		-4.5000
25-09-28 07:14:55.261 - INFO: Learning rate: 3.1622776601683795e-05
25-09-28 07:14:55.261 - INFO: Train epoch 375:   Loss: 20479.2336 | r_Loss: 3055.7824 | g_Loss: 3875.7169 | l_Loss: 1324.6048 | 
                                                                                 375		20479.2336		-4.5000
25-09-28 07:17:43.089 - INFO: Learning rate: 3.1622776601683795e-05
25-09-28 07:17:43.090 - INFO: Train epoch 376:   Loss: 20014.0022 | r_Loss: 2997.3749 | g_Loss: 3738.3662 | l_Loss: 1288.7617 | 
                                                                                 376		20014.0022		-4.5000
25-09-28 07:20:30.092 - INFO: Learning rate: 3.1622776601683795e-05
25-09-28 07:20:30.093 - INFO: Train epoch 377:   Loss: 19594.9228 | r_Loss: 2947.6537 | g_Loss: 3606.2584 | l_Loss: 1250.3957 | 
                                                                                 377		19594.9228		-4.5000
25-09-28 07:23:17.318 - INFO: Learning rate: 3.1622776601683795e-05
25-09-28 07:23:17.318 - INFO: Train epoch 378:   Loss: 19168.8100 | r_Loss: 2894.5157 | g_Loss: 3482.7366 | l_Loss: 1213.4948 | 
                                                                                 378		19168.8100		-4.5000
25-09-28 07:26:04.508 - INFO: Learning rate: 3.1622776601683795e-05
25-09-28 07:26:04.509 - INFO: Train epoch 379:   Loss: 18835.9955 | r_Loss: 2859.7631 | g_Loss: 3362.3204 | l_Loss: 1174.8597 | 
                                                                                 379		18835.9955		-4.5000
25-09-28 07:28:51.884 - INFO: Learning rate: 3.1622776601683795e-05
25-09-28 07:28:51.885 - INFO: Train epoch 380:   Loss: 18396.4194 | r_Loss: 2802.1377 | g_Loss: 3252.3476 | l_Loss: 1133.3833 | 
                                                                                 380		18396.4194		-4.5000
25-09-28 07:31:38.835 - INFO: Learning rate: 3.1622776601683795e-05
25-09-28 07:31:38.836 - INFO: Train epoch 381:   Loss: 17991.0878 | r_Loss: 2751.7904 | g_Loss: 3139.9393 | l_Loss: 1092.1963 | 
                                                                                 381		17991.0878		-4.5000
25-09-28 07:34:25.857 - INFO: Learning rate: 3.1622776601683795e-05
25-09-28 07:34:25.858 - INFO: Train epoch 382:   Loss: 17673.8731 | r_Loss: 2717.6253 | g_Loss: 3036.7245 | l_Loss: 1049.0222 | 
                                                                                 382		17673.8731		-4.5000
25-09-28 07:37:12.800 - INFO: Learning rate: 3.1622776601683795e-05
25-09-28 07:37:12.800 - INFO: Train epoch 383:   Loss: 17319.6691 | r_Loss: 2675.7653 | g_Loss: 2937.0956 | l_Loss: 1003.7470 | 
                                                                                 383		17319.6691		-4.5000
25-09-28 07:39:59.403 - INFO: Learning rate: 3.1622776601683795e-05
25-09-28 07:39:59.404 - INFO: Train epoch 384:   Loss: 17006.7792 | r_Loss: 2641.2941 | g_Loss: 2840.5702 | l_Loss: 959.7382 | 
                                                                                 384		17006.7792		-4.5000
25-09-28 07:42:46.104 - INFO: Learning rate: 3.1622776601683795e-05
25-09-28 07:42:46.105 - INFO: Train epoch 385:   Loss: 16579.4327 | r_Loss: 2585.4150 | g_Loss: 2741.0349 | l_Loss: 911.3228 | 
                                                                                 385		16579.4327		-4.5000
25-09-28 07:45:32.905 - INFO: Learning rate: 3.1622776601683795e-05
25-09-28 07:45:32.906 - INFO: Train epoch 386:   Loss: 16242.4739 | r_Loss: 2545.0783 | g_Loss: 2651.2774 | l_Loss: 865.8049 | 
                                                                                 386		16242.4739		-4.5000
25-09-28 07:48:19.767 - INFO: Learning rate: 3.1622776601683795e-05
25-09-28 07:48:19.768 - INFO: Train epoch 387:   Loss: 15871.6266 | r_Loss: 2499.3687 | g_Loss: 2556.6177 | l_Loss: 818.1652 | 
                                                                                 387		15871.6266		-4.5000
25-09-28 07:51:06.667 - INFO: Learning rate: 3.1622776601683795e-05
25-09-28 07:51:06.668 - INFO: Train epoch 388:   Loss: 15526.2524 | r_Loss: 2458.6901 | g_Loss: 2465.1570 | l_Loss: 767.6447 | 
                                                                                 388		15526.2524		-4.5000
25-09-28 07:53:53.405 - INFO: Learning rate: 3.1622776601683795e-05
25-09-28 07:53:53.406 - INFO: Train epoch 389:   Loss: 15170.0751 | r_Loss: 2415.1504 | g_Loss: 2374.3112 | l_Loss: 720.0120 | 
                                                                                 389		15170.0751		-4.5000
25-09-28 07:56:40.050 - INFO: Learning rate: 3.1622776601683795e-05
25-09-28 07:56:40.050 - INFO: Train epoch 390:   Loss: 14830.4486 | r_Loss: 2375.0447 | g_Loss: 2285.4103 | l_Loss: 669.8149 | 
                                                                                 390		14830.4486		-4.5000
25-09-28 07:59:26.772 - INFO: Learning rate: 3.1622776601683795e-05
25-09-28 07:59:26.772 - INFO: Train epoch 391:   Loss: 14587.8647 | r_Loss: 2354.0050 | g_Loss: 2198.5775 | l_Loss: 619.2623 | 
                                                                                 391		14587.8647		-4.5000
25-09-28 08:02:13.546 - INFO: Learning rate: 3.1622776601683795e-05
25-09-28 08:02:13.547 - INFO: Train epoch 392:   Loss: 14120.8887 | r_Loss: 2286.5589 | g_Loss: 2116.8896 | l_Loss: 571.2047 | 
                                                                                 392		14120.8887		-4.5000
25-09-28 08:05:00.626 - INFO: Learning rate: 3.1622776601683795e-05
25-09-28 08:05:00.627 - INFO: Train epoch 393:   Loss: 13852.3790 | r_Loss: 2257.9996 | g_Loss: 2038.2258 | l_Loss: 524.1554 | 
                                                                                 393		13852.3790		-4.5000
25-09-28 08:07:47.217 - INFO: Learning rate: 3.1622776601683795e-05
25-09-28 08:07:47.217 - INFO: Train epoch 394:   Loss: 13581.3425 | r_Loss: 2227.0885 | g_Loss: 1966.5631 | l_Loss: 479.3370 | 
                                                                                 394		13581.3425		-4.5000
25-09-28 08:10:33.933 - INFO: Learning rate: 3.1622776601683795e-05
25-09-28 08:10:33.934 - INFO: Train epoch 395:   Loss: 13262.8085 | r_Loss: 2185.8909 | g_Loss: 1897.1815 | l_Loss: 436.1724 | 
                                                                                 395		13262.8085		-4.5000
25-09-28 08:13:21.104 - INFO: Learning rate: 3.1622776601683795e-05
25-09-28 08:13:21.105 - INFO: Train epoch 396:   Loss: 13041.5679 | r_Loss: 2161.9079 | g_Loss: 1834.9506 | l_Loss: 397.0778 | 
                                                                                 396		13041.5679		-4.5000
25-09-28 08:16:07.916 - INFO: Learning rate: 3.1622776601683795e-05
25-09-28 08:16:07.917 - INFO: Train epoch 397:   Loss: 12771.5429 | r_Loss: 2127.2013 | g_Loss: 1774.7744 | l_Loss: 360.7622 | 
                                                                                 397		12771.5429		-4.5000
25-09-28 08:18:54.797 - INFO: Learning rate: 3.1622776601683795e-05
25-09-28 08:18:54.797 - INFO: Train epoch 398:   Loss: 12575.3695 | r_Loss: 2104.4819 | g_Loss: 1724.6462 | l_Loss: 328.3139 | 
                                                                                 398		12575.3695		-4.5000
25-09-28 08:21:41.787 - INFO: Learning rate: 3.1622776601683795e-05
25-09-28 08:21:41.788 - INFO: Train epoch 399:   Loss: 12260.2792 | r_Loss: 2056.2646 | g_Loss: 1681.1484 | l_Loss: 297.8081 | 
                                                                                 399		12260.2792		-4.5000
25-09-28 08:25:22.750 - INFO: TEST:   PSNR_S: 28.8785 | PSNR_C: 27.9221 | 
25-09-28 08:25:22.752 - INFO: Learning rate: 3.1622776601683795e-05
25-09-28 08:25:22.753 - INFO: Train epoch 400:   Loss: 11993.8697 | r_Loss: 2014.8605 | g_Loss: 1648.5039 | l_Loss: 271.0635 | 
                                                                                 400		11993.8697		-4.5000
25-09-28 08:28:10.474 - INFO: Learning rate: 3.1622776601683795e-05
25-09-28 08:28:10.474 - INFO: Train epoch 401:   Loss: 11670.9612 | r_Loss: 1958.8501 | g_Loss: 1630.4218 | l_Loss: 246.2892 | 
                                                                                 401		11670.9612		-4.5000
25-09-28 08:30:57.758 - INFO: Learning rate: 3.1622776601683795e-05
25-09-28 08:30:57.759 - INFO: Train epoch 402:   Loss: 11418.2153 | r_Loss: 1911.8841 | g_Loss: 1633.3347 | l_Loss: 225.4602 | 
                                                                                 402		11418.2153		-4.5000
25-09-28 08:33:44.955 - INFO: Learning rate: 3.1622776601683795e-05
25-09-28 08:33:44.956 - INFO: Train epoch 403:   Loss: 11152.7402 | r_Loss: 1857.4912 | g_Loss: 1658.2174 | l_Loss: 207.0668 | 
                                                                                 403		11152.7402		-4.5000
25-09-28 08:36:31.550 - INFO: Learning rate: 3.1622776601683795e-05
25-09-28 08:36:31.551 - INFO: Train epoch 404:   Loss: 10806.7083 | r_Loss: 1783.5314 | g_Loss: 1698.7133 | l_Loss: 190.3381 | 
                                                                                 404		10806.7083		-4.5000
25-09-28 08:39:18.179 - INFO: Learning rate: 3.1622776601683795e-05
25-09-28 08:39:18.180 - INFO: Train epoch 405:   Loss: 10433.4915 | r_Loss: 1702.6294 | g_Loss: 1743.8891 | l_Loss: 176.4552 | 
                                                                                 405		10433.4915		-4.5000
25-09-28 08:42:04.756 - INFO: Learning rate: 3.1622776601683795e-05
25-09-28 08:42:04.756 - INFO: Train epoch 406:   Loss: 9983.8087 | r_Loss: 1606.4212 | g_Loss: 1788.1317 | l_Loss: 163.5709 | 
                                                                                 406		9983.8087		-4.5000
25-09-28 08:44:51.373 - INFO: Learning rate: 3.1622776601683795e-05
25-09-28 08:44:51.373 - INFO: Train epoch 407:   Loss: 9568.4608 | r_Loss: 1521.4292 | g_Loss: 1808.4991 | l_Loss: 152.8157 | 
                                                                                 407		9568.4608		-4.5000
25-09-28 08:47:37.684 - INFO: Learning rate: 3.1622776601683795e-05
25-09-28 08:47:37.685 - INFO: Train epoch 408:   Loss: 9028.3395 | r_Loss: 1430.3403 | g_Loss: 1733.5252 | l_Loss: 143.1125 | 
                                                                                 408		9028.3395		-4.5000
25-09-28 08:50:24.594 - INFO: Learning rate: 3.1622776601683795e-05
25-09-28 08:50:24.594 - INFO: Train epoch 409:   Loss: 8734.3760 | r_Loss: 1397.4983 | g_Loss: 1612.3740 | l_Loss: 134.5105 | 
                                                                                 409		8734.3760		-4.5000
25-09-28 08:53:11.905 - INFO: Learning rate: 3.1622776601683795e-05
25-09-28 08:53:11.906 - INFO: Train epoch 410:   Loss: 8555.5803 | r_Loss: 1380.6512 | g_Loss: 1524.7979 | l_Loss: 127.5266 | 
                                                                                 410		8555.5803		-4.5000
25-09-28 08:55:59.074 - INFO: Learning rate: 3.1622776601683795e-05
25-09-28 08:55:59.075 - INFO: Train epoch 411:   Loss: 8344.1947 | r_Loss: 1351.6212 | g_Loss: 1465.3527 | l_Loss: 120.7360 | 
                                                                                 411		8344.1947		-4.5000
25-09-28 08:58:46.249 - INFO: Learning rate: 3.1622776601683795e-05
25-09-28 08:58:46.250 - INFO: Train epoch 412:   Loss: 8108.7938 | r_Loss: 1316.4220 | g_Loss: 1412.7428 | l_Loss: 113.9411 | 
                                                                                 412		8108.7938		-4.5000
25-09-28 09:01:33.391 - INFO: Learning rate: 3.1622776601683795e-05
25-09-28 09:01:33.392 - INFO: Train epoch 413:   Loss: 7973.4909 | r_Loss: 1297.9068 | g_Loss: 1376.1147 | l_Loss: 107.8424 | 
                                                                                 413		7973.4909		-4.5000
25-09-28 09:04:20.426 - INFO: Learning rate: 3.1622776601683795e-05
25-09-28 09:04:20.427 - INFO: Train epoch 414:   Loss: 7967.3959 | r_Loss: 1301.0721 | g_Loss: 1358.5090 | l_Loss: 103.5262 | 
                                                                                 414		7967.3959		-4.5000
25-09-28 09:07:07.128 - INFO: Learning rate: 3.1622776601683795e-05
25-09-28 09:07:07.129 - INFO: Train epoch 415:   Loss: 7757.8382 | r_Loss: 1266.4625 | g_Loss: 1327.9514 | l_Loss: 97.5743 | 
                                                                                 415		7757.8382		-4.5000
25-09-28 09:09:53.868 - INFO: Learning rate: 3.1622776601683795e-05
25-09-28 09:09:53.869 - INFO: Train epoch 416:   Loss: 7570.7337 | r_Loss: 1235.2492 | g_Loss: 1301.8179 | l_Loss: 92.6699 | 
                                                                                 416		7570.7337		-4.5000
25-09-28 09:12:40.687 - INFO: Learning rate: 3.1622776601683795e-05
25-09-28 09:12:40.688 - INFO: Train epoch 417:   Loss: 7593.4198 | r_Loss: 1242.7713 | g_Loss: 1290.5924 | l_Loss: 88.9707 | 
                                                                                 417		7593.4198		-4.5000
25-09-28 09:15:27.687 - INFO: Learning rate: 3.1622776601683795e-05
25-09-28 09:15:27.687 - INFO: Train epoch 418:   Loss: 7390.2292 | r_Loss: 1209.1086 | g_Loss: 1260.8008 | l_Loss: 83.8855 | 
                                                                                 418		7390.2292		-4.5000
25-09-28 09:18:14.635 - INFO: Learning rate: 3.1622776601683795e-05
25-09-28 09:18:14.635 - INFO: Train epoch 419:   Loss: 7356.3268 | r_Loss: 1206.2526 | g_Loss: 1245.1962 | l_Loss: 79.8678 | 
                                                                                 419		7356.3268		-4.5000
25-09-28 09:21:01.369 - INFO: Learning rate: 3.1622776601683795e-05
25-09-28 09:21:01.370 - INFO: Train epoch 420:   Loss: 7234.6457 | r_Loss: 1185.2689 | g_Loss: 1231.8079 | l_Loss: 76.4932 | 
                                                                                 420		7234.6457		-4.5000
25-09-28 09:23:48.099 - INFO: Learning rate: 3.1622776601683795e-05
25-09-28 09:23:48.100 - INFO: Train epoch 421:   Loss: 7188.3205 | r_Loss: 1180.1091 | g_Loss: 1214.9611 | l_Loss: 72.8140 | 
                                                                                 421		7188.3205		-4.5000
25-09-28 09:26:34.730 - INFO: Learning rate: 3.1622776601683795e-05
25-09-28 09:26:34.730 - INFO: Train epoch 422:   Loss: 7168.1828 | r_Loss: 1176.8697 | g_Loss: 1213.2175 | l_Loss: 70.6166 | 
                                                                                 422		7168.1828		-4.5000
25-09-28 09:29:21.548 - INFO: Learning rate: 3.1622776601683795e-05
25-09-28 09:29:21.549 - INFO: Train epoch 423:   Loss: 6943.2643 | r_Loss: 1136.8028 | g_Loss: 1193.0399 | l_Loss: 66.2105 | 
                                                                                 423		6943.2643		-4.5000
25-09-28 09:32:08.206 - INFO: Learning rate: 3.1622776601683795e-05
25-09-28 09:32:08.207 - INFO: Train epoch 424:   Loss: 6928.3481 | r_Loss: 1137.8549 | g_Loss: 1175.8942 | l_Loss: 63.1794 | 
                                                                                 424		6928.3481		-4.5000
25-09-28 09:34:55.133 - INFO: Learning rate: 3.1622776601683795e-05
25-09-28 09:34:55.133 - INFO: Train epoch 425:   Loss: 6757.9047 | r_Loss: 1104.9601 | g_Loss: 1172.6724 | l_Loss: 60.4318 | 
                                                                                 425		6757.9047		-4.5000
25-09-28 09:37:42.201 - INFO: Learning rate: 3.1622776601683795e-05
25-09-28 09:37:42.201 - INFO: Train epoch 426:   Loss: 6812.2780 | r_Loss: 1116.1639 | g_Loss: 1172.3612 | l_Loss: 59.0971 | 
                                                                                 426		6812.2780		-4.5000
25-09-28 09:40:28.863 - INFO: Learning rate: 3.1622776601683795e-05
25-09-28 09:40:28.864 - INFO: Train epoch 427:   Loss: 6523.5015 | r_Loss: 1061.2258 | g_Loss: 1162.1718 | l_Loss: 55.2006 | 
                                                                                 427		6523.5015		-4.5000
25-09-28 09:43:15.721 - INFO: Learning rate: 3.1622776601683795e-05
25-09-28 09:43:15.722 - INFO: Train epoch 428:   Loss: 6395.9982 | r_Loss: 1036.7715 | g_Loss: 1159.0499 | l_Loss: 53.0907 | 
                                                                                 428		6395.9982		-4.5000
25-09-28 09:46:02.848 - INFO: Learning rate: 3.1622776601683795e-05
25-09-28 09:46:02.849 - INFO: Train epoch 429:   Loss: 6214.1025 | r_Loss: 998.5003 | g_Loss: 1169.7406 | l_Loss: 51.8604 | 
                                                                                 429		6214.1025		-4.5000
25-09-28 09:48:50.299 - INFO: Learning rate: 3.1622776601683795e-05
25-09-28 09:48:50.300 - INFO: Train epoch 430:   Loss: 5928.7305 | r_Loss: 937.2614 | g_Loss: 1191.1781 | l_Loss: 51.2455 | 
                                                                                 430		5928.7305		-4.5000
25-09-28 09:51:37.295 - INFO: Learning rate: 3.1622776601683795e-05
25-09-28 09:51:37.296 - INFO: Train epoch 431:   Loss: 5339.8150 | r_Loss: 819.1405 | g_Loss: 1193.5390 | l_Loss: 50.5733 | 
                                                                                 431		5339.8150		-4.5000
25-09-28 09:54:24.196 - INFO: Learning rate: 3.1622776601683795e-05
25-09-28 09:54:24.196 - INFO: Train epoch 432:   Loss: 4695.1159 | r_Loss: 704.0216 | g_Loss: 1127.3931 | l_Loss: 47.6150 | 
                                                                                 432		4695.1159		-4.5000
25-09-28 09:57:10.993 - INFO: Learning rate: 3.1622776601683795e-05
25-09-28 09:57:10.994 - INFO: Train epoch 433:   Loss: 4551.0471 | r_Loss: 680.4001 | g_Loss: 1102.9248 | l_Loss: 46.1220 | 
                                                                                 433		4551.0471		-4.5000
25-09-28 09:59:57.998 - INFO: Learning rate: 3.1622776601683795e-05
25-09-28 09:59:57.998 - INFO: Train epoch 434:   Loss: 4131.7708 | r_Loss: 601.9725 | g_Loss: 1078.5020 | l_Loss: 43.4065 | 
                                                                                 434		4131.7708		-4.5000
25-09-28 10:02:45.107 - INFO: Learning rate: 3.1622776601683795e-05
25-09-28 10:02:45.108 - INFO: Train epoch 435:   Loss: 3681.9533 | r_Loss: 513.1326 | g_Loss: 1073.3245 | l_Loss: 42.9656 | 
                                                                                 435		3681.9533		-4.5000
25-09-28 10:05:31.941 - INFO: Learning rate: 3.1622776601683795e-05
25-09-28 10:05:31.942 - INFO: Train epoch 436:   Loss: 3235.5479 | r_Loss: 431.9675 | g_Loss: 1034.4240 | l_Loss: 41.2863 | 
                                                                                 436		3235.5479		-4.5000
25-09-28 10:08:18.737 - INFO: Learning rate: 3.1622776601683795e-05
25-09-28 10:08:18.738 - INFO: Train epoch 437:   Loss: 3299.2801 | r_Loss: 452.0705 | g_Loss: 999.6163 | l_Loss: 39.3112 | 
                                                                                 437		3299.2801		-4.5000
25-09-28 10:11:05.548 - INFO: Learning rate: 3.1622776601683795e-05
25-09-28 10:11:05.549 - INFO: Train epoch 438:   Loss: 3069.0479 | r_Loss: 411.1328 | g_Loss: 976.1974 | l_Loss: 37.1865 | 
                                                                                 438		3069.0479		-4.5000
25-09-28 10:13:52.479 - INFO: Learning rate: 3.1622776601683795e-05
25-09-28 10:13:52.480 - INFO: Train epoch 439:   Loss: 2987.5663 | r_Loss: 396.7637 | g_Loss: 967.3424 | l_Loss: 36.4056 | 
                                                                                 439		2987.5663		-4.5000
25-09-28 10:16:39.228 - INFO: Learning rate: 3.1622776601683795e-05
25-09-28 10:16:39.229 - INFO: Train epoch 440:   Loss: 2880.9049 | r_Loss: 380.6497 | g_Loss: 942.8377 | l_Loss: 34.8188 | 
                                                                                 440		2880.9049		-4.5000
25-09-28 10:19:26.070 - INFO: Learning rate: 3.1622776601683795e-05
25-09-28 10:19:26.072 - INFO: Train epoch 441:   Loss: 2780.4675 | r_Loss: 364.3953 | g_Loss: 925.0345 | l_Loss: 33.4568 | 
                                                                                 441		2780.4675		-4.5000
25-09-28 10:22:12.761 - INFO: Learning rate: 3.1622776601683795e-05
25-09-28 10:22:12.762 - INFO: Train epoch 442:   Loss: 2664.9992 | r_Loss: 346.3137 | g_Loss: 901.3153 | l_Loss: 32.1155 | 
                                                                                 442		2664.9992		-4.5000
25-09-28 10:24:59.584 - INFO: Learning rate: 3.1622776601683795e-05
25-09-28 10:24:59.584 - INFO: Train epoch 443:   Loss: 3113.5437 | r_Loss: 435.2431 | g_Loss: 903.6685 | l_Loss: 33.6596 | 
                                                                                 443		3113.5437		-4.5000
25-09-28 10:27:46.342 - INFO: Learning rate: 3.1622776601683795e-05
25-09-28 10:27:46.343 - INFO: Train epoch 444:   Loss: 2378.1285 | r_Loss: 289.4150 | g_Loss: 900.5045 | l_Loss: 30.5488 | 
                                                                                 444		2378.1285		-4.5000
25-09-28 10:30:33.224 - INFO: Learning rate: 3.1622776601683795e-05
25-09-28 10:30:33.225 - INFO: Train epoch 445:   Loss: 2594.7134 | r_Loss: 339.6092 | g_Loss: 866.2893 | l_Loss: 30.3779 | 
                                                                                 445		2594.7134		-4.5000
25-09-28 10:33:20.174 - INFO: Learning rate: 3.1622776601683795e-05
25-09-28 10:33:20.174 - INFO: Train epoch 446:   Loss: 2370.3487 | r_Loss: 299.4537 | g_Loss: 844.7338 | l_Loss: 28.3464 | 
                                                                                 446		2370.3487		-4.5000
25-09-28 10:36:07.388 - INFO: Learning rate: 3.1622776601683795e-05
25-09-28 10:36:07.389 - INFO: Train epoch 447:   Loss: 2374.5129 | r_Loss: 302.2793 | g_Loss: 835.0202 | l_Loss: 28.0962 | 
                                                                                 447		2374.5129		-4.5000
25-09-28 10:38:54.183 - INFO: Learning rate: 3.1622776601683795e-05
25-09-28 10:38:54.184 - INFO: Train epoch 448:   Loss: 2343.0815 | r_Loss: 300.7736 | g_Loss: 811.9985 | l_Loss: 27.2152 | 
                                                                                 448		2343.0815		-4.5000
25-09-28 10:41:41.061 - INFO: Learning rate: 3.1622776601683795e-05
25-09-28 10:41:41.062 - INFO: Train epoch 449:   Loss: 2310.5179 | r_Loss: 297.3764 | g_Loss: 797.4296 | l_Loss: 26.2064 | 
                                                                                 449		2310.5179		-4.5000
25-09-28 10:45:21.625 - INFO: TEST:   PSNR_S: 36.1670 | PSNR_C: 31.2651 | 
25-09-28 10:45:21.627 - INFO: Learning rate: 3.1622776601683795e-05
25-09-28 10:45:21.628 - INFO: Train epoch 450:   Loss: 2218.2114 | r_Loss: 279.5926 | g_Loss: 794.1344 | l_Loss: 26.1143 | 
                                                                                 450		2218.2114		-4.5000
25-09-28 10:48:08.785 - INFO: Learning rate: 3.1622776601683795e-05
25-09-28 10:48:08.785 - INFO: Train epoch 451:   Loss: 2140.7670 | r_Loss: 268.8421 | g_Loss: 771.9509 | l_Loss: 24.6058 | 
                                                                                 451		2140.7670		-4.5000
25-09-28 10:50:56.005 - INFO: Learning rate: 3.1622776601683795e-05
25-09-28 10:50:56.005 - INFO: Train epoch 452:   Loss: 2073.8708 | r_Loss: 258.3724 | g_Loss: 757.8329 | l_Loss: 24.1760 | 
                                                                                 452		2073.8708		-4.5000
25-09-28 10:53:43.094 - INFO: Learning rate: 3.1622776601683795e-05
25-09-28 10:53:43.095 - INFO: Train epoch 453:   Loss: 2065.3980 | r_Loss: 258.8530 | g_Loss: 747.2450 | l_Loss: 23.8883 | 
                                                                                 453		2065.3980		-4.5000
25-09-28 10:56:30.042 - INFO: Learning rate: 3.1622776601683795e-05
25-09-28 10:56:30.043 - INFO: Train epoch 454:   Loss: 2060.3129 | r_Loss: 259.5925 | g_Loss: 739.0205 | l_Loss: 23.3297 | 
                                                                                 454		2060.3129		-4.5000
25-09-28 10:59:16.983 - INFO: Learning rate: 3.1622776601683795e-05
25-09-28 10:59:16.984 - INFO: Train epoch 455:   Loss: 2250.7552 | r_Loss: 297.7301 | g_Loss: 737.3989 | l_Loss: 24.7059 | 
                                                                                 455		2250.7552		-4.5000
25-09-28 11:02:03.987 - INFO: Learning rate: 3.1622776601683795e-05
25-09-28 11:02:03.988 - INFO: Train epoch 456:   Loss: 1756.8600 | r_Loss: 201.8159 | g_Loss: 725.4119 | l_Loss: 22.3686 | 
                                                                                 456		1756.8600		-4.5000
25-09-28 11:04:51.033 - INFO: Learning rate: 3.1622776601683795e-05
25-09-28 11:04:51.034 - INFO: Train epoch 457:   Loss: 1930.4758 | r_Loss: 239.8351 | g_Loss: 708.7238 | l_Loss: 22.5765 | 
                                                                                 457		1930.4758		-4.5000
25-09-28 11:07:37.663 - INFO: Learning rate: 3.1622776601683795e-05
25-09-28 11:07:37.664 - INFO: Train epoch 458:   Loss: 1839.5919 | r_Loss: 225.5294 | g_Loss: 689.6337 | l_Loss: 22.3113 | 
                                                                                 458		1839.5919		-4.5000
25-09-28 11:10:24.442 - INFO: Learning rate: 3.1622776601683795e-05
25-09-28 11:10:24.443 - INFO: Train epoch 459:   Loss: 1905.7409 | r_Loss: 240.3132 | g_Loss: 681.6313 | l_Loss: 22.5437 | 
                                                                                 459		1905.7409		-4.5000
25-09-28 11:13:11.074 - INFO: Learning rate: 3.1622776601683795e-05
25-09-28 11:13:11.075 - INFO: Train epoch 460:   Loss: 1835.0482 | r_Loss: 229.1622 | g_Loss: 668.0543 | l_Loss: 21.1828 | 
                                                                                 460		1835.0482		-4.5000
25-09-28 11:15:57.784 - INFO: Learning rate: 3.1622776601683795e-05
25-09-28 11:15:57.784 - INFO: Train epoch 461:   Loss: 1687.4163 | r_Loss: 200.5626 | g_Loss: 662.1862 | l_Loss: 22.4169 | 
                                                                                 461		1687.4163		-4.5000
25-09-28 11:18:44.521 - INFO: Learning rate: 3.1622776601683795e-05
25-09-28 11:18:44.522 - INFO: Train epoch 462:   Loss: 1687.9285 | r_Loss: 203.4616 | g_Loss: 650.0577 | l_Loss: 20.5627 | 
                                                                                 462		1687.9285		-4.5000
25-09-28 11:21:31.286 - INFO: Learning rate: 3.1622776601683795e-05
25-09-28 11:21:31.287 - INFO: Train epoch 463:   Loss: 1579.9926 | r_Loss: 186.1059 | g_Loss: 629.1151 | l_Loss: 20.3479 | 
                                                                                 463		1579.9926		-4.5000
25-09-28 11:24:18.077 - INFO: Learning rate: 3.1622776601683795e-05
25-09-28 11:24:18.078 - INFO: Train epoch 464:   Loss: 1643.1100 | r_Loss: 199.7458 | g_Loss: 623.2323 | l_Loss: 21.1488 | 
                                                                                 464		1643.1100		-4.5000
25-09-28 11:27:04.694 - INFO: Learning rate: 3.1622776601683795e-05
25-09-28 11:27:04.695 - INFO: Train epoch 465:   Loss: 1732.7026 | r_Loss: 215.4863 | g_Loss: 631.7686 | l_Loss: 23.5025 | 
                                                                                 465		1732.7026		-4.5000
25-09-28 11:29:51.317 - INFO: Learning rate: 3.1622776601683795e-05
25-09-28 11:29:51.318 - INFO: Train epoch 466:   Loss: 1446.6351 | r_Loss: 164.2493 | g_Loss: 601.8243 | l_Loss: 23.5640 | 
                                                                                 466		1446.6351		-4.5000
25-09-28 11:32:38.259 - INFO: Learning rate: 3.1622776601683795e-05
25-09-28 11:32:38.260 - INFO: Train epoch 467:   Loss: 1650.5455 | r_Loss: 207.2456 | g_Loss: 592.0034 | l_Loss: 22.3139 | 
                                                                                 467		1650.5455		-4.5000
25-09-28 11:35:25.072 - INFO: Learning rate: 3.1622776601683795e-05
25-09-28 11:35:25.073 - INFO: Train epoch 468:   Loss: 1382.4896 | r_Loss: 155.9322 | g_Loss: 582.0678 | l_Loss: 20.7609 | 
                                                                                 468		1382.4896		-4.5000
25-09-28 11:38:11.939 - INFO: Learning rate: 3.1622776601683795e-05
25-09-28 11:38:11.940 - INFO: Train epoch 469:   Loss: 1469.1767 | r_Loss: 175.5454 | g_Loss: 571.2375 | l_Loss: 20.2123 | 
                                                                                 469		1469.1767		-4.5000
25-09-28 11:40:58.864 - INFO: Learning rate: 3.1622776601683795e-05
25-09-28 11:40:58.864 - INFO: Train epoch 470:   Loss: 1391.8358 | r_Loss: 161.6484 | g_Loss: 559.1860 | l_Loss: 24.4080 | 
                                                                                 470		1391.8358		-4.5000
25-09-28 11:43:45.969 - INFO: Learning rate: 3.1622776601683795e-05
25-09-28 11:43:45.970 - INFO: Train epoch 471:   Loss: 1398.2090 | r_Loss: 165.3996 | g_Loss: 546.6952 | l_Loss: 24.5158 | 
                                                                                 471		1398.2090		-4.5000
25-09-28 11:46:32.953 - INFO: Learning rate: 3.1622776601683795e-05
25-09-28 11:46:32.954 - INFO: Train epoch 472:   Loss: 1391.3190 | r_Loss: 163.5974 | g_Loss: 549.4161 | l_Loss: 23.9157 | 
                                                                                 472		1391.3190		-4.5000
25-09-28 11:49:20.332 - INFO: Learning rate: 3.1622776601683795e-05
25-09-28 11:49:20.333 - INFO: Train epoch 473:   Loss: 1338.4020 | r_Loss: 157.0963 | g_Loss: 529.6044 | l_Loss: 23.3162 | 
                                                                                 473		1338.4020		-4.5000
25-09-28 11:52:07.750 - INFO: Learning rate: 3.1622776601683795e-05
25-09-28 11:52:07.751 - INFO: Train epoch 474:   Loss: 1301.8524 | r_Loss: 151.4066 | g_Loss: 522.4875 | l_Loss: 22.3320 | 
                                                                                 474		1301.8524		-4.5000
25-09-28 11:54:54.823 - INFO: Learning rate: 3.1622776601683795e-05
25-09-28 11:54:54.824 - INFO: Train epoch 475:   Loss: 1344.3271 | r_Loss: 157.2042 | g_Loss: 522.3212 | l_Loss: 35.9848 | 
                                                                                 475		1344.3271		-4.5000
25-09-28 11:57:42.024 - INFO: Learning rate: 3.1622776601683795e-05
25-09-28 11:57:42.025 - INFO: Train epoch 476:   Loss: 1198.8760 | r_Loss: 136.0773 | g_Loss: 499.3190 | l_Loss: 19.1703 | 
                                                                                 476		1198.8760		-4.5000
25-09-28 12:00:29.008 - INFO: Learning rate: 3.1622776601683795e-05
25-09-28 12:00:29.009 - INFO: Train epoch 477:   Loss: 1233.7029 | r_Loss: 144.1525 | g_Loss: 490.4943 | l_Loss: 22.4460 | 
                                                                                 477		1233.7029		-4.5000
25-09-28 12:03:15.905 - INFO: Learning rate: 3.1622776601683795e-05
25-09-28 12:03:15.905 - INFO: Train epoch 478:   Loss: 1179.5242 | r_Loss: 134.5588 | g_Loss: 483.5166 | l_Loss: 23.2139 | 
                                                                                 478		1179.5242		-4.5000
25-09-28 12:06:02.886 - INFO: Learning rate: 3.1622776601683795e-05
25-09-28 12:06:02.887 - INFO: Train epoch 479:   Loss: 1342.4921 | r_Loss: 158.5097 | g_Loss: 507.4127 | l_Loss: 42.5310 | 
                                                                                 479		1342.4921		-4.5000
25-09-28 12:08:49.895 - INFO: Learning rate: 3.1622776601683795e-05
25-09-28 12:08:49.895 - INFO: Train epoch 480:   Loss: 1029.5608 | r_Loss: 110.0047 | g_Loss: 464.5508 | l_Loss: 14.9866 | 
                                                                                 480		1029.5608		-4.5000
25-09-28 12:11:36.517 - INFO: Learning rate: 3.1622776601683795e-05
25-09-28 12:11:36.518 - INFO: Train epoch 481:   Loss: 1122.6220 | r_Loss: 127.7417 | g_Loss: 457.4839 | l_Loss: 26.4294 | 
                                                                                 481		1122.6220		-4.5000
25-09-28 12:14:23.133 - INFO: Learning rate: 3.1622776601683795e-05
25-09-28 12:14:23.134 - INFO: Train epoch 482:   Loss: 1079.3637 | r_Loss: 120.8333 | g_Loss: 448.7789 | l_Loss: 26.4185 | 
                                                                                 482		1079.3637		-4.5000
25-09-28 12:17:09.867 - INFO: Learning rate: 3.1622776601683795e-05
25-09-28 12:17:09.868 - INFO: Train epoch 483:   Loss: 1092.0780 | r_Loss: 123.9760 | g_Loss: 444.1083 | l_Loss: 28.0898 | 
                                                                                 483		1092.0780		-4.5000
25-09-28 12:19:56.593 - INFO: Learning rate: 3.1622776601683795e-05
25-09-28 12:19:56.594 - INFO: Train epoch 484:   Loss: 1096.3653 | r_Loss: 123.4672 | g_Loss: 443.2733 | l_Loss: 35.7562 | 
                                                                                 484		1096.3653		-4.5000
25-09-28 12:22:43.370 - INFO: Learning rate: 3.1622776601683795e-05
25-09-28 12:22:43.371 - INFO: Train epoch 485:   Loss: 969.6867 | r_Loss: 107.6036 | g_Loss: 414.8175 | l_Loss: 16.8513 | 
                                                                                 485		969.6867		-4.5000
25-09-28 12:25:30.216 - INFO: Learning rate: 3.1622776601683795e-05
25-09-28 12:25:30.217 - INFO: Train epoch 486:   Loss: 1074.9892 | r_Loss: 121.1355 | g_Loss: 435.3617 | l_Loss: 33.9498 | 
                                                                                 486		1074.9892		-4.5000
25-09-28 12:28:17.207 - INFO: Learning rate: 3.1622776601683795e-05
25-09-28 12:28:17.208 - INFO: Train epoch 487:   Loss: 975.2727 | r_Loss: 106.6096 | g_Loss: 411.6592 | l_Loss: 30.5653 | 
                                                                                 487		975.2727		-4.5000
25-09-28 12:31:03.802 - INFO: Learning rate: 3.1622776601683795e-05
25-09-28 12:31:03.803 - INFO: Train epoch 488:   Loss: 941.3954 | r_Loss: 104.8844 | g_Loss: 394.7625 | l_Loss: 22.2112 | 
                                                                                 488		941.3954		-4.5000
25-09-28 12:33:51.167 - INFO: Learning rate: 3.1622776601683795e-05
25-09-28 12:33:51.168 - INFO: Train epoch 489:   Loss: 949.3247 | r_Loss: 105.9040 | g_Loss: 393.3921 | l_Loss: 26.4123 | 
                                                                                 489		949.3247		-4.5000
25-09-28 12:36:38.628 - INFO: Learning rate: 3.1622776601683795e-05
25-09-28 12:36:38.629 - INFO: Train epoch 490:   Loss: 20205.1063 | r_Loss: 3131.6084 | g_Loss: 3618.1680 | l_Loss: 928.8956 | 
                                                                                 490		20205.1063		-4.5000
25-09-28 12:39:25.597 - INFO: Learning rate: 3.1622776601683795e-05
25-09-28 12:39:25.597 - INFO: Train epoch 491:   Loss: 2964.6894 | r_Loss: 275.6631 | g_Loss: 1530.9699 | l_Loss: 55.4040 | 
                                                                                 491		2964.6894		-4.5000
25-09-28 12:42:12.676 - INFO: Learning rate: 3.1622776601683795e-05
25-09-28 12:42:12.676 - INFO: Train epoch 492:   Loss: 2021.6592 | r_Loss: 197.5050 | g_Loss: 1005.3604 | l_Loss: 28.7740 | 
                                                                                 492		2021.6592		-4.5000
25-09-28 12:44:59.659 - INFO: Learning rate: 3.1622776601683795e-05
25-09-28 12:44:59.660 - INFO: Train epoch 493:   Loss: 1691.9902 | r_Loss: 175.0262 | g_Loss: 794.6956 | l_Loss: 22.1636 | 
                                                                                 493		1691.9902		-4.5000
25-09-28 12:47:46.490 - INFO: Learning rate: 3.1622776601683795e-05
25-09-28 12:47:46.491 - INFO: Train epoch 494:   Loss: 1427.3113 | r_Loss: 145.4736 | g_Loss: 681.2877 | l_Loss: 18.6555 | 
                                                                                 494		1427.3113		-4.5000
25-09-28 12:50:33.378 - INFO: Learning rate: 3.1622776601683795e-05
25-09-28 12:50:33.379 - INFO: Train epoch 495:   Loss: 1361.0565 | r_Loss: 148.3531 | g_Loss: 602.4188 | l_Loss: 16.8720 | 
                                                                                 495		1361.0565		-4.5000
25-09-28 12:53:20.812 - INFO: Learning rate: 3.1622776601683795e-05
25-09-28 12:53:20.813 - INFO: Train epoch 496:   Loss: 1242.0393 | r_Loss: 133.6755 | g_Loss: 557.9387 | l_Loss: 15.7228 | 
                                                                                 496		1242.0393		-4.5000
25-09-28 12:56:07.765 - INFO: Learning rate: 3.1622776601683795e-05
25-09-28 12:56:07.766 - INFO: Train epoch 497:   Loss: 1132.4055 | r_Loss: 120.7899 | g_Loss: 513.8774 | l_Loss: 14.5787 | 
                                                                                 497		1132.4055		-4.5000
25-09-28 12:58:54.830 - INFO: Learning rate: 3.1622776601683795e-05
25-09-28 12:58:54.832 - INFO: Train epoch 498:   Loss: 1128.5327 | r_Loss: 125.1371 | g_Loss: 488.4024 | l_Loss: 14.4448 | 
                                                                                 498		1128.5327		-4.5000
25-09-28 13:01:41.950 - INFO: Learning rate: 3.1622776601683795e-05
25-09-28 13:01:41.951 - INFO: Train epoch 499:   Loss: 1218.8278 | r_Loss: 140.8866 | g_Loss: 497.7262 | l_Loss: 16.6685 | 
                                                                                 499		1218.8278		-4.5000
25-09-28 13:05:22.363 - INFO: TEST:   PSNR_S: 43.1987 | PSNR_C: 33.8112 | 
25-09-28 13:05:22.364 - INFO: Learning rate: 3.1622776601683795e-05
25-09-28 13:05:22.365 - INFO: Train epoch 500:   Loss: 943.5155 | r_Loss: 96.3898 | g_Loss: 449.4028 | l_Loss: 12.1636 | 
                                                                                 500		943.5155		-4.5000
25-09-28 13:08:09.587 - INFO: Learning rate: 3.1622776601683795e-05
25-09-28 13:08:09.588 - INFO: Train epoch 501:   Loss: 971.6037 | r_Loss: 106.6280 | g_Loss: 426.1569 | l_Loss: 12.3068 | 
                                                                                 501		971.6037		-4.5000
25-09-28 13:10:56.847 - INFO: Learning rate: 3.1622776601683795e-05
25-09-28 13:10:56.848 - INFO: Train epoch 502:   Loss: 960.4640 | r_Loss: 107.6660 | g_Loss: 410.5017 | l_Loss: 11.6322 | 
                                                                                 502		960.4640		-4.5000
25-09-28 13:13:44.385 - INFO: Learning rate: 3.1622776601683795e-05
25-09-28 13:13:44.386 - INFO: Train epoch 503:   Loss: 917.1428 | r_Loss: 102.0698 | g_Loss: 395.4915 | l_Loss: 11.3023 | 
                                                                                 503		917.1428		-4.5000
25-09-28 13:16:31.085 - INFO: Learning rate: 3.1622776601683795e-05
25-09-28 13:16:31.086 - INFO: Train epoch 504:   Loss: 884.0688 | r_Loss: 98.3145 | g_Loss: 381.9312 | l_Loss: 10.5652 | 
                                                                                 504		884.0688		-4.5000
25-09-28 13:19:17.825 - INFO: Learning rate: 3.1622776601683795e-05
25-09-28 13:19:17.826 - INFO: Train epoch 505:   Loss: 804.9344 | r_Loss: 85.7429 | g_Loss: 366.1751 | l_Loss: 10.0446 | 
                                                                                 505		804.9344		-4.5000
25-09-28 13:22:04.681 - INFO: Learning rate: 3.1622776601683795e-05
25-09-28 13:22:04.681 - INFO: Train epoch 506:   Loss: 836.5396 | r_Loss: 93.9378 | g_Loss: 356.8272 | l_Loss: 10.0233 | 
                                                                                 506		836.5396		-4.5000
25-09-28 13:24:51.486 - INFO: Learning rate: 3.1622776601683795e-05
25-09-28 13:24:51.487 - INFO: Train epoch 507:   Loss: 811.5855 | r_Loss: 90.5995 | g_Loss: 348.4135 | l_Loss: 10.1746 | 
                                                                                 507		811.5855		-4.5000
25-09-28 13:27:38.504 - INFO: Learning rate: 3.1622776601683795e-05
25-09-28 13:27:38.505 - INFO: Train epoch 508:   Loss: 792.5402 | r_Loss: 88.3733 | g_Loss: 340.2530 | l_Loss: 10.4206 | 
                                                                                 508		792.5402		-4.5000
25-09-28 13:30:25.787 - INFO: Learning rate: 3.1622776601683795e-05
25-09-28 13:30:25.788 - INFO: Train epoch 509:   Loss: 784.1733 | r_Loss: 88.3812 | g_Loss: 330.6396 | l_Loss: 11.6277 | 
                                                                                 509		784.1733		-4.5000
25-09-28 13:33:12.849 - INFO: Learning rate: 3.1622776601683795e-05
25-09-28 13:33:12.850 - INFO: Train epoch 510:   Loss: 758.2928 | r_Loss: 83.4878 | g_Loss: 329.7069 | l_Loss: 11.1468 | 
                                                                                 510		758.2928		-4.5000
25-09-28 13:36:00.040 - INFO: Learning rate: 3.1622776601683795e-05
25-09-28 13:36:00.041 - INFO: Train epoch 511:   Loss: 757.1993 | r_Loss: 83.6651 | g_Loss: 323.8925 | l_Loss: 14.9814 | 
                                                                                 511		757.1993		-4.5000
25-09-28 13:38:47.206 - INFO: Learning rate: 3.1622776601683795e-05
25-09-28 13:38:47.206 - INFO: Train epoch 512:   Loss: 710.6714 | r_Loss: 78.2424 | g_Loss: 308.6256 | l_Loss: 10.8339 | 
                                                                                 512		710.6714		-4.5000
25-09-28 13:41:34.098 - INFO: Learning rate: 3.1622776601683795e-05
25-09-28 13:41:34.098 - INFO: Train epoch 513:   Loss: 737.1259 | r_Loss: 79.9465 | g_Loss: 314.9323 | l_Loss: 22.4608 | 
                                                                                 513		737.1259		-4.5000
25-09-28 13:44:20.767 - INFO: Learning rate: 3.1622776601683795e-05
25-09-28 13:44:20.768 - INFO: Train epoch 514:   Loss: 713.2733 | r_Loss: 78.7448 | g_Loss: 302.8234 | l_Loss: 16.7260 | 
                                                                                 514		713.2733		-4.5000
25-09-28 13:47:07.716 - INFO: Learning rate: 3.1622776601683795e-05
25-09-28 13:47:07.717 - INFO: Train epoch 515:   Loss: 819.6049 | r_Loss: 93.5279 | g_Loss: 318.1245 | l_Loss: 33.8411 | 
                                                                                 515		819.6049		-4.5000
25-09-28 13:49:54.647 - INFO: Learning rate: 3.1622776601683795e-05
25-09-28 13:49:54.648 - INFO: Train epoch 516:   Loss: 665.5263 | r_Loss: 68.7545 | g_Loss: 310.6280 | l_Loss: 11.1259 | 
                                                                                 516		665.5263		-4.5000
25-09-28 13:52:41.286 - INFO: Learning rate: 3.1622776601683795e-05
25-09-28 13:52:41.287 - INFO: Train epoch 517:   Loss: 665.4764 | r_Loss: 72.8955 | g_Loss: 284.9611 | l_Loss: 16.0380 | 
                                                                                 517		665.4764		-4.5000
25-09-28 13:55:28.039 - INFO: Learning rate: 3.1622776601683795e-05
25-09-28 13:55:28.039 - INFO: Train epoch 518:   Loss: 659.7630 | r_Loss: 71.8870 | g_Loss: 282.4639 | l_Loss: 17.8642 | 
                                                                                 518		659.7630		-4.5000
25-09-28 13:58:14.800 - INFO: Learning rate: 3.1622776601683795e-05
25-09-28 13:58:14.801 - INFO: Train epoch 519:   Loss: 668.1562 | r_Loss: 72.6457 | g_Loss: 283.7995 | l_Loss: 21.1280 | 
                                                                                 519		668.1562		-4.5000
25-09-28 14:01:01.733 - INFO: Learning rate: 3.1622776601683795e-05
25-09-28 14:01:01.734 - INFO: Train epoch 520:   Loss: 655.2287 | r_Loss: 72.4009 | g_Loss: 277.0412 | l_Loss: 16.1828 | 
                                                                                 520		655.2287		-4.5000
25-09-28 14:03:48.642 - INFO: Learning rate: 3.1622776601683795e-05
25-09-28 14:03:48.643 - INFO: Train epoch 521:   Loss: 679.2099 | r_Loss: 72.4049 | g_Loss: 286.6892 | l_Loss: 30.4962 | 
                                                                                 521		679.2099		-4.5000
25-09-28 14:06:35.585 - INFO: Learning rate: 3.1622776601683795e-05
25-09-28 14:06:35.585 - INFO: Train epoch 522:   Loss: 658.3885 | r_Loss: 70.6348 | g_Loss: 279.7680 | l_Loss: 25.4463 | 
                                                                                 522		658.3885		-4.5000
25-09-28 14:09:22.261 - INFO: Learning rate: 3.1622776601683795e-05
25-09-28 14:09:22.262 - INFO: Train epoch 523:   Loss: 642.2883 | r_Loss: 68.6663 | g_Loss: 275.7480 | l_Loss: 23.2086 | 
                                                                                 523		642.2883		-4.5000
25-09-28 14:12:08.767 - INFO: Learning rate: 3.1622776601683795e-05
25-09-28 14:12:08.768 - INFO: Train epoch 524:   Loss: 622.5848 | r_Loss: 68.1378 | g_Loss: 263.0339 | l_Loss: 18.8617 | 
                                                                                 524		622.5848		-4.5000
25-09-28 14:14:55.515 - INFO: Learning rate: 3.1622776601683795e-05
25-09-28 14:14:55.516 - INFO: Train epoch 525:   Loss: 30290.2996 | r_Loss: 4757.1152 | g_Loss: 4910.0418 | l_Loss: 1594.6816 | 
                                                                                 525		30290.2996		-4.5000
25-09-28 14:17:42.508 - INFO: Learning rate: 3.1622776601683795e-05
25-09-28 14:17:42.509 - INFO: Train epoch 526:   Loss: 3107.1735 | r_Loss: 298.2554 | g_Loss: 1568.5932 | l_Loss: 47.3031 | 
                                                                                 526		3107.1735		-4.5000
25-09-28 14:20:29.297 - INFO: Learning rate: 3.1622776601683795e-05
25-09-28 14:20:29.298 - INFO: Train epoch 527:   Loss: 2593.4833 | r_Loss: 283.8901 | g_Loss: 1130.8373 | l_Loss: 43.1955 | 
                                                                                 527		2593.4833		-4.5000
25-09-28 14:23:15.750 - INFO: Learning rate: 3.1622776601683795e-05
25-09-28 14:23:15.751 - INFO: Train epoch 528:   Loss: 1784.6482 | r_Loss: 190.2983 | g_Loss: 810.7785 | l_Loss: 22.3784 | 
                                                                                 528		1784.6482		-4.5000
25-09-28 14:26:02.689 - INFO: Learning rate: 3.1622776601683795e-05
25-09-28 14:26:02.689 - INFO: Train epoch 529:   Loss: 1479.3141 | r_Loss: 157.4646 | g_Loss: 673.6602 | l_Loss: 18.3307 | 
                                                                                 529		1479.3141		-4.5000
25-09-28 14:28:49.381 - INFO: Learning rate: 3.1622776601683795e-05
25-09-28 14:28:49.382 - INFO: Train epoch 530:   Loss: 1315.5225 | r_Loss: 143.6026 | g_Loss: 581.3437 | l_Loss: 16.1658 | 
                                                                                 530		1315.5225		-4.5000
25-09-28 14:31:36.152 - INFO: Learning rate: 3.1622776601683795e-05
25-09-28 14:31:36.153 - INFO: Train epoch 531:   Loss: 1179.0636 | r_Loss: 128.5919 | g_Loss: 521.5817 | l_Loss: 14.5222 | 
                                                                                 531		1179.0636		-4.5000
25-09-28 14:34:22.851 - INFO: Learning rate: 3.1622776601683795e-05
25-09-28 14:34:22.852 - INFO: Train epoch 532:   Loss: 1031.4661 | r_Loss: 108.9783 | g_Loss: 473.3755 | l_Loss: 13.1991 | 
                                                                                 532		1031.4661		-4.5000
25-09-28 14:37:09.450 - INFO: Learning rate: 3.1622776601683795e-05
25-09-28 14:37:09.450 - INFO: Train epoch 533:   Loss: 1025.2021 | r_Loss: 112.9310 | g_Loss: 447.7245 | l_Loss: 12.8226 | 
                                                                                 533		1025.2021		-4.5000
25-09-28 14:39:56.669 - INFO: Learning rate: 3.1622776601683795e-05
25-09-28 14:39:56.670 - INFO: Train epoch 534:   Loss: 914.6170 | r_Loss: 99.1913 | g_Loss: 407.1475 | l_Loss: 11.5130 | 
                                                                                 534		914.6170		-4.5000
25-09-28 14:42:43.818 - INFO: Learning rate: 3.1622776601683795e-05
25-09-28 14:42:43.818 - INFO: Train epoch 535:   Loss: 858.2773 | r_Loss: 93.5638 | g_Loss: 379.8035 | l_Loss: 10.6547 | 
                                                                                 535		858.2773		-4.5000
25-09-28 14:45:30.433 - INFO: Learning rate: 3.1622776601683795e-05
25-09-28 14:45:30.433 - INFO: Train epoch 536:   Loss: 887.1342 | r_Loss: 101.0598 | g_Loss: 371.2812 | l_Loss: 10.5542 | 
                                                                                 536		887.1342		-4.5000
25-09-28 14:48:17.048 - INFO: Learning rate: 3.1622776601683795e-05
25-09-28 14:48:17.048 - INFO: Train epoch 537:   Loss: 780.3490 | r_Loss: 84.6128 | g_Loss: 347.3389 | l_Loss: 9.9461 | 
                                                                                 537		780.3490		-4.5000
25-09-28 14:51:04.180 - INFO: Learning rate: 3.1622776601683795e-05
25-09-28 14:51:04.181 - INFO: Train epoch 538:   Loss: 739.4062 | r_Loss: 80.0187 | g_Loss: 329.9267 | l_Loss: 9.3861 | 
                                                                                 538		739.4062		-4.5000
25-09-28 14:53:51.312 - INFO: Learning rate: 3.1622776601683795e-05
25-09-28 14:53:51.313 - INFO: Train epoch 539:   Loss: 728.9072 | r_Loss: 81.3824 | g_Loss: 313.0487 | l_Loss: 8.9465 | 
                                                                                 539		728.9072		-4.5000
25-09-28 14:56:38.144 - INFO: Learning rate: 3.1622776601683795e-05
25-09-28 14:56:38.145 - INFO: Train epoch 540:   Loss: 716.7750 | r_Loss: 80.0780 | g_Loss: 306.9499 | l_Loss: 9.4350 | 
                                                                                 540		716.7750		-4.5000
25-09-28 14:59:25.125 - INFO: Learning rate: 3.1622776601683795e-05
25-09-28 14:59:25.126 - INFO: Train epoch 541:   Loss: 709.5255 | r_Loss: 80.2281 | g_Loss: 299.1985 | l_Loss: 9.1868 | 
                                                                                 541		709.5255		-4.5000
25-09-28 15:02:12.689 - INFO: Learning rate: 3.1622776601683795e-05
25-09-28 15:02:12.690 - INFO: Train epoch 542:   Loss: 667.0330 | r_Loss: 74.0134 | g_Loss: 288.3641 | l_Loss: 8.6022 | 
                                                                                 542		667.0330		-4.5000
25-09-28 15:04:59.838 - INFO: Learning rate: 3.1622776601683795e-05
25-09-28 15:04:59.838 - INFO: Train epoch 543:   Loss: 663.9300 | r_Loss: 74.4924 | g_Loss: 282.1957 | l_Loss: 9.2722 | 
                                                                                 543		663.9300		-4.5000
25-09-28 15:07:46.353 - INFO: Learning rate: 3.1622776601683795e-05
25-09-28 15:07:46.354 - INFO: Train epoch 544:   Loss: 643.4523 | r_Loss: 71.8145 | g_Loss: 275.3302 | l_Loss: 9.0495 | 
                                                                                 544		643.4523		-4.5000
25-09-28 15:10:33.016 - INFO: Learning rate: 3.1622776601683795e-05
25-09-28 15:10:33.017 - INFO: Train epoch 545:   Loss: 649.3935 | r_Loss: 73.1501 | g_Loss: 274.3755 | l_Loss: 9.2675 | 
                                                                                 545		649.3935		-4.5000
25-09-28 15:13:19.835 - INFO: Learning rate: 3.1622776601683795e-05
25-09-28 15:13:19.836 - INFO: Train epoch 546:   Loss: 659.8154 | r_Loss: 74.3566 | g_Loss: 276.6161 | l_Loss: 11.4161 | 
                                                                                 546		659.8154		-4.5000
25-09-28 15:16:06.714 - INFO: Learning rate: 3.1622776601683795e-05
25-09-28 15:16:06.714 - INFO: Train epoch 547:   Loss: 613.5783 | r_Loss: 69.0759 | g_Loss: 260.0015 | l_Loss: 8.1973 | 
                                                                                 547		613.5783		-4.5000
25-09-28 15:18:53.236 - INFO: Learning rate: 3.1622776601683795e-05
25-09-28 15:18:53.236 - INFO: Train epoch 548:   Loss: 624.2684 | r_Loss: 69.0618 | g_Loss: 264.9637 | l_Loss: 13.9957 | 
                                                                                 548		624.2684		-4.5000
25-09-28 15:21:40.437 - INFO: Learning rate: 3.1622776601683795e-05
25-09-28 15:21:40.438 - INFO: Train epoch 549:   Loss: 615.5189 | r_Loss: 69.6449 | g_Loss: 257.8487 | l_Loss: 9.4455 | 
                                                                                 549		615.5189		-4.5000
25-09-28 15:25:21.176 - INFO: TEST:   PSNR_S: 47.9586 | PSNR_C: 38.5868 | 
25-09-28 15:25:21.177 - INFO: Learning rate: 3.1622776601683795e-05
25-09-28 15:25:21.178 - INFO: Train epoch 550:   Loss: 647.1726 | r_Loss: 68.5935 | g_Loss: 275.3061 | l_Loss: 28.8988 | 
                                                                                 550		647.1726		-4.5000
25-09-28 15:28:08.832 - INFO: Learning rate: 3.1622776601683795e-05
25-09-28 15:28:08.833 - INFO: Train epoch 551:   Loss: 603.9388 | r_Loss: 67.2449 | g_Loss: 254.6577 | l_Loss: 13.0565 | 
                                                                                 551		603.9388		-4.5000
25-09-28 15:30:55.699 - INFO: Learning rate: 3.1622776601683795e-05
25-09-28 15:30:55.700 - INFO: Train epoch 552:   Loss: 592.2672 | r_Loss: 66.0176 | g_Loss: 250.4752 | l_Loss: 11.7039 | 
                                                                                 552		592.2672		-4.5000
25-09-28 15:33:42.571 - INFO: Learning rate: 3.1622776601683795e-05
25-09-28 15:33:42.572 - INFO: Train epoch 553:   Loss: 106693.7484 | r_Loss: 15666.5656 | g_Loss: 23487.5573 | l_Loss: 4873.3661 | 
                                                                                 553		106693.7484		-4.5000
25-09-28 15:36:29.838 - INFO: Learning rate: 3.1622776601683795e-05
25-09-28 15:36:29.838 - INFO: Train epoch 554:   Loss: 13313.1528 | r_Loss: 1787.3263 | g_Loss: 4061.9736 | l_Loss: 314.5477 | 
                                                                                 554		13313.1528		-4.5000
25-09-28 15:39:16.809 - INFO: Learning rate: 3.1622776601683795e-05
25-09-28 15:39:16.809 - INFO: Train epoch 555:   Loss: 9172.5428 | r_Loss: 1256.7508 | g_Loss: 2713.1332 | l_Loss: 175.6559 | 
                                                                                 555		9172.5428		-4.5000
25-09-28 15:42:03.388 - INFO: Learning rate: 3.1622776601683795e-05
25-09-28 15:42:03.389 - INFO: Train epoch 556:   Loss: 6939.3986 | r_Loss: 940.9376 | g_Loss: 2120.0831 | l_Loss: 114.6275 | 
                                                                                 556		6939.3986		-4.5000
25-09-28 15:44:50.139 - INFO: Learning rate: 3.1622776601683795e-05
25-09-28 15:44:50.140 - INFO: Train epoch 557:   Loss: 5653.3686 | r_Loss: 748.6879 | g_Loss: 1823.3989 | l_Loss: 86.5300 | 
                                                                                 557		5653.3686		-4.5000
25-09-28 15:47:37.194 - INFO: Learning rate: 3.1622776601683795e-05
25-09-28 15:47:37.195 - INFO: Train epoch 558:   Loss: 4722.7700 | r_Loss: 605.8995 | g_Loss: 1622.4068 | l_Loss: 70.8659 | 
                                                                                 558		4722.7700		-4.5000
25-09-28 15:50:24.468 - INFO: Learning rate: 3.1622776601683795e-05
25-09-28 15:50:24.468 - INFO: Train epoch 559:   Loss: 4035.7759 | r_Loss: 500.5361 | g_Loss: 1472.5133 | l_Loss: 60.5821 | 
                                                                                 559		4035.7759		-4.5000
25-09-28 15:53:12.042 - INFO: Learning rate: 3.1622776601683795e-05
25-09-28 15:53:12.043 - INFO: Train epoch 560:   Loss: 3565.8048 | r_Loss: 431.9838 | g_Loss: 1352.6947 | l_Loss: 53.1910 | 
                                                                                 560		3565.8048		-4.5000
25-09-28 15:55:58.841 - INFO: Learning rate: 3.1622776601683795e-05
25-09-28 15:55:58.842 - INFO: Train epoch 561:   Loss: 3257.0804 | r_Loss: 391.6241 | g_Loss: 1251.5677 | l_Loss: 47.3919 | 
                                                                                 561		3257.0804		-4.5000
25-09-28 15:58:46.017 - INFO: Learning rate: 3.1622776601683795e-05
25-09-28 15:58:46.018 - INFO: Train epoch 562:   Loss: 2971.6295 | r_Loss: 352.8777 | g_Loss: 1164.5016 | l_Loss: 42.7391 | 
                                                                                 562		2971.6295		-4.5000
25-09-28 16:01:33.022 - INFO: Learning rate: 3.1622776601683795e-05
25-09-28 16:01:33.023 - INFO: Train epoch 563:   Loss: 2826.4730 | r_Loss: 338.7381 | g_Loss: 1093.5328 | l_Loss: 39.2498 | 
                                                                                 563		2826.4730		-4.5000
25-09-28 16:04:20.011 - INFO: Learning rate: 3.1622776601683795e-05
25-09-28 16:04:20.012 - INFO: Train epoch 564:   Loss: 2603.7424 | r_Loss: 307.8909 | g_Loss: 1028.0132 | l_Loss: 36.2747 | 
                                                                                 564		2603.7424		-4.5000
25-09-28 16:07:06.925 - INFO: Learning rate: 3.1622776601683795e-05
25-09-28 16:07:06.926 - INFO: Train epoch 565:   Loss: 2807.5375 | r_Loss: 357.6919 | g_Loss: 982.6581 | l_Loss: 36.4197 | 
                                                                                 565		2807.5375		-4.5000
25-09-28 16:09:54.106 - INFO: Learning rate: 3.1622776601683795e-05
25-09-28 16:09:54.107 - INFO: Train epoch 566:   Loss: 2236.0471 | r_Loss: 251.7171 | g_Loss: 946.6290 | l_Loss: 30.8328 | 
                                                                                 566		2236.0471		-4.5000
25-09-28 16:12:41.538 - INFO: Learning rate: 3.1622776601683795e-05
25-09-28 16:12:41.539 - INFO: Train epoch 567:   Loss: 2128.8659 | r_Loss: 244.7335 | g_Loss: 876.4325 | l_Loss: 28.7657 | 
                                                                                 567		2128.8659		-4.5000
25-09-28 16:15:28.302 - INFO: Learning rate: 3.1622776601683795e-05
25-09-28 16:15:28.303 - INFO: Train epoch 568:   Loss: 2157.3472 | r_Loss: 258.4214 | g_Loss: 837.3447 | l_Loss: 27.8957 | 
                                                                                 568		2157.3472		-4.5000
25-09-28 16:18:15.486 - INFO: Learning rate: 3.1622776601683795e-05
25-09-28 16:18:15.487 - INFO: Train epoch 569:   Loss: 1955.8897 | r_Loss: 227.0575 | g_Loss: 794.8581 | l_Loss: 25.7443 | 
                                                                                 569		1955.8897		-4.5000
25-09-28 16:21:02.995 - INFO: Learning rate: 3.1622776601683795e-05
25-09-28 16:21:02.996 - INFO: Train epoch 570:   Loss: 1891.6141 | r_Loss: 221.2052 | g_Loss: 761.0859 | l_Loss: 24.5023 | 
                                                                                 570		1891.6141		-4.5000
25-09-28 16:23:49.661 - INFO: Learning rate: 3.1622776601683795e-05
25-09-28 16:23:49.662 - INFO: Train epoch 571:   Loss: 1837.0730 | r_Loss: 216.7121 | g_Loss: 730.1557 | l_Loss: 23.3569 | 
                                                                                 571		1837.0730		-4.5000
25-09-28 16:26:36.328 - INFO: Learning rate: 3.1622776601683795e-05
25-09-28 16:26:36.329 - INFO: Train epoch 572:   Loss: 1767.6882 | r_Loss: 207.6360 | g_Loss: 707.2605 | l_Loss: 22.2480 | 
                                                                                 572		1767.6882		-4.5000
25-09-28 16:29:22.952 - INFO: Learning rate: 3.1622776601683795e-05
25-09-28 16:29:22.953 - INFO: Train epoch 573:   Loss: 1707.8599 | r_Loss: 200.5840 | g_Loss: 683.7286 | l_Loss: 21.2114 | 
                                                                                 573		1707.8599		-4.5000
25-09-28 16:32:09.487 - INFO: Learning rate: 3.1622776601683795e-05
25-09-28 16:32:09.488 - INFO: Train epoch 574:   Loss: 1581.6643 | r_Loss: 182.6969 | g_Loss: 648.5902 | l_Loss: 19.5894 | 
                                                                                 574		1581.6643		-4.5000
25-09-28 16:34:56.158 - INFO: Learning rate: 3.1622776601683795e-05
25-09-28 16:34:56.159 - INFO: Train epoch 575:   Loss: 1586.2191 | r_Loss: 187.6497 | g_Loss: 628.8832 | l_Loss: 19.0873 | 
                                                                                 575		1586.2191		-4.5000
25-09-28 16:37:43.051 - INFO: Learning rate: 3.1622776601683795e-05
25-09-28 16:37:43.052 - INFO: Train epoch 576:   Loss: 1530.4396 | r_Loss: 179.3478 | g_Loss: 615.0565 | l_Loss: 18.6439 | 
                                                                                 576		1530.4396		-4.5000
25-09-28 16:40:30.182 - INFO: Learning rate: 3.1622776601683795e-05
25-09-28 16:40:30.183 - INFO: Train epoch 577:   Loss: 1484.1576 | r_Loss: 174.0745 | g_Loss: 595.9018 | l_Loss: 17.8835 | 
                                                                                 577		1484.1576		-4.5000
25-09-28 16:43:16.924 - INFO: Learning rate: 3.1622776601683795e-05
25-09-28 16:43:16.925 - INFO: Train epoch 578:   Loss: 1421.9640 | r_Loss: 167.2787 | g_Loss: 569.2837 | l_Loss: 16.2867 | 
                                                                                 578		1421.9640		-4.5000
25-09-28 16:46:03.920 - INFO: Learning rate: 3.1622776601683795e-05
25-09-28 16:46:03.921 - INFO: Train epoch 579:   Loss: 1342.1746 | r_Loss: 154.3802 | g_Loss: 554.4734 | l_Loss: 15.8002 | 
                                                                                 579		1342.1746		-4.5000
25-09-28 16:48:50.715 - INFO: Learning rate: 3.1622776601683795e-05
25-09-28 16:48:50.716 - INFO: Train epoch 580:   Loss: 1287.5784 | r_Loss: 147.5604 | g_Loss: 534.7559 | l_Loss: 15.0204 | 
                                                                                 580		1287.5784		-4.5000
25-09-28 16:51:37.333 - INFO: Learning rate: 3.1622776601683795e-05
25-09-28 16:51:37.334 - INFO: Train epoch 581:   Loss: 1256.1780 | r_Loss: 144.5046 | g_Loss: 519.1316 | l_Loss: 14.5236 | 
                                                                                 581		1256.1780		-4.5000
25-09-28 16:54:24.245 - INFO: Learning rate: 3.1622776601683795e-05
25-09-28 16:54:24.246 - INFO: Train epoch 582:   Loss: 1237.1093 | r_Loss: 144.3621 | g_Loss: 501.2789 | l_Loss: 14.0199 | 
                                                                                 582		1237.1093		-4.5000
25-09-28 16:57:11.425 - INFO: Learning rate: 3.1622776601683795e-05
25-09-28 16:57:11.426 - INFO: Train epoch 583:   Loss: 1162.8483 | r_Loss: 132.4079 | g_Loss: 487.0934 | l_Loss: 13.7153 | 
                                                                                 583		1162.8483		-4.5000
25-09-28 16:59:58.355 - INFO: Learning rate: 3.1622776601683795e-05
25-09-28 16:59:58.356 - INFO: Train epoch 584:   Loss: 1118.9244 | r_Loss: 127.2223 | g_Loss: 469.8176 | l_Loss: 12.9954 | 
                                                                                 584		1118.9244		-4.5000
25-09-28 17:02:45.005 - INFO: Learning rate: 3.1622776601683795e-05
25-09-28 17:02:45.005 - INFO: Train epoch 585:   Loss: 1129.2734 | r_Loss: 131.3612 | g_Loss: 459.4510 | l_Loss: 13.0162 | 
                                                                                 585		1129.2734		-4.5000
25-09-28 17:05:32.528 - INFO: Learning rate: 3.1622776601683795e-05
25-09-28 17:05:32.528 - INFO: Train epoch 586:   Loss: 1056.8064 | r_Loss: 120.4891 | g_Loss: 441.8636 | l_Loss: 12.4970 | 
                                                                                 586		1056.8064		-4.5000
25-09-28 17:08:19.507 - INFO: Learning rate: 3.1622776601683795e-05
25-09-28 17:08:19.508 - INFO: Train epoch 587:   Loss: 1052.7335 | r_Loss: 120.8234 | g_Loss: 435.7054 | l_Loss: 12.9110 | 
                                                                                 587		1052.7335		-4.5000
25-09-28 17:11:05.838 - INFO: Learning rate: 3.1622776601683795e-05
25-09-28 17:11:05.839 - INFO: Train epoch 588:   Loss: 990.0910 | r_Loss: 111.7098 | g_Loss: 419.4452 | l_Loss: 12.0968 | 
                                                                                 588		990.0910		-4.5000
25-09-28 17:13:52.410 - INFO: Learning rate: 3.1622776601683795e-05
25-09-28 17:13:52.411 - INFO: Train epoch 589:   Loss: 1006.6730 | r_Loss: 117.6221 | g_Loss: 405.7825 | l_Loss: 12.7798 | 
                                                                                 589		1006.6730		-4.5000
25-09-28 17:16:39.474 - INFO: Learning rate: 3.1622776601683795e-05
25-09-28 17:16:39.475 - INFO: Train epoch 590:   Loss: 933.8307 | r_Loss: 103.6845 | g_Loss: 402.0291 | l_Loss: 13.3788 | 
                                                                                 590		933.8307		-4.5000
25-09-28 17:19:26.496 - INFO: Learning rate: 3.1622776601683795e-05
25-09-28 17:19:26.497 - INFO: Train epoch 591:   Loss: 909.3674 | r_Loss: 101.3873 | g_Loss: 386.6758 | l_Loss: 15.7549 | 
                                                                                 591		909.3674		-4.5000
25-09-28 17:22:13.085 - INFO: Learning rate: 3.1622776601683795e-05
25-09-28 17:22:13.086 - INFO: Train epoch 592:   Loss: 931.0922 | r_Loss: 106.4057 | g_Loss: 385.1266 | l_Loss: 13.9373 | 
                                                                                 592		931.0922		-4.5000
25-09-28 17:24:59.822 - INFO: Learning rate: 3.1622776601683795e-05
25-09-28 17:24:59.823 - INFO: Train epoch 593:   Loss: 882.2187 | r_Loss: 97.3788 | g_Loss: 377.7001 | l_Loss: 17.6244 | 
                                                                                 593		882.2187		-4.5000
25-09-28 17:27:46.483 - INFO: Learning rate: 3.1622776601683795e-05
25-09-28 17:27:46.484 - INFO: Train epoch 594:   Loss: 819.7810 | r_Loss: 90.1052 | g_Loss: 352.6192 | l_Loss: 16.6356 | 
                                                                                 594		819.7810		-4.5000
25-09-28 17:30:33.150 - INFO: Learning rate: 3.1622776601683795e-05
25-09-28 17:30:33.151 - INFO: Train epoch 595:   Loss: 835.5158 | r_Loss: 94.0855 | g_Loss: 348.0136 | l_Loss: 17.0749 | 
                                                                                 595		835.5158		-4.5000
25-09-28 17:33:20.070 - INFO: Learning rate: 3.1622776601683795e-05
25-09-28 17:33:20.071 - INFO: Train epoch 596:   Loss: 796.9774 | r_Loss: 86.1619 | g_Loss: 342.5201 | l_Loss: 23.6479 | 
                                                                                 596		796.9774		-4.5000
25-09-28 17:36:07.352 - INFO: Learning rate: 3.1622776601683795e-05
25-09-28 17:36:07.352 - INFO: Train epoch 597:   Loss: 803.3612 | r_Loss: 86.4343 | g_Loss: 339.8002 | l_Loss: 31.3898 | 
                                                                                 597		803.3612		-4.5000
25-09-28 17:38:55.592 - INFO: Learning rate: 3.1622776601683795e-05
25-09-28 17:38:55.592 - INFO: Train epoch 598:   Loss: 768.2041 | r_Loss: 87.5481 | g_Loss: 316.8024 | l_Loss: 13.6614 | 
                                                                                 598		768.2041		-4.5000
25-09-28 17:41:42.780 - INFO: Learning rate: 3.1622776601683795e-05
25-09-28 17:41:42.781 - INFO: Train epoch 599:   Loss: 6203.6298 | r_Loss: 831.2040 | g_Loss: 1690.1811 | l_Loss: 357.4288 | 
                                                                                 599		6203.6298		-4.5000
25-09-28 17:45:23.418 - INFO: TEST:   PSNR_S: 41.2436 | PSNR_C: 30.9517 | 
25-09-28 17:45:23.419 - INFO: Learning rate: 3.1622776601683795e-05
25-09-28 17:45:23.421 - INFO: Train epoch 600:   Loss: 1546.0084 | r_Loss: 141.2711 | g_Loss: 814.9879 | l_Loss: 24.6648 | 
                                                                                 600		1546.0084		-4.5000
25-09-28 17:48:10.163 - INFO: Learning rate: 3.1622776601683795e-05
25-09-28 17:48:10.164 - INFO: Train epoch 601:   Loss: 1235.1467 | r_Loss: 130.2922 | g_Loss: 566.4327 | l_Loss: 17.2530 | 
                                                                                 601		1235.1467		-4.5000
25-09-28 17:50:57.368 - INFO: Learning rate: 3.1622776601683795e-05
25-09-28 17:50:57.369 - INFO: Train epoch 602:   Loss: 918.3135 | r_Loss: 90.4097 | g_Loss: 453.8889 | l_Loss: 12.3759 | 
                                                                                 602		918.3135		-4.5000
25-09-28 17:53:44.300 - INFO: Learning rate: 3.1622776601683795e-05
25-09-28 17:53:44.301 - INFO: Train epoch 603:   Loss: 864.5148 | r_Loss: 92.2305 | g_Loss: 392.2309 | l_Loss: 11.1315 | 
                                                                                 603		864.5148		-4.5000
25-09-28 17:56:32.466 - INFO: Learning rate: 3.1622776601683795e-05
25-09-28 17:56:32.467 - INFO: Train epoch 604:   Loss: 794.9391 | r_Loss: 85.1976 | g_Loss: 358.8523 | l_Loss: 10.0990 | 
                                                                                 604		794.9391		-4.5000
25-09-28 17:59:19.668 - INFO: Learning rate: 3.1622776601683795e-05
25-09-28 17:59:19.669 - INFO: Train epoch 605:   Loss: 768.5591 | r_Loss: 84.9399 | g_Loss: 334.2499 | l_Loss: 9.6096 | 
                                                                                 605		768.5591		-4.5000
25-09-28 18:02:06.466 - INFO: Learning rate: 3.1622776601683795e-05
25-09-28 18:02:06.467 - INFO: Train epoch 606:   Loss: 721.0340 | r_Loss: 78.7551 | g_Loss: 318.1940 | l_Loss: 9.0645 | 
                                                                                 606		721.0340		-4.5000
25-09-28 18:04:53.902 - INFO: Learning rate: 3.1622776601683795e-05
25-09-28 18:04:53.903 - INFO: Train epoch 607:   Loss: 720.1957 | r_Loss: 82.0251 | g_Loss: 301.2568 | l_Loss: 8.8133 | 
                                                                                 607		720.1957		-4.5000
25-09-28 18:07:42.390 - INFO: Learning rate: 3.1622776601683795e-05
25-09-28 18:07:42.391 - INFO: Train epoch 608:   Loss: 676.8438 | r_Loss: 74.0979 | g_Loss: 297.8172 | l_Loss: 8.5371 | 
                                                                                 608		676.8438		-4.5000
25-09-28 18:10:29.448 - INFO: Learning rate: 3.1622776601683795e-05
25-09-28 18:10:29.449 - INFO: Train epoch 609:   Loss: 673.0190 | r_Loss: 75.5555 | g_Loss: 285.4555 | l_Loss: 9.7863 | 
                                                                                 609		673.0190		-4.5000
